./DeepSpeed/csrc/aio/py_test/aio_bench_generate_param.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/csrc/aio/py_test/aio_bench_generate_param.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/csrc/aio/py_test/aio_bench_generate_param.py: DeepSpeed Team
./DeepSpeed/csrc/aio/py_test/perf_sweep_utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/csrc/aio/py_test/perf_sweep_utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/csrc/aio/py_test/perf_sweep_utils.py: DeepSpeed Team
./DeepSpeed/csrc/aio/py_test/test_ds_aio_utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/csrc/aio/py_test/test_ds_aio_utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/csrc/aio/py_test/test_ds_aio_utils.py: DeepSpeed Team
./DeepSpeed/csrc/aio/py_test/test_ds_aio_utils.py:    print(f'pool_results = {pool_results}')
./DeepSpeed/csrc/aio/py_test/validate_async_io.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/csrc/aio/py_test/validate_async_io.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/csrc/aio/py_test/validate_async_io.py: DeepSpeed Team
./DeepSpeed/csrc/aio/py_test/test_ds_aio.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/csrc/aio/py_test/test_ds_aio.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/csrc/aio/py_test/test_ds_aio.py: DeepSpeed Team
./DeepSpeed/csrc/aio/py_test/aio_bench_perf_sweep.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/csrc/aio/py_test/aio_bench_perf_sweep.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/csrc/aio/py_test/aio_bench_perf_sweep.py: DeepSpeed Team
./DeepSpeed/csrc/aio/py_test/aio_bench_perf_sweep.py:    dump_cmd_lines(cmd_list)
./DeepSpeed/csrc/aio/py_test/aio_bench_perf_sweep.py:    dump_cmd_lines(read_cmd_lines)
./DeepSpeed/csrc/aio/py_test/aio_bench_perf_sweep.py:    dump_cmd_lines(write_cmd_lines)
./DeepSpeed/csrc/aio/py_test/ds_aio_basic.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/csrc/aio/py_test/ds_aio_basic.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/csrc/aio/py_test/ds_aio_basic.py: DeepSpeed Team
./DeepSpeed/csrc/aio/py_test/ds_aio_basic.py:     Create schedule
./DeepSpeed/csrc/aio/py_test/ds_aio_basic.py:     Run pre task
./DeepSpeed/csrc/aio/py_test/ds_aio_basic.py:     Run main tasks in a loop
./DeepSpeed/csrc/aio/py_test/ds_aio_basic.py:     Run post task
./DeepSpeed/csrc/aio/py_test/parse_aio_stats.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/csrc/aio/py_test/parse_aio_stats.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/csrc/aio/py_test/parse_aio_stats.py: DeepSpeed Team
./DeepSpeed/csrc/aio/py_test/ds_aio_handle.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/csrc/aio/py_test/ds_aio_handle.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/csrc/aio/py_test/ds_aio_handle.py: DeepSpeed Team
./DeepSpeed/csrc/aio/py_test/ds_aio_handle.py:     Create schedule
./DeepSpeed/csrc/aio/py_test/ds_aio_handle.py:     Run pre task
./DeepSpeed/csrc/aio/py_test/ds_aio_handle.py:     Run main tasks in a loop
./DeepSpeed/csrc/aio/py_test/ds_aio_handle.py:     Run post task
./DeepSpeed/deepspeed/elasticity/config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/elasticity/config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/elasticity/config.py: DeepSpeed Team
./DeepSpeed/deepspeed/elasticity/constants.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/elasticity/constants.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/elasticity/constants.py: DeepSpeed Team
./DeepSpeed/deepspeed/elasticity/constants.py:########################################
./DeepSpeed/deepspeed/elasticity/constants.py: Elasticity
./DeepSpeed/deepspeed/elasticity/constants.py:########################################
./DeepSpeed/deepspeed/elasticity/constants.py: Current elasticity version
./DeepSpeed/deepspeed/elasticity/constants.py: Max acceptable train_batch_size
./DeepSpeed/deepspeed/elasticity/constants.py: Acceptable micro batch sizes, same as train_micro_batch_size_per_gpu
./DeepSpeed/deepspeed/elasticity/constants.py: Min/max of GPUs to search over
./DeepSpeed/deepspeed/elasticity/constants.py: Minimum running time (minutes) before the scheduler will scale us, 0 implies it's unknown
./DeepSpeed/deepspeed/elasticity/constants.py: When finding a suitable batch size, attempt to find one that is closest
./DeepSpeed/deepspeed/elasticity/constants.py: to the max train batch size given.
./DeepSpeed/deepspeed/elasticity/constants.py: In order to reduce confusion, if elastic mode is enabled we
./DeepSpeed/deepspeed/elasticity/constants.py: require (via assert) that no batch info is set outside of the
./DeepSpeed/deepspeed/elasticity/constants.py: elastic config. You can turn off this assert via this config
./DeepSpeed/deepspeed/elasticity/constants.py: but keep in mind that all batch info defined outside the
./DeepSpeed/deepspeed/elasticity/constants.py: elastic mode *will be ignored*.
./DeepSpeed/deepspeed/elasticity/constants.py: Version of elastic logic to use
./DeepSpeed/deepspeed/elasticity/constants.py: Minimum deepspeed version to use elasticity
./DeepSpeed/deepspeed/elasticity/constants.py: Environment variable storing elastic config from resource scheduler
./DeepSpeed/deepspeed/elasticity/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/elasticity/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/elasticity/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/elasticity/elastic_agent.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/elasticity/elastic_agent.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/elasticity/elastic_agent.py: DeepSpeed Team
./DeepSpeed/deepspeed/elasticity/elastic_agent.py:             master_addr = _get_fq_hostname()
./DeepSpeed/deepspeed/elasticity/elastic_agent.py:         scaling events do not count towards restarts (gets same attempt #)
./DeepSpeed/deepspeed/elasticity/elastic_agent.py:         remove existing log dir if this restart is due to a scaling event
./DeepSpeed/deepspeed/elasticity/elastic_agent.py:         NOTE: currently only works for a single role
./DeepSpeed/deepspeed/elasticity/elastic_agent.py:                     rdzv_handler._state_holder.state.restart = False
./DeepSpeed/deepspeed/elasticity/elastic_agent.py:                 membership changes do not count as retries
./DeepSpeed/deepspeed/elasticity/utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/elasticity/utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/elasticity/utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/elasticity/elasticity.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/elasticity/elasticity.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/elasticity/elasticity.py: DeepSpeed Team
./DeepSpeed/deepspeed/elasticity/elasticity.py: Thirty eight smallest highly composite numbers. The list should
./DeepSpeed/deepspeed/elasticity/elasticity.py: be enough to support up to 720K batch size.
./DeepSpeed/deepspeed/elasticity/elasticity.py:             find all factors less than max_gpus / 2
./DeepSpeed/deepspeed/elasticity/elasticity.py:     Ensure target deepspeed version works with intended elasticity version
./DeepSpeed/deepspeed/elasticity/elasticity.py:         ensure batch size is int dtype
./DeepSpeed/deepspeed/elasticity/elasticity.py:         ensure batch size is int dtype
./DeepSpeed/deepspeed/elasticity/elasticity.py:         Pick largest valid micro batch size
./DeepSpeed/deepspeed/elasticity/elasticity.py:         Pick a valid micro batch size
./DeepSpeed/deepspeed/launcher/runner.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/launcher/runner.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/launcher/runner.py: DeepSpeed Team
./DeepSpeed/deepspeed/launcher/runner.py: On AISC compute, each node sets environment variables independently, want to prevent
./DeepSpeed/deepspeed/launcher/runner.py: exporting rank-0 env variables in case of heterogeneous compute.
./DeepSpeed/deepspeed/launcher/runner.py:     e.g., worker-0 slots=16
./DeepSpeed/deepspeed/launcher/runner.py:     Regex matches one or more non-whitespace characters (\S+) at the start of
./DeepSpeed/deepspeed/launcher/runner.py:     the line, followed by one or more whitespace characters (\s+), followed
./DeepSpeed/deepspeed/launcher/runner.py:     by the string "slots=", followed by one or more digits (\d+).
./DeepSpeed/deepspeed/launcher/runner.py:             hostfile comment or empty line, ignore
./DeepSpeed/deepspeed/launcher/runner.py:     Create a new list in the same order as original but with duplicates
./DeepSpeed/deepspeed/launcher/runner.py:     removed, should never be more than ~16 elements so simple is best
./DeepSpeed/deepspeed/launcher/runner.py:     Constants that define our syntax
./DeepSpeed/deepspeed/launcher/runner.py:     Ensure include/exclude are mutually exclusive
./DeepSpeed/deepspeed/launcher/runner.py:     no-op
./DeepSpeed/deepspeed/launcher/runner.py:     Either build from scratch or remove items
./DeepSpeed/deepspeed/launcher/runner.py:     foreach node in the list
./DeepSpeed/deepspeed/launcher/runner.py:         Node can either be alone or node:slot,slot,slot
./DeepSpeed/deepspeed/launcher/runner.py:             sanity checks
./DeepSpeed/deepspeed/launcher/runner.py:             If include string, build the list from here
./DeepSpeed/deepspeed/launcher/runner.py:         User just specified the whole node
./DeepSpeed/deepspeed/launcher/runner.py:             sanity check hostname
./DeepSpeed/deepspeed/launcher/runner.py:     Post-processing to remove duplicates and empty nodes
./DeepSpeed/deepspeed/launcher/runner.py:         Remove duplicates
./DeepSpeed/deepspeed/launcher/runner.py:         Remove empty hosts
./DeepSpeed/deepspeed/launcher/runner.py:     Lastly, go over filtered_hosts and convert to a OrderedDict() to ensure
./DeepSpeed/deepspeed/launcher/runner.py:     we map ranks to nodes correctly by maintaining host_info ordering.
./DeepSpeed/deepspeed/launcher/runner.py:     For when argparse interprets remaining args as a single string
./DeepSpeed/deepspeed/launcher/runner.py:     respect CUDA_VISIBLE_DEVICES for a single node and no explicit resource filters
./DeepSpeed/deepspeed/launcher/runner.py:     validate that passwordless-ssh is workly properly with this hostfile
./DeepSpeed/deepspeed/launcher/runner.py:     encode world info as base64 to make it easier to pass via command line
./DeepSpeed/deepspeed/launcher/runner.py:                 key exists in launcher env -> var list should be used
./DeepSpeed/deepspeed/launcher/runner.py:             handle if users to enter path for `DS_ENV_FILE`
./DeepSpeed/deepspeed/launcher/runner.py:     In case of failure must propagate the error-condition back to the caller (usually shell). The
./DeepSpeed/deepspeed/launcher/runner.py:     actual error and traceback should have been printed in the subprocess, so in order to avoid
./DeepSpeed/deepspeed/launcher/runner.py:     unnecessary noise we just quietly exit here with the same code as the subprocess
./DeepSpeed/deepspeed/launcher/multinode_runner.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/launcher/multinode_runner.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/launcher/multinode_runner.py: DeepSpeed Team
./DeepSpeed/deepspeed/launcher/multinode_runner.py:         PDSH flags for max node fan out and specific hosts to launch on
./DeepSpeed/deepspeed/launcher/multinode_runner.py:         See https://linux.die.net/man/1/pdsh for flag details
./DeepSpeed/deepspeed/launcher/multinode_runner.py:         https://linux.die.net/man/1/pdsh
./DeepSpeed/deepspeed/launcher/multinode_runner.py:         %n will be replaced by pdsh command
./DeepSpeed/deepspeed/launcher/multinode_runner.py:        TODO: if IB is available we should suggestion mvapich
./DeepSpeed/deepspeed/launcher/multinode_runner.py:        TODO: Allow for include/exclude at node-level but not gpu-level
./DeepSpeed/deepspeed/launcher/multinode_runner.py:        TODO: if IB is available we should suggestion mpich
./DeepSpeed/deepspeed/launcher/multinode_runner.py:        TODO: Allow for include/exclude at node-level but not gpu-level
./DeepSpeed/deepspeed/launcher/multinode_runner.py:        TODO: if IB is available we should suggestion mpich
./DeepSpeed/deepspeed/launcher/multinode_runner.py:        TODO: Allow for include/exclude at node-level but not gpu-level
./DeepSpeed/deepspeed/launcher/multinode_runner.py:         turn off IMPI core binding, use deepspeed's own core binding
./DeepSpeed/deepspeed/launcher/multinode_runner.py:         Disable the CMA kernel module, not available on Ubuntu systems
./DeepSpeed/deepspeed/launcher/multinode_runner.py:         If we fail this will output more verbose logging
./DeepSpeed/deepspeed/launcher/multinode_runner.py:         Enabled cuda-aware communication
./DeepSpeed/deepspeed/launcher/multinode_runner.py:         Support deep learning frameworks: http://hidl.cse.ohio-state.edu/userguide/horovod/
./DeepSpeed/deepspeed/launcher/multinode_runner.py:         Support MPI_THREAD_MULTIPLE
./DeepSpeed/deepspeed/launcher/multinode_runner.py:         Performance tuning flags for allgather
./DeepSpeed/deepspeed/launcher/multinode_runner.py:        TODO: if IB is available we should suggestion mvapich
./DeepSpeed/deepspeed/launcher/multinode_runner.py:        TODO: Allow for include/exclude at node-level but not gpu-level
./DeepSpeed/deepspeed/launcher/constants.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/launcher/constants.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/launcher/constants.py: DeepSpeed Team
./DeepSpeed/deepspeed/launcher/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/launcher/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/launcher/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/launcher/launch.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/launcher/launch.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/launcher/launch.py: DeepSpeed Team
./DeepSpeed/deepspeed/launcher/launch.py:     Optional arguments for the launch helper
./DeepSpeed/deepspeed/launcher/launch.py:     positional
./DeepSpeed/deepspeed/launcher/launch.py:     rest from the training program
./DeepSpeed/deepspeed/launcher/launch.py: Adapted from https://psutil.readthedocs.io/en/latest/#kill-process-tree
./DeepSpeed/deepspeed/launcher/launch.py:     set PyTorch distributed related environmental variables
./DeepSpeed/deepspeed/launcher/launch.py:             prepare the log path and the file name prefix
./DeepSpeed/deepspeed/launcher/launch.py:             each process's rank
./DeepSpeed/deepspeed/launcher/launch.py:             spawn the processes
./DeepSpeed/deepspeed/launcher/launch.py:             A user may not want to pass local_rank as a keyword arg so we make this optional.
./DeepSpeed/deepspeed/launcher/launch.py:         Get config and arguments
./DeepSpeed/deepspeed/launcher/launch.py:         Creating config for rendezvous class
./DeepSpeed/deepspeed/launcher/launch.py:     pass SIGINT/SIGTERM to children if the parent is being terminated
./DeepSpeed/deepspeed/launcher/launch.py:                 the process is still running
./DeepSpeed/deepspeed/launcher/launch.py:                     exited cleanly
./DeepSpeed/deepspeed/model_implementations/diffusers/unet.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/model_implementations/diffusers/unet.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/model_implementations/diffusers/unet.py: DeepSpeed Team
./DeepSpeed/deepspeed/model_implementations/diffusers/unet.py:         SD pipeline accesses this attribute
./DeepSpeed/deepspeed/model_implementations/diffusers/unet.py:         warmup to create the workspace and cublas handle
./DeepSpeed/deepspeed/model_implementations/diffusers/unet.py:         create cuda_graph and assign static_inputs and static_outputs
./DeepSpeed/deepspeed/model_implementations/diffusers/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/model_implementations/diffusers/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/model_implementations/diffusers/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/model_implementations/diffusers/vae.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/model_implementations/diffusers/vae.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/model_implementations/diffusers/vae.py: DeepSpeed Team
./DeepSpeed/deepspeed/model_implementations/diffusers/vae.py:         warmup to create the workspace and cublas handle
./DeepSpeed/deepspeed/model_implementations/diffusers/vae.py:         create cuda_graph and assign static_inputs and static_outputs
./DeepSpeed/deepspeed/model_implementations/diffusers/vae.py:         warmup to create the workspace and cublas handle
./DeepSpeed/deepspeed/model_implementations/diffusers/vae.py:         create cuda_graph and assign static_inputs and static_outputs
./DeepSpeed/deepspeed/model_implementations/diffusers/vae.py:         warmup to create the workspace and cublas handle
./DeepSpeed/deepspeed/model_implementations/diffusers/vae.py:         create cuda_graph and assign static_inputs and static_outputs
./DeepSpeed/deepspeed/model_implementations/features/cuda_graph.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/model_implementations/features/cuda_graph.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/model_implementations/features/cuda_graph.py: DeepSpeed Team
./DeepSpeed/deepspeed/model_implementations/features/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/model_implementations/features/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/model_implementations/features/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/model_implementations/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/model_implementations/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/model_implementations/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/model_implementations/transformers/ds_bert.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/model_implementations/transformers/ds_bert.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/model_implementations/transformers/ds_bert.py: DeepSpeed Team
./DeepSpeed/deepspeed/model_implementations/transformers/ds_base.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/model_implementations/transformers/ds_base.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/model_implementations/transformers/ds_base.py: DeepSpeed Team
./DeepSpeed/deepspeed/model_implementations/transformers/ds_base.py:     this would be the new clean base class that will replace DeepSpeedTransformerInference.
./DeepSpeed/deepspeed/model_implementations/transformers/ds_base.py:     we currently don't know how this will look like but keeping it here as a placeholder.
./DeepSpeed/deepspeed/model_implementations/transformers/ds_opt.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/model_implementations/transformers/ds_opt.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/model_implementations/transformers/ds_opt.py: DeepSpeed Team
./DeepSpeed/deepspeed/model_implementations/transformers/ds_bloom.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/model_implementations/transformers/ds_bloom.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/model_implementations/transformers/ds_bloom.py: DeepSpeed Team
./DeepSpeed/deepspeed/model_implementations/transformers/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/model_implementations/transformers/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/model_implementations/transformers/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/model_implementations/transformers/clip_encoder.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/model_implementations/transformers/clip_encoder.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/model_implementations/transformers/clip_encoder.py: DeepSpeed Team
./DeepSpeed/deepspeed/model_implementations/transformers/clip_encoder.py:         warmup to create the workspace and cublas handle
./DeepSpeed/deepspeed/model_implementations/transformers/clip_encoder.py:         create cuda_graph and assign static_inputs and static_outputs
./DeepSpeed/deepspeed/model_implementations/transformers/ds_gpt.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/model_implementations/transformers/ds_gpt.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/model_implementations/transformers/ds_gpt.py: DeepSpeed Team
./DeepSpeed/deepspeed/model_implementations/transformers/ds_megatron_gpt.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/model_implementations/transformers/ds_megatron_gpt.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/model_implementations/transformers/ds_megatron_gpt.py: DeepSpeed Team
./DeepSpeed/deepspeed/model_implementations/transformers/ds_llama2.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/model_implementations/transformers/ds_llama2.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/model_implementations/transformers/ds_llama2.py: DeepSpeed Team
./DeepSpeed/deepspeed/model_implementations/transformers/ds_llama2.py:         Allocate memory only on first layer forward
./DeepSpeed/deepspeed/model_implementations/transformers/ds_llama2.py:         We set the prev key/value to None when there is a prompt
./DeepSpeed/deepspeed/model_implementations/transformers/ds_transformer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/model_implementations/transformers/ds_transformer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/model_implementations/transformers/ds_transformer.py: DeepSpeed Team
./DeepSpeed/deepspeed/model_implementations/transformers/ds_transformer.py:             TODO(arashb): 'layer_head_mask' and 'past_key_value' are only added to satisfy the OPT models API.
./DeepSpeed/deepspeed/model_implementations/transformers/ds_transformer.py:             This needs to be redesigned later!
./DeepSpeed/deepspeed/model_implementations/transformers/ds_transformer.py:         Allocate memory only on first layer forward
./DeepSpeed/deepspeed/model_implementations/transformers/ds_transformer.py:         We set the prev key/value to None when there is a prompt
./DeepSpeed/deepspeed/pipe/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/pipe/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/pipe/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/replace_policy.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/replace_policy.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/replace_policy.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/replace_policy.py: transformer-based policies
./DeepSpeed/deepspeed/module_inject/replace_policy.py: non-transformer-based policies
./DeepSpeed/deepspeed/module_inject/auto_tp.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/auto_tp.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/auto_tp.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/auto_tp.py: Automatic Tensor Parallelism
./DeepSpeed/deepspeed/module_inject/auto_tp.py:                 meta tensor cannot be casted or copied to, so we need to replace it with a normal tensor here
./DeepSpeed/deepspeed/module_inject/auto_tp.py:                     meta tensor cannot be casted or copied to, so we need to replace it with a normal tensor here
./DeepSpeed/deepspeed/module_inject/auto_tp.py:                     meta tensor cannot be casted or copied to, so we need to replace it with a normal tensor here
./DeepSpeed/deepspeed/module_inject/auto_tp.py:                         meta tensor cannot be casted or copied to, so we need to replace it with a normal tensor here
./DeepSpeed/deepspeed/module_inject/auto_tp.py:                 if module already exists in policy, combine gems and remove duplicates
./DeepSpeed/deepspeed/module_inject/auto_tp.py:             instantiate a throw-away policy in order to populate the _orig_layer_class
./DeepSpeed/deepspeed/module_inject/auto_tp.py:             if conv_linear_layer [weight_shape[1], weight_shape[0] // mp_size]
./DeepSpeed/deepspeed/module_inject/auto_tp.py:             else [weight_shape[0], weight_shape[1] // mp_size]
./DeepSpeed/deepspeed/module_inject/auto_tp.py:             if conv_linear_layer [weight_shape[1], weight_shape[0] // mp_size]
./DeepSpeed/deepspeed/module_inject/auto_tp.py:             else [weight_shape[0] // mp_size, weight_shape[1]]
./DeepSpeed/deepspeed/module_inject/auto_tp.py:                for detecting fused type
./DeepSpeed/deepspeed/module_inject/auto_tp.py:                The copy is a regular copy, The shape of dst and src is the same
./DeepSpeed/deepspeed/module_inject/auto_tp.py:                 Added for falcon model support
./DeepSpeed/deepspeed/module_inject/auto_tp.py:                 Note: isinstance will account for class inheritance, child.__class__ does not
./DeepSpeed/deepspeed/module_inject/fusedqkv_utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/fusedqkv_utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/fusedqkv_utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/fusedqkv_utils.py:         codegen_mp_num defined in https://github.com/huggingface/transformers/blob/main/src/transformers/models/codegen/modeling_codegen.py
./DeepSpeed/deepspeed/module_inject/fusedqkv_utils.py:        input : [3*hidden_dim, hidden_dim](weight) or [3*hidden_dim](bias)
./DeepSpeed/deepspeed/module_inject/fusedqkv_utils.py:        num_mp_blocks : [codegen_mp_num, 3*hidden_dim/codegen_mp_num, :]
./DeepSpeed/deepspeed/module_inject/fusedqkv_utils.py:        input : [3*hidden_dim, hidden_dim](weight) or [3*hidden_dim](bias)
./DeepSpeed/deepspeed/module_inject/fusedqkv_utils.py:         suppose num_heads=n, q(n)_w means the n-th q head linear weight, the weight format are as following
./DeepSpeed/deepspeed/module_inject/fusedqkv_utils.py:         bloomtype: [q(1)_w,k(1)_w,v(1)_w,q(2)_w,k(2)_w,v(2)_w,...,q(n)_w,k(n)_w,v(n)_w]
./DeepSpeed/deepspeed/module_inject/fusedqkv_utils.py:         glmtype:  [q(1)_w, q(2)_w,...,q(n)_w,k(1)_w,k(2)_w,...,k(n)_w,v(1)_w,v(2)_w,...,v(n)_w]
./DeepSpeed/deepspeed/module_inject/fusedqkv_utils.py:         codegentype: [q(1)_w,q(2)_w,...,q(n/t)_w,k(1)_w,k(2)_w,...,k(n/t)_w,v(1)_2,v(2)_w,...v(n/t)_w,q(n/t+1)_w,...], where t is a const defined in model file.
./DeepSpeed/deepspeed/module_inject/policy.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/policy.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/policy.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/policy.py:     a static class variable containing the HuggingFace model configuration.
./DeepSpeed/deepspeed/module_inject/policy.py:     see e.g., transformers.models.opt.configuration_opt.OPTConfig
./DeepSpeed/deepspeed/module_inject/policy.py:             the type of activation function used in MLP
./DeepSpeed/deepspeed/module_inject/policy.py:             applies layer norm before attention if `pre_attn_norm` is set to True
./DeepSpeed/deepspeed/module_inject/policy.py:             this flag shows whether or not using prefix in loading the checkpoint
./DeepSpeed/deepspeed/module_inject/policy.py:             whether or not the qkv is stored in the split-format
./DeepSpeed/deepspeed/module_inject/policy.py:             Type of normalization to perform
./DeepSpeed/deepspeed/module_inject/policy.py: TODO (lekurile): This function exists in base container as well, consolidate as some point
./DeepSpeed/deepspeed/module_inject/policy.py: TODO (lekurile): This function exists in megatron feature container as well, consolidate as some point
./DeepSpeed/deepspeed/module_inject/policy.py: This checks if the parameter exits in the checkpoint file and maybe copies it into the corresponding destination tensor.
./DeepSpeed/deepspeed/module_inject/policy.py: Note that not all parameters are saved in one checkpoint, that's why we always need to check if they exist!
./DeepSpeed/deepspeed/module_inject/policy.py: Extending the maybe_copy function for when the q, k, and v are in separate parameters!
./DeepSpeed/deepspeed/module_inject/policy.py: Extending the `maybe_copy` function for when mlp1 is in separate parameters for GeGLU
./DeepSpeed/deepspeed/module_inject/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/tp_shard.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/tp_shard.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/tp_shard.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/tp_shard.py:     When we have num_kv_heads defined, uneven division is possible, otherwise enforce even division
./DeepSpeed/deepspeed/module_inject/inject.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/inject.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/inject.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/inject.py:             copy relevant state from child -> new module
./DeepSpeed/deepspeed/module_inject/inject.py:    base_model = LinearStack()
./DeepSpeed/deepspeed/module_inject/inject.py:    base_model.eval()
./DeepSpeed/deepspeed/module_inject/inject.py:    test_model.eval()
./DeepSpeed/deepspeed/module_inject/inject.py:    test_input = torch.rand(1, base_model.input_dim)
./DeepSpeed/deepspeed/module_inject/inject.py:    base_output = base_model(test_input)
./DeepSpeed/deepspeed/module_inject/inject.py:    test_output = test_model(test_input)
./DeepSpeed/deepspeed/module_inject/inject.py:    
./DeepSpeed/deepspeed/module_inject/inject.py:    assert torch.allclose(base_output, test_output, atol=3e-8)
./DeepSpeed/deepspeed/module_inject/replace_module.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/replace_module.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/replace_module.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/replace_module.py:        replace_transformer_layer(None,
./DeepSpeed/deepspeed/module_inject/replace_module.py:                                  module.text_encoder,
./DeepSpeed/deepspeed/module_inject/replace_module.py:                                  training=False,
./DeepSpeed/deepspeed/module_inject/replace_module.py:                                  replace_with_kernel_inject=True,
./DeepSpeed/deepspeed/module_inject/replace_module.py:                                  triangular_masking=True,
./DeepSpeed/deepspeed/module_inject/replace_module.py:                                  max_out_tokens=8192)
./DeepSpeed/deepspeed/module_inject/replace_module.py:     defining globals as internally defined functions inherit these everywhere
./DeepSpeed/deepspeed/module_inject/replace_module.py:     todo: Refactor later. In future, let's minimize the style used above and use config.** instead
./DeepSpeed/deepspeed/module_inject/replace_module.py:             policy says cuda graph is not supported raise an error if set
./DeepSpeed/deepspeed/module_inject/replace_module.py:         1. Create a model-specific container object using the policy object.
./DeepSpeed/deepspeed/module_inject/replace_module.py:         2. Set the tensor parallelism config
./DeepSpeed/deepspeed/module_inject/replace_module.py:         3. Initialize tensors
./DeepSpeed/deepspeed/module_inject/replace_module.py:         4. deal with data types -- needs refactor to use dtype instead of fp16
./DeepSpeed/deepspeed/module_inject/replace_module.py:         5. Set the quantization config
./DeepSpeed/deepspeed/module_inject/replace_module.py:         6. create a DS Inference config object
./DeepSpeed/deepspeed/module_inject/replace_module.py:         7. use the config and create the module
./DeepSpeed/deepspeed/module_inject/replace_module.py:         8. transpose the weights and bias if needed
./DeepSpeed/deepspeed/module_inject/replace_module.py:         9. deal with tensor parallelism.
./DeepSpeed/deepspeed/module_inject/replace_module.py:         10. copy the tensors from the model-specific container to the new module
./DeepSpeed/deepspeed/module_inject/replace_module.py:         11. set global for generic checkpoint loading
./DeepSpeed/deepspeed/module_inject/replace_module.py:        mp_replace = ReplaceWithTensorSlicing(mp_group=config.tensor_parallel.tp_group)
./DeepSpeed/deepspeed/module_inject/replace_module.py:         1. Create AutoTP object
./DeepSpeed/deepspeed/module_inject/replace_module.py:         2. Set the tensor parallelism config
./DeepSpeed/deepspeed/module_inject/replace_module.py:         3. Try to get num_key_heads from model_config.num_key_value_heads
./DeepSpeed/deepspeed/module_inject/replace_module.py:         4. When we have num_kv_heads defined, uneven division is possible, otherwise enforce even division
./DeepSpeed/deepspeed/module_inject/replace_module.py:         5. Set linear policies
./DeepSpeed/deepspeed/module_inject/replace_module.py:         6. Replace modules
./DeepSpeed/deepspeed/module_inject/replace_module.py:             copy relevant state from child -> new module
./DeepSpeed/deepspeed/module_inject/replace_module.py:             copy relevant state from child -> new module
./DeepSpeed/deepspeed/module_inject/replace_module.py:         enable tensor parallel for the last linear
./DeepSpeed/deepspeed/module_inject/replace_module.py:         AutoTP shard loading
./DeepSpeed/deepspeed/module_inject/replace_module.py:        from turing.nvidia_modelingpreln import BertLayer
./DeepSpeed/deepspeed/module_inject/replace_module.py:         copy relevant state from child -> original module
./DeepSpeed/deepspeed/module_inject/replace_module.py:             instantiate a throw-away policy in order to populate the _orig_layer_class
./DeepSpeed/deepspeed/module_inject/replace_module.py:     if keys start with 'model.', don't skip level 0 prefix
./DeepSpeed/deepspeed/module_inject/replace_module.py:     Add the reset_cache func to the model, so that it can be called in the beginning of text-generation.
./DeepSpeed/deepspeed/module_inject/utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/utils.py: helper function to map between DS policies and DS containers
./DeepSpeed/deepspeed/module_inject/auto_tp_model_utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/auto_tp_model_utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/auto_tp_model_utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/auto_tp_model_utils.py:     Note: alibi will added to the attention bias that will be applied to the query, key product of attention
./DeepSpeed/deepspeed/module_inject/auto_tp_model_utils.py:     => therefore alibi will have to be of shape (batch_size, num_heads, query_length, key_length)
./DeepSpeed/deepspeed/module_inject/auto_tp_model_utils.py:     => here we set (batch_size=1, num_heads=num_heads, query_length=1, key_length=max_length)
./DeepSpeed/deepspeed/module_inject/auto_tp_model_utils.py:     => the query_length dimension will then be broadcasted correctly
./DeepSpeed/deepspeed/module_inject/auto_tp_model_utils.py:     This is more or less identical to T5's relative position bias:
./DeepSpeed/deepspeed/module_inject/auto_tp_model_utils.py:     https://github.com/huggingface/transformers/blob/f681437203baa7671de3174b0fa583c349d9d5e1/src/transformers/models/t5/modeling_t5.py#L527
./DeepSpeed/deepspeed/module_inject/load_checkpoint.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/load_checkpoint.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/load_checkpoint.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/load_checkpoint.py:         if keys start with 'model.' or 'transformer.', don't skip level 0 prefix
./DeepSpeed/deepspeed/module_inject/load_checkpoint.py:             OPT models
./DeepSpeed/deepspeed/module_inject/load_checkpoint.py:             BLOOM models
./DeepSpeed/deepspeed/module_inject/load_checkpoint.py:                 meta tensor cannot be casted or copied to, so we need to replace it with a normal tensor here
./DeepSpeed/deepspeed/module_inject/load_checkpoint.py:                             set the quantizer number of groups using the checkpoint scale shape
./DeepSpeed/deepspeed/module_inject/load_checkpoint.py:                                     Check if the weight tensor is for the QKV parameter
./DeepSpeed/deepspeed/module_inject/load_checkpoint.py:                                     Check if the weight tensor is for the QKV parameter
./DeepSpeed/deepspeed/module_inject/containers/gpt2.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/gpt2.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/gpt2.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/gpt2.py:         All model specific things should be defined here instead of the base class.
./DeepSpeed/deepspeed/module_inject/containers/gpt2.py:         HuggingFace GPT2 uses convolutional layer instead of linear layer
./DeepSpeed/deepspeed/module_inject/containers/unet.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/unet.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/unet.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/unet.py:         TODO(cmikeh2): Enable cuda graph should be an inference configuration
./DeepSpeed/deepspeed/module_inject/containers/unet.py:            return None
./DeepSpeed/deepspeed/module_inject/containers/unet.py:            kvw = Parameter(torch.cat((kw, vw), dim=0), requires_grad=False)
./DeepSpeed/deepspeed/module_inject/containers/features/split_qkv.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/features/split_qkv.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/features/split_qkv.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/features/split_qkv.py:         Only need to alter
./DeepSpeed/deepspeed/module_inject/containers/features/split_qkv.py:             In initialize_tensors, we create a fused qkvw with the appropriate shape
./DeepSpeed/deepspeed/module_inject/containers/features/split_qkv.py:             and copy the qw, qb, kw, kb, vw, vb into it
./DeepSpeed/deepspeed/module_inject/containers/features/split_qkv.py:             We reset the data for qw (which is the original model parameter) to point
./DeepSpeed/deepspeed/module_inject/containers/features/split_qkv.py:             to the fused weight matrix we have created here
./DeepSpeed/deepspeed/module_inject/containers/features/split_qkv.py:             Assume if one of the biases is not None, then all of them are not None
./DeepSpeed/deepspeed/module_inject/containers/features/split_qkv.py:             In ZeRO-3 this will be managed by ZeRO and handled separately in the
./DeepSpeed/deepspeed/module_inject/containers/features/split_qkv.py:             forward of ds_attention
./DeepSpeed/deepspeed/module_inject/containers/features/hybrid_engine.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/features/hybrid_engine.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/features/hybrid_engine.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/features/hybrid_engine.py:         Setup the new Attention module
./DeepSpeed/deepspeed/module_inject/containers/features/hybrid_engine.py:         Setup the new MLP module
./DeepSpeed/deepspeed/module_inject/containers/features/hybrid_engine.py:         Apply weight quantization
./DeepSpeed/deepspeed/module_inject/containers/features/hybrid_engine.py:         TODO(cmikeh2): Re-enable this once verified
./DeepSpeed/deepspeed/module_inject/containers/features/hybrid_engine.py:        self.apply_weight_quantization()
./DeepSpeed/deepspeed/module_inject/containers/features/gated_mlp.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/features/gated_mlp.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/features/gated_mlp.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/features/gated_mlp.py:         Only need to alter behavior if we can't do the normal destructive copy
./DeepSpeed/deepspeed/module_inject/containers/features/gated_mlp.py:             In initialize_tensors, we create a fused inter projection with the appropriate shape
./DeepSpeed/deepspeed/module_inject/containers/features/gated_mlp.py:             and copy the up projection and gate projection into it
./DeepSpeed/deepspeed/module_inject/containers/features/meta_tensor.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/features/meta_tensor.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/features/meta_tensor.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/features/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/features/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/features/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/features/hybrid_megatron.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/features/hybrid_megatron.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/features/hybrid_megatron.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/features/hybrid_megatron.py:         If parameter is distributed, handle gathering it
./DeepSpeed/deepspeed/module_inject/containers/features/megatron.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/features/megatron.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/features/megatron.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/gptj.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/gptj.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/gptj.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/gptj.py:         All model specific things should be defined here instead of the base class.
./DeepSpeed/deepspeed/module_inject/containers/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/megatron_gpt_moe.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/megatron_gpt_moe.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/megatron_gpt_moe.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/megatron_gpt_moe.py:         All model specific things should be defined here instead of the base class.
./DeepSpeed/deepspeed/module_inject/containers/megatron_gpt_moe.py: TODO: Megatron GPT MoE inherits from Megatron policy and replaces mlp
./DeepSpeed/deepspeed/module_inject/containers/megatron_gpt_moe.py: TODO: Generalize MoE overall goal, expand beyond Megatron
./DeepSpeed/deepspeed/module_inject/containers/megatron_gpt_moe.py:         we use megatron version to differentiate between the old and new
./DeepSpeed/deepspeed/module_inject/containers/megatron_gpt_moe.py:         megatron-lm source code
./DeepSpeed/deepspeed/module_inject/containers/megatron_gpt_moe.py:         for now, all of this is tightly coupled to megatron-deepspeed moe implementation
./DeepSpeed/deepspeed/module_inject/containers/megatron_gpt_moe.py:         todo: think and refactor this to be more general
./DeepSpeed/deepspeed/module_inject/containers/megatron_gpt_moe.py:        from deepspeed.moe.utils import has_moe_layers
./DeepSpeed/deepspeed/module_inject/containers/megatron_gpt_moe.py:        moe, _ = has_moe_layers(self.client_module)
./DeepSpeed/deepspeed/module_inject/containers/opt.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/opt.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/opt.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/opt.py:         All model specific things should be defined here instead of the base class.
./DeepSpeed/deepspeed/module_inject/containers/base_moe.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/base_moe.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/base_moe.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/base_moe.py: Create a container object to save model-specific tensors using the policy file above.
./DeepSpeed/deepspeed/module_inject/containers/base_moe.py:         Call the init function of the parent class to initialize the tensors and configs from parent class
./DeepSpeed/deepspeed/module_inject/containers/base_moe.py:         MoE models will have a list of mlp related tensors
./DeepSpeed/deepspeed/module_inject/containers/base_moe.py:         Residual MoE needs extra parameters
./DeepSpeed/deepspeed/module_inject/containers/base_moe.py:         Set the tensors from policy (user module) to container (DS module)
./DeepSpeed/deepspeed/module_inject/containers/base_moe.py:         setup the new Attention module
./DeepSpeed/deepspeed/module_inject/containers/base_moe.py:         quantize attention weights
./DeepSpeed/deepspeed/module_inject/containers/base_moe.py:         setup the new MLP module
./DeepSpeed/deepspeed/module_inject/containers/base_moe.py:             mlp inter
./DeepSpeed/deepspeed/module_inject/containers/base_moe.py:             mlp output
./DeepSpeed/deepspeed/module_inject/containers/llama.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/llama.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/llama.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/llama.py:         All model specific things should be defined here instead of the base class.
./DeepSpeed/deepspeed/module_inject/containers/llama.py:         This line is necessary for proper output when kernels + meta tensors are used in Llama models
./DeepSpeed/deepspeed/module_inject/containers/llama.py:         TODO: Investigate root-cause and fix meta tensor loading
./DeepSpeed/deepspeed/module_inject/containers/gptneo.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/gptneo.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/gptneo.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/gptneo.py:         All model specific things should be defined here instead of the base class.
./DeepSpeed/deepspeed/module_inject/containers/megatron_gpt.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/megatron_gpt.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/megatron_gpt.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/megatron_gpt.py:         All model specific things should be defined here instead of the base class.
./DeepSpeed/deepspeed/module_inject/containers/megatron_gpt.py: TODO: Megatron GPT MoE inherits from Megatron policy and replaces mlp
./DeepSpeed/deepspeed/module_inject/containers/megatron_gpt.py: TODO: Generalize MoE overall goal, expand beyond Megatron
./DeepSpeed/deepspeed/module_inject/containers/megatron_gpt.py:         we use megatron version to differentiate between the old and new
./DeepSpeed/deepspeed/module_inject/containers/megatron_gpt.py:         megatron-lm source code
./DeepSpeed/deepspeed/module_inject/containers/bert.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/bert.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/bert.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/bert.py:         All model specific things should be defined here instead of the base class.
./DeepSpeed/deepspeed/module_inject/containers/clip.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/clip.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/clip.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/clip.py:         All model specific things should be defined here instead of the base class.
./DeepSpeed/deepspeed/module_inject/containers/internlm.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/internlm.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/internlm.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/internlm.py:         All model specific things should be defined here instead of the base class.
./DeepSpeed/deepspeed/module_inject/containers/llama2.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/llama2.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/llama2.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/llama2.py:         All model specific things should be defined here instead of the base class.
./DeepSpeed/deepspeed/module_inject/containers/gptneox.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/gptneox.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/gptneox.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/gptneox.py:         All model specific things should be defined here instead of the base class.
./DeepSpeed/deepspeed/module_inject/containers/base.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/base.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/base.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/base.py: Create a container object to save model-specific tensors using the policy file above.
./DeepSpeed/deepspeed/module_inject/containers/base.py: If the intermediate size attribute is set DEFAULT_INTERMEDIATE_SIZE
./DeepSpeed/deepspeed/module_inject/containers/base.py: it is assumed the intermediate size is 4x the embedding dimension
./DeepSpeed/deepspeed/module_inject/containers/base.py:     not implemented
./DeepSpeed/deepspeed/module_inject/containers/base.py:         configuration for models. todo: can this be moved to a pydantic model config?
./DeepSpeed/deepspeed/module_inject/containers/base.py:         Attention tensors
./DeepSpeed/deepspeed/module_inject/containers/base.py:         MLP tensors
./DeepSpeed/deepspeed/module_inject/containers/base.py:         LayerNorm tensors
./DeepSpeed/deepspeed/module_inject/containers/base.py:         Triton
./DeepSpeed/deepspeed/module_inject/containers/base.py:         Set the tensors from policy (user module) to container (DS module)
./DeepSpeed/deepspeed/module_inject/containers/base.py:        self.check_meta_tensor_support()
./DeepSpeed/deepspeed/module_inject/containers/base.py:         Note: converting tensors to fp16 requires that we do it in-place using self.__dict__ and not make a list/dict copy
./DeepSpeed/deepspeed/module_inject/containers/base.py:                 The list comprehension is used for MoE tensor lists
./DeepSpeed/deepspeed/module_inject/containers/base.py:         quantize attention weights
./DeepSpeed/deepspeed/module_inject/containers/base.py:         quantize mlp weights
./DeepSpeed/deepspeed/module_inject/containers/base.py:         setup the new Attention module
./DeepSpeed/deepspeed/module_inject/containers/base.py:         setup the new MLP module
./DeepSpeed/deepspeed/module_inject/containers/base.py:         Apply weight quantization
./DeepSpeed/deepspeed/module_inject/containers/base.py:         TODO(cmikeh2): Re-enable this once verified
./DeepSpeed/deepspeed/module_inject/containers/base.py:        self.apply_weight_quantization()
./DeepSpeed/deepspeed/module_inject/containers/vae.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/vae.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/vae.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/vae.py:                 Diffusers >= 0.12.0 changes location of AutoencoderKL
./DeepSpeed/deepspeed/module_inject/containers/vae.py:         TODO(cmikeh2): Enable cuda graph should be an inference configuration
./DeepSpeed/deepspeed/module_inject/containers/vae.py:     NOTE (lekurile): Should we have a diffusers policy class?
./DeepSpeed/deepspeed/module_inject/containers/distil_bert.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/distil_bert.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/distil_bert.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/distil_bert.py:         All model specific things should be defined here instead of the base class.
./DeepSpeed/deepspeed/module_inject/containers/bloom.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/containers/bloom.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/containers/bloom.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/containers/bloom.py:         All model specific things should be defined here instead of the base class.
./DeepSpeed/deepspeed/module_inject/layers.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/layers.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/layers.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/layers.py:         OPT is set up so that if padding_idx is specified then offset the embedding ids by 2
./DeepSpeed/deepspeed/module_inject/layers.py:         and adjust num_embeddings appropriately. Other models don't have this hack
./DeepSpeed/deepspeed/module_inject/layers.py:         create positions depending on attention_mask
./DeepSpeed/deepspeed/module_inject/layers.py:         cut positions if `past_key_values_length` is > 0
./DeepSpeed/deepspeed/module_inject/module_quantize.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/module_inject/module_quantize.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/module_inject/module_quantize.py: DeepSpeed Team
./DeepSpeed/deepspeed/module_inject/module_quantize.py:             Quantize megatron GPT2 / GPT3 trained model
./DeepSpeed/deepspeed/module_inject/module_quantize.py:             Quantize either DeepSpeed or HuggingFace trained model
./DeepSpeed/deepspeed/monitor/csv_monitor.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/monitor/csv_monitor.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/monitor/csv_monitor.py: DeepSpeed Team
./DeepSpeed/deepspeed/monitor/csv_monitor.py:             NOTE: This code path currently is never used since the default tensorboard_output_path is an empty string and not None. Saving it in case we want this functionality in the future.
./DeepSpeed/deepspeed/monitor/csv_monitor.py:             We assume each event_list element is a tensorboard-style tuple in the format: (log_name: String, value, step: Int)
./DeepSpeed/deepspeed/monitor/csv_monitor.py:                 Set the header to the log_name
./DeepSpeed/deepspeed/monitor/csv_monitor.py:                 Need this check because the deepspeed engine currently formats log strings to separate with '/'
./DeepSpeed/deepspeed/monitor/csv_monitor.py:                 sanitize common naming conventions into filename
./DeepSpeed/deepspeed/monitor/csv_monitor.py:                 Open file and record event. Insert header if this is the first time writing
./DeepSpeed/deepspeed/monitor/config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/monitor/config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/monitor/config.py: DeepSpeed Team
./DeepSpeed/deepspeed/monitor/monitor.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/monitor/monitor.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/monitor/monitor.py: DeepSpeed Team
./DeepSpeed/deepspeed/monitor/wandb.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/monitor/wandb.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/monitor/wandb.py: DeepSpeed Team
./DeepSpeed/deepspeed/monitor/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/monitor/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/monitor/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/monitor/utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/monitor/utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/monitor/utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/monitor/utils.py:         torch.utils.tensorboard will fail if `tensorboard` is not available,
./DeepSpeed/deepspeed/monitor/utils.py:         see their docs for more details: https://pytorch.org/docs/1.8.0/tensorboard.html
./DeepSpeed/deepspeed/monitor/tensorboard.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/monitor/tensorboard.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/monitor/tensorboard.py: DeepSpeed Team
./DeepSpeed/deepspeed/monitor/tensorboard.py:             NOTE: This code path currently is never used since the default output_path is an empty string and not None. Saving it in case we want this functionality in the future.
./DeepSpeed/deepspeed/checkpoint/deepspeed_checkpoint.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/checkpoint/deepspeed_checkpoint.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/checkpoint/deepspeed_checkpoint.py: DeepSpeed Team
./DeepSpeed/deepspeed/checkpoint/deepspeed_checkpoint.py:         XXX: this is not guaranteed
./DeepSpeed/deepspeed/checkpoint/deepspeed_checkpoint.py:        print(f"{transformer_layer_keys} {layers_per_pp}")
./DeepSpeed/deepspeed/checkpoint/deepspeed_checkpoint.py:         XXX: fix me - isn't always the case
./DeepSpeed/deepspeed/checkpoint/deepspeed_checkpoint.py:         only true with  --pp-partition-method 'type:transformer|embedding' \
./DeepSpeed/deepspeed/checkpoint/deepspeed_checkpoint.py:         assert (len(self.layer_keys) - 2) % self.pp_degree == 0
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py: DeepSpeed Team
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py:     Build the data-parallel groups.
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py:     Build the model-parallel groups.
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py:     Build the tensor model-parallel groups.
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py:     # Build the pipeline model-parallel groups and embedding groups
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py:     # (first and last rank in each pipeline model-parallel group).
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py:     for i in range(num_pipeline_model_parallel_groups):
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py:         ranks = range(i, world_size,
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py:                       num_pipeline_model_parallel_groups)
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py:         print(f"EMB{i}", list(ranks))
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py:     handle tp contraction first
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py:     handle pp contraction next
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py: easy
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py:reshape([2,2,1],[1,1,1])
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py: probably need more logic to suggest how to pack
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py:reshape([4,4,1],[2,2,1])
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py:reshape([2,4,2], [8,32,1])
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py: get_mpu_ranks(2,2,2)
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py: get_mpu_ranks(4,2,1)
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py: get_mpu_ranks(2,4,1)
./DeepSpeed/deepspeed/checkpoint/reshape_meg_2d.py: get_mpu_ranks(1,1,8)
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py: DeepSpeed Team
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:         need to deal with slices that were averaged.
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:         the opposite of averaging here becomes an exact copy of the first slice
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:         I thought of 2 ways:
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:         implementation a. find a way for a client to pass a dict with patterns
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:         if any(re.search(pattern, folder) for pattern in WEIGHTS_TO_AVERAGE_PATTERNS):
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:             tp_rank = 0
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:             tp_world_size = 1
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:         the other approach is to assume that the saved data is correct and if full_hp_param.shape ==
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:         self.shape that means we automatically copy?
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:         implementation b.
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:         this version requires no additional data passed from the client
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:         if the shapes already match it must be slices that were averaged - so we just hack around those
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:         special case for word_embeddings weights which get padded differently depending on TP degree.
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:         the converter to universal currently strips the original padding completely so the saved
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:         weight is padding-free and we just need to add new padding depending on the target TP
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:         degree
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:             In the absence of data passed from the user wrt new padded vocab specific to tp degree
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:             we can again derive that data by reverse engineering the target shapes like so:
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:                if key == FP32_WEIGHT_KEY and 'word_embeddings.weight' in folder:
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:                    print_rank_0(f'{full_hp_param[:10]=}', force=True)
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:                print(f"{full_hp_param.shape=} {full_param_numel=} {folder=}")
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:                print(f"{dst_tensor.shape=} {dst_tensor.numel()=}{folder=}")
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:         since when we do many to 1 on tp we cat sometimes on dim=0 and other times on dim=1 we have to do exactly the same in reverse
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:         special case is when a single parameter is effectively a container for multiple sub parameters
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:         (more details at PARAM_N_SUB_PARAMS definition)
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:             this performs the opposite of cat when merging TP slices
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:                print(f"{key} SHAPE: {tp_hp_slice.shape=}")
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:                print(f"{key} SHAPE: {dst_tensor.shape=}")
./DeepSpeed/deepspeed/checkpoint/universal_checkpoint.py:                print(f"{key} SHAPE: {tp_hp_fragment.shape=}")
./DeepSpeed/deepspeed/checkpoint/constants.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/checkpoint/constants.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/checkpoint/constants.py: DeepSpeed Team
./DeepSpeed/deepspeed/checkpoint/constants.py:########################################
./DeepSpeed/deepspeed/checkpoint/constants.py: Optimizer checkpoint keys
./DeepSpeed/deepspeed/checkpoint/constants.py:########################################
./DeepSpeed/deepspeed/checkpoint/constants.py:########################################
./DeepSpeed/deepspeed/checkpoint/constants.py: Module checkpoint keys
./DeepSpeed/deepspeed/checkpoint/constants.py:########################################
./DeepSpeed/deepspeed/checkpoint/constants.py:########################################
./DeepSpeed/deepspeed/checkpoint/constants.py: Checkpoint naming constants
./DeepSpeed/deepspeed/checkpoint/constants.py:########################################
./DeepSpeed/deepspeed/checkpoint/constants.py:########################################
./DeepSpeed/deepspeed/checkpoint/constants.py: Checkpoint utility keys
./DeepSpeed/deepspeed/checkpoint/constants.py:########################################
./DeepSpeed/deepspeed/checkpoint/constants.py:########################################
./DeepSpeed/deepspeed/checkpoint/constants.py: Universal Checkpoint keys
./DeepSpeed/deepspeed/checkpoint/constants.py:########################################
./DeepSpeed/deepspeed/checkpoint/constants.py: Reserve version 0.1  for the hardcoded logic used in BLOOM-176B training
./DeepSpeed/deepspeed/checkpoint/constants.py: Vocabulary padding
./DeepSpeed/deepspeed/checkpoint/constants.py: Parameter splitting/merging
./DeepSpeed/deepspeed/checkpoint/constants.py: Following is a special case where a parameter effectively contains sub parameters.
./DeepSpeed/deepspeed/checkpoint/constants.py: As an example, consider Megatron-DeepSpeed GPT SWIGLU implementation (mlp.h_to_4h).
./DeepSpeed/deepspeed/checkpoint/constants.py: In this case, a single parameter ia allocated contiguously, but used as separate parameters.
./DeepSpeed/deepspeed/checkpoint/constants.py: When using universal checkpoint, we have to normalize the representation of the full parameter.
./DeepSpeed/deepspeed/checkpoint/constants.py: We normalize it by concatenating all slices of the sub params and then concatenating the sub params.
./DeepSpeed/deepspeed/checkpoint/constants.py: All concat operations are done on CAT_DIM (currently, no support for different concat dims sub params and TP slicing).
./DeepSpeed/deepspeed/checkpoint/constants.py: Similarly, load_hp_checkpoint_state has to take the needed actions when loading from universal.
./DeepSpeed/deepspeed/checkpoint/constants.py: Regex list of parameters that require special handling
./DeepSpeed/deepspeed/checkpoint/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/checkpoint/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/checkpoint/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/checkpoint/reshape_3d_utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/checkpoint/reshape_3d_utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/checkpoint/reshape_3d_utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/checkpoint/utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/checkpoint/utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/checkpoint/utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/checkpoint/utils.py: We pass cloned tensors to torch.save() to avoid checkpoint bloat that occurs when torch.save()
./DeepSpeed/deepspeed/checkpoint/utils.py: saves the underlying storage rather than the slice of the storage corresponding to individual tensors.
./DeepSpeed/deepspeed/checkpoint/utils.py: This is a problem in DeepSpeed because we often allocate tensors using slices of large flattened buffers.
./DeepSpeed/deepspeed/checkpoint/utils.py: Tensor cloning helps to avoid this problem because the storage of cloned tensors are closer to the true size.
./DeepSpeed/deepspeed/checkpoint/utils.py: It is expected that the garbage collector will reclaim the cloned tensor storage to avoid memory bloat.
./DeepSpeed/deepspeed/checkpoint/utils.py: See https://pytorch.org/docs/stable/notes/serialization.html#preserve-storage-sharing
./DeepSpeed/deepspeed/checkpoint/reshape_utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/checkpoint/reshape_utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/checkpoint/reshape_utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:!/usr/bin/env python
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py: DeepSpeed Team
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py: from pprint import pprint
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:     pprint(f"Processing {dp_index=} {pp_index=}, {tp_index=}")
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:     print(f'{pipeline_replicated_params=}')
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:     dict
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:     list
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:                 Skip tied weights that are replicated in first and last pp stages
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:             pprint(f"dpt{dp_index}{pp_index}{tp_index} {param_group_id} {name} => {fragment_mapping.start}:{fragment_mapping.numel}")
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:    print(f"{param_name}: {offset}: {numel} => {path}")
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:        print(f"Expected shape: {shape}")
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:        print(f"Fragment sizes:", list(frag.shape for frag in slices))
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:             print(f'replicate {name} using first slice')
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:             print(f'merge {name} using average')
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:             print(f"merge {name} with CAT DIM: {cat_dim}")
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:            print(f"Before {param.shape=}")
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:             strip padding
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:            print(f"After {param.shape=}")
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:        print(f"Final shape: {param.shape}")
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:     pprint(f'{_3d_range_list=}')
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:     pprint(f'{work_chunks=}')
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:     extract_zero_shards(temp_dir, ds_checkpoint, _3d_range_list[0])
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:    pprint(work_chunks)
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:     verify that all patterns were used
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:     if a pattern was not used by any of the workers, then it was not used at all -> assert/alert
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:    _create_latest_file(args.output_folder, iteration)
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:     fix back to normal flat dict, merge duplicates for tp>1
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:     Copy mp* files into output folder
./DeepSpeed/deepspeed/checkpoint/ds_to_universal.py:     Update latest to output folder
./DeepSpeed/deepspeed/checkpoint/zero_checkpoint.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/checkpoint/zero_checkpoint.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/checkpoint/zero_checkpoint.py: DeepSpeed Team
./DeepSpeed/deepspeed/nebula/config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/nebula/config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/nebula/config.py: DeepSpeed Team
./DeepSpeed/deepspeed/nebula/constants.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/nebula/constants.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/nebula/constants.py: DeepSpeed Team
./DeepSpeed/deepspeed/nebula/constants.py:########################################
./DeepSpeed/deepspeed/nebula/constants.py: nebula
./DeepSpeed/deepspeed/nebula/constants.py:########################################
./DeepSpeed/deepspeed/nebula/constants.py: Nebula. By default, this feature is not enabled.
./DeepSpeed/deepspeed/nebula/constants.py: Users can configure in ds_config.json as below example:
./DeepSpeed/deepspeed/nebula/constants.py: There is a case where customer want to load the checkpoint saved
./DeepSpeed/deepspeed/nebula/constants.py: by raw torch. Because nebula cannot load torch checkpoint directly
./DeepSpeed/deepspeed/nebula/constants.py: as they have different folder structures to bring the gap for
./DeepSpeed/deepspeed/nebula/constants.py: loading(the data are totally same in bytes for torch and nebula
./DeepSpeed/deepspeed/nebula/constants.py: saving).
./DeepSpeed/deepspeed/nebula/constants.py: In this case, we must disable nebula load to use raw torch load.
./DeepSpeed/deepspeed/nebula/constants.py: Customer can just set NEBULA_ENABLE_NEBULA_LOAD to False. Then use
./DeepSpeed/deepspeed/nebula/constants.py: original way of deepspeed to load, i.e. set the value of "--load".
./DeepSpeed/deepspeed/nebula/constants.py: When you want to resume the previous checkpoint saved by nebula,
./DeepSpeed/deepspeed/nebula/constants.py: you can set NEBULA_LOAD_PATH as the parent folder of checkpoint.
./DeepSpeed/deepspeed/nebula/constants.py: If NEBULA_LOAD_PATH is None, the NEBULA_PERSISTENT_STORAGE_PATH
./DeepSpeed/deepspeed/nebula/constants.py: will be the default path to load.
./DeepSpeed/deepspeed/nebula/constants.py: Nebula will save the checkpoint under NEBULA_LOAD_PATH in the
./DeepSpeed/deepspeed/nebula/constants.py: asynchronous way.
./DeepSpeed/deepspeed/nebula/constants.py: Time interval to trigger the nebula persistence.
./DeepSpeed/deepspeed/nebula/constants.py: Checkpoint number which will be kept in memory. Let us say,
./DeepSpeed/deepspeed/nebula/constants.py: if the value is 2. Then we have checkpoints 1 and 2 are ready
./DeepSpeed/deepspeed/nebula/constants.py: now. When it comes to checkpoint 3, the 1 will be removed if
./DeepSpeed/deepspeed/nebula/constants.py: 1 has been persisted to disk.
./DeepSpeed/deepspeed/nebula/constants.py: Nebula envs
./DeepSpeed/deepspeed/nebula/constants.py: ITP env files
./DeepSpeed/deepspeed/nebula/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/nebula/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/nebula/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/git_version_info.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/git_version_info.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/git_version_info.py: DeepSpeed Team
./DeepSpeed/deepspeed/git_version_info.py:      This is populated by setup.py
./DeepSpeed/deepspeed/git_version_info.py:         Will be missing from checkouts that haven't been installed (e.g., readthedocs)
./DeepSpeed/deepspeed/constants.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/constants.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/constants.py: DeepSpeed Team
./DeepSpeed/deepspeed/constants.py:############################################
./DeepSpeed/deepspeed/constants.py: Torch distributed constants
./DeepSpeed/deepspeed/constants.py:############################################
./DeepSpeed/deepspeed/constants.py: Default process group wide timeout, if applicable.
./DeepSpeed/deepspeed/constants.py: This only applies to the gloo and nccl backends
./DeepSpeed/deepspeed/constants.py: (only if NCCL_BLOCKING_WAIT or NCCL_ASYNC_ERROR_HANDLING is set to 1).
./DeepSpeed/deepspeed/constants.py: To make an attempt at backwards compatibility with THD, we use an
./DeepSpeed/deepspeed/constants.py: extraordinarily high default timeout, given that THD did not have timeouts.
./DeepSpeed/deepspeed/runtime/data_pipeline/curriculum_scheduler.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/data_pipeline/curriculum_scheduler.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/data_pipeline/curriculum_scheduler.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/data_pipeline/config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/data_pipeline/config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/data_pipeline/config.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/data_pipeline/config.py: TODO: Reducing config verbosity by returning None or {} when disabled.
./DeepSpeed/deepspeed/runtime/data_pipeline/config.py: One challenge is that we still need to somehow include the default values,
./DeepSpeed/deepspeed/runtime/data_pipeline/config.py: for example the *_ENABLED has default of false.
./DeepSpeed/deepspeed/runtime/data_pipeline/constants.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/data_pipeline/constants.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/data_pipeline/constants.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/data_pipeline/constants.py:########################################
./DeepSpeed/deepspeed/runtime/data_pipeline/constants.py: Data efficiency - Data Sampling
./DeepSpeed/deepspeed/runtime/data_pipeline/constants.py:########################################
./DeepSpeed/deepspeed/runtime/data_pipeline/constants.py:########################################
./DeepSpeed/deepspeed/runtime/data_pipeline/constants.py: Data efficiency - Data Sampling - Curriculum Learning
./DeepSpeed/deepspeed/runtime/data_pipeline/constants.py:########################################
./DeepSpeed/deepspeed/runtime/data_pipeline/constants.py:########################################
./DeepSpeed/deepspeed/runtime/data_pipeline/constants.py: Curriculum Learning legacy implementation
./DeepSpeed/deepspeed/runtime/data_pipeline/constants.py:########################################
./DeepSpeed/deepspeed/runtime/data_pipeline/constants.py:########################################
./DeepSpeed/deepspeed/runtime/data_pipeline/constants.py: Data efficiency - Data Routing
./DeepSpeed/deepspeed/runtime/data_pipeline/constants.py:########################################
./DeepSpeed/deepspeed/runtime/data_pipeline/constants.py:########################################
./DeepSpeed/deepspeed/runtime/data_pipeline/constants.py: Data efficiency - Data Routing - Random LTD
./DeepSpeed/deepspeed/runtime/data_pipeline/constants.py:########################################
./DeepSpeed/deepspeed/runtime/data_pipeline/constants.py: scheduler
./DeepSpeed/deepspeed/runtime/data_pipeline/constants.py: learning rate schedulers
./DeepSpeed/deepspeed/runtime/data_pipeline/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/data_pipeline/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/data_pipeline/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py: Copyright (c) Facebook, Inc. and its affiliates.
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py:
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py: This source code is licensed under the MIT license found in the
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py: LICENSE file in the root directory of this source tree.
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py: copied from fairseq/fairseq/data/indexed_dataset.py
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py: Removed IndexedRawTextDataset since it relied on Fairseq dictionary
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py: other slight modifications to remove fairseq dependencies
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py: Added document index to index file and made it accessible.
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py:    An empty sentence no longer separates documents.
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py: Some of the fixes/improvements are adopted from
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py: https://github.com/bigscience-workshop/Megatron-DeepSpeed/blob/main/megatron/data/indexed_dataset.py
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py:     @lru_cache(maxsize=8)
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py:             close and delete data file after prefetch so we can pickle
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py:     @lru_cache(maxsize=8)
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py:             Hack just to make this work, can optimizer later if necessary
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py:     given an array holding the result of an inclusive scan (cumsum),
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py:     convert to an exclusive scan (shift to the right)
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py:     [10, 30, 35, 50] --> [0, 10, 30, 35]
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py:     scale values in sizes array by elemsize to get sizes in bytes
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py:     get total number of bytes from all sizes (last element)
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py:     convert to byte offsets
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py:                     compute element sizes in bytes
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py:     @lru_cache(maxsize=8)
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py:         Concatenate index
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py:         Concatenate data
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/data_analyzer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/data_analyzer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/data_analyzer.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/data_analyzer.py:                 sample_to_metric
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/data_analyzer.py:                 metric_to_sample
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/data_sampler.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/data_sampler.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/data_sampler.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/data_sampler.py:         Keep a copy of input params for later use.
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/data_sampler.py:         Sanity checks.
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/data_sampler.py:         Backward compatibility: previously data_cluster_paths were stored as
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/data_sampler.py:         absolute paths. Now we changed it to just the file name so that even
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/data_sampler.py:         if user moved the cluster files, the checkpoint loading still works
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/data_sampler.py:         as long as user set the correct new CURRICULUM_LEARNING_CLUSTER_PATH
./DeepSpeed/deepspeed/runtime/data_pipeline/data_sampling/data_sampler.py:         in deepspeed json config.
./DeepSpeed/deepspeed/runtime/data_pipeline/data_routing/basic_layer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/data_pipeline/data_routing/basic_layer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/data_pipeline/data_routing/basic_layer.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/data_pipeline/data_routing/basic_layer.py:####based on the paper random-ltd: https://arxiv.org/abs/2211.11586
./DeepSpeed/deepspeed/runtime/data_pipeline/data_routing/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/data_pipeline/data_routing/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/data_pipeline/data_routing/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/data_pipeline/data_routing/utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/data_pipeline/data_routing/utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/data_pipeline/data_routing/utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/data_pipeline/data_routing/utils.py:     random-layer-token-drop
./DeepSpeed/deepspeed/runtime/data_pipeline/data_routing/helper.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/data_pipeline/data_routing/helper.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/data_pipeline/data_routing/helper.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/data_pipeline/data_routing/scheduler.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/data_pipeline/data_routing/scheduler.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/data_pipeline/data_routing/scheduler.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/data_pipeline/data_routing/scheduler.py: from deepspeed.runtime.lr_schedules import WarmupLR
./DeepSpeed/deepspeed/runtime/data_pipeline/data_routing/scheduler.py:####based on the paper random-ltd: https://arxiv.org/abs/2211.11586
./DeepSpeed/deepspeed/runtime/data_pipeline/data_routing/scheduler.py:         self.first_step = True
./DeepSpeed/deepspeed/runtime/hybrid_engine.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/hybrid_engine.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/hybrid_engine.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/hybrid_engine.py:         synch seed between all GPUs
./DeepSpeed/deepspeed/runtime/hybrid_engine.py:         inference containers / fwds
./DeepSpeed/deepspeed/runtime/hybrid_engine.py:         Performance stats
./DeepSpeed/deepspeed/runtime/hybrid_engine.py:                 TODO(cmikeh2) Evaluate if this can be deferred when release_inference_cache
./DeepSpeed/deepspeed/runtime/hybrid_engine.py:                 is enabled.
./DeepSpeed/deepspeed/runtime/hybrid_engine.py:                         mp_group is used for broader collective
./DeepSpeed/deepspeed/runtime/hybrid_engine.py:                         mp_replace is used for container tensor slicing
./DeepSpeed/deepspeed/runtime/hybrid_engine.py:                     Use the is_lora_fused flag to prevent multiple fusion in Z3 with non-pinned memory
./DeepSpeed/deepspeed/runtime/hybrid_engine.py:                     Set the is_lora_fused to true when reaching the last layer
./DeepSpeed/deepspeed/runtime/pipe/p2p.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/pipe/p2p.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/pipe/p2p.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/pipe/p2p.py: To query whether we have send/recv support
./DeepSpeed/deepspeed/runtime/pipe/p2p.py:initializes adjacent process groups
./DeepSpeed/deepspeed/runtime/pipe/p2p.py:run this only after deepspeed.init_distributed() has been called
./DeepSpeed/deepspeed/runtime/pipe/p2p.py:     serialize the message
./DeepSpeed/deepspeed/runtime/pipe/p2p.py:     construct a tensor to send
./DeepSpeed/deepspeed/runtime/pipe/p2p.py:     Send meta and message
./DeepSpeed/deepspeed/runtime/pipe/p2p.py:     Get message meta
./DeepSpeed/deepspeed/runtime/pipe/p2p.py:     Receive and deserialize
./DeepSpeed/deepspeed/runtime/pipe/p2p.py:         handle kwargs
./DeepSpeed/deepspeed/runtime/pipe/p2p.py:         Anything else is a no-op
./DeepSpeed/deepspeed/runtime/pipe/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/pipe/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/pipe/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/pipe/engine.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/pipe/engine.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/pipe/engine.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         We schedule the all-reduces, so disable it in super().backward()
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         BF16 Optimizer is hardcoded for fp32 gradient accumulation
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         used to disable the pipeline all-reduce when used with 1-bit Adam/1-bit LAMB
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         pipeline step for logging
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Set Grid and Communication Groups
./DeepSpeed/deepspeed/runtime/pipe/engine.py:          Set Stage Inf
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         PipelineEngine needs to handle data loading specially due to only the first
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         and last stages loading inputs/labels. We construct a sampler that uses
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Partition input/output buffers
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         XXX temporarily disable while I revert some partition hacks.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Subtract tied parameters if we don't own them
./DeepSpeed/deepspeed/runtime/pipe/engine.py:        initialize peer-2-peer communication and allreduce groups
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Pipeline buffers
./DeepSpeed/deepspeed/runtime/pipe/engine.py:        stores the loss for the current micro batch being processed
./DeepSpeed/deepspeed/runtime/pipe/engine.py:        stores the loss for the entire batch
./DeepSpeed/deepspeed/runtime/pipe/engine.py:             set use_reentrant default to True.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 set activation_checkpoint_func to non_reentrant_checkpoint func.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Initialize pipeline communicators. Just send a 0.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         XXX look into timer reporting timing
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Initialize some timers because of early weirdness.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Build a loader and make it repeating.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         We need to run this first to write to self.averaged_gradients;
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         since this class turns `enable_backward_allreduce` off,
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         `self.overlapping_partition_gradients_reduce_epilogue()` defined in the DeepSpeedEngine
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         never actually runs. I suspect this is because of efficiency problems; get_flat_partition in
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         stage2.py might do something expensive; someone will have to look into that later. But
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         in the meantime, this fixes ZeRO2 + Pipelining enough to run a demo. Further profiling
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         needed to decide if it actually breaks everything.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         (see https://github.com/EleutherAI/gpt-neox/issues/62#issuecomment-761471944)
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 PP+BF16 work for ZeRO Stage 1
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Make our own list of gradients from the optimizer's FP32 grads
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Curriculum learning could change activation shape
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Do the work
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Monitoring
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         TODO: should return precisely what loss returned and allow others to be queried?
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Curriculum learning could change activation shape
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Use the provided data iterator
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Do the work
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         prevent dead-lock with multiple evals sequence
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Restore the training iterator
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Reset any buffers that may have been populated during the forward passes.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:        ds_checkpointing.reset()
./DeepSpeed/deepspeed/runtime/pipe/engine.py:             first sum over all microbatches
./DeepSpeed/deepspeed/runtime/pipe/engine.py:             Average over the microbatches
./DeepSpeed/deepspeed/runtime/pipe/engine.py:             Average over DP groups
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Default to last stage (e.g., for broadcasting loss)
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Scale loss, average among DP ranks, and bcast loss to the rest of my DP group
./DeepSpeed/deepspeed/runtime/pipe/engine.py:            # Average loss across all data-parallel groups
./DeepSpeed/deepspeed/runtime/pipe/engine.py:            print(f'RANK={self.global_rank} bcast SENDER src={self.global_rank} group={self.grid.pp_group}', flush=True)
./DeepSpeed/deepspeed/runtime/pipe/engine.py:             Get loss from last stage
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         If using 3D parallelism, only some first-stage ranks may do IO
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Any post-processing, like broadcasting across a slice-parallel group.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         collect the partitioned input from the previous stage
./DeepSpeed/deepspeed/runtime/pipe/engine.py:             skip mask
./DeepSpeed/deepspeed/runtime/pipe/engine.py:            inputs[1].requires_grad = True
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         inputs has no gradient because it is from a cloned tensor
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Reset activation checkpointing buffers.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Need to call this between evaluation iterations
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Partition the outputs if we are not the last stage
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 TODO: Improve pipe partitioning to pass multiple tensors that require grads
./DeepSpeed/deepspeed/runtime/pipe/engine.py:             Clear the large output data, but save the computation graph
./DeepSpeed/deepspeed/runtime/pipe/engine.py:             Inject the partitioned tensor into the output before sending
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Optionally compute loss on the last device
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 Some models just return loss from forward()
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         The last stage just runs backward on the loss using DeepSpeed's typical
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         mechanisms.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Reconstruct if we previously partitioned the output. We must be
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         careful to also restore the computational graph of the tensors we partitioned.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 Already restored from partition
./DeepSpeed/deepspeed/runtime/pipe/engine.py:            print(f'RANK={self.global_rank} BEFORE-BWD restoring grad={self.grad_layer[0].size()} {self.grad_layer[1].size()}')
./DeepSpeed/deepspeed/runtime/pipe/engine.py:            print(f'RANK={self.global_rank} BEFORE-BWD restored grad={self.grad_layer[0].size()} {self.grad_layer[1].size()}')
./DeepSpeed/deepspeed/runtime/pipe/engine.py:             manually call because we don't call optimizer.backward()
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         This handles either a single tensor or tuple of tensors.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:             manually call because we don't call optimizer.backward()
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Free up the memory from the output of forward()
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 Assume list or tuple
./DeepSpeed/deepspeed/runtime/pipe/engine.py:             XXX: torch 1.6.0 DataLoader will auto convert tuple to list
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 Useful for performance debugging.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 Useful for performance debugging.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Useful for performance debugging.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         A single tensor will be sent.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         List or tuple of tensors
./DeepSpeed/deepspeed/runtime/pipe/engine.py:             Convert to tuples if requested.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         NCCL does not like to send torch.BoolTensor types, so cast the mask to half().
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         We could do char, but with half() we can eventually flatten with other fp16
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         messages (TODO)
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Restore the boolean tensor
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Partition the gradient
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         XXX Terrible hack
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Drop the attention mask from the input buffer here. It does not have
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         a grad that needs to be communicated. We free the buffer immediately
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         after, so no need to restore it. The receiver also has a hack that skips
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         the recv. This is because NCCL does not let us send torch.BoolTensor :-(.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:             XXX terrible hacky branch
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 First two sends are partitioned gradient
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                     Skip tensors that will not produce a grad
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         We can free up the input buffer now
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Allocate the buffer if necessary
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 XXX hardcode meta type
./DeepSpeed/deepspeed/runtime/pipe/engine.py:             NCCL does not like to send torch.BoolTensor types, so un-cast the
./DeepSpeed/deepspeed/runtime/pipe/engine.py:             attention mask
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         XXX these shapes are hardcoded for Megatron
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Restore partitioned output if it was partitioned and we are sending full gradients
./DeepSpeed/deepspeed/runtime/pipe/engine.py:             save for backward
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Allocate gradient if necessary
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 XXX This is a HACK
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 When we exchange activations/gradients, the two pipe stages
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 need to issue the send/recv with the same buffer sizes or
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 else there is a deadlock. The is_floating_point() filter is
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 used to avoid sending gradients for tensors that do not
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 produce gradients. When TP>1, we partition the first
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 activations/gradients across TP ranks to save communication
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 volume and memory. That partitioned tensor is represented as
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 two tensors: a 1/TPth chunk of the original data and also a
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 small LongTensor storing the metadata used to reconstruct on
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 the other side. When combined, the floating point filter also
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 filtered out the metadata tensor. This quick (hacky) fix just
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 branches on is_grad_partitioned so we don't filter out the
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 metadata tensor.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 XXX GPT-2 hack
./DeepSpeed/deepspeed/runtime/pipe/engine.py:            return
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         convert to GB for printing
./DeepSpeed/deepspeed/runtime/pipe/engine.py:     A map of PipeInstruction types to methods. Each method will be executed with the
./DeepSpeed/deepspeed/runtime/pipe/engine.py:     kwargs provided to the PipeInstruction from the scheduler.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         Reserve and reset buffers.
./DeepSpeed/deepspeed/runtime/pipe/engine.py:         For each step in the schedule
./DeepSpeed/deepspeed/runtime/pipe/engine.py:             For each instruction in the step
./DeepSpeed/deepspeed/runtime/pipe/engine.py:                 Equivalent to: self._exec_forward_pass(buffer_id=0)
./DeepSpeed/deepspeed/runtime/pipe/topology.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/pipe/topology.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/pipe/topology.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/pipe/topology.py:         This is actually a class that lets us hash {'row':3, 'col':2} mappings
./DeepSpeed/deepspeed/runtime/pipe/topology.py:         example: 1, (0,0,1)
./DeepSpeed/deepspeed/runtime/pipe/topology.py:             for example, {ProcessCoord(row=0, col=1) : 1}
./DeepSpeed/deepspeed/runtime/pipe/topology.py:         We don't want to RuntimeError because it allows us to write more generalized
./DeepSpeed/deepspeed/runtime/pipe/topology.py:         code for hybrid parallelisms.
./DeepSpeed/deepspeed/runtime/pipe/topology.py:         Grab all axes but `axis`
./DeepSpeed/deepspeed/runtime/pipe/topology.py:         Construct all combinations of coords with other_axes
./DeepSpeed/deepspeed/runtime/pipe/topology.py:             now go over all ranks in `axis`.
./DeepSpeed/deepspeed/runtime/pipe/topology.py:         This could be faster by generating the desired keys directly instead of
./DeepSpeed/deepspeed/runtime/pipe/topology.py:         filtering.
./DeepSpeed/deepspeed/runtime/pipe/topology.py:         TODO use process_group if provided
./DeepSpeed/deepspeed/runtime/pipe/topology.py:         Create new ProcessGroups for all model parallelism. DeepSpeedLight uses these
./DeepSpeed/deepspeed/runtime/pipe/topology.py:         to detect overflow, etc.
./DeepSpeed/deepspeed/runtime/pipe/topology.py:                print(f'RANK={self.global_rank} building DeepSpeed model group: {ranks}')
./DeepSpeed/deepspeed/runtime/pipe/topology.py:         Create new ProcessGroup for gradient all-reduces - these are the data parallel groups
./DeepSpeed/deepspeed/runtime/pipe/topology.py:         Create new ProcessGroup for pipeline collectives - these are pipe parallel groups
./DeepSpeed/deepspeed/runtime/pipe/topology.py:                print(f'RANK={self.global_rank} building pipeline group: {ranks}')
./DeepSpeed/deepspeed/runtime/pipe/topology.py:         Create new ProcessGroup for model (tensor-slicing) collectives
./DeepSpeed/deepspeed/runtime/pipe/topology.py:         Short circuit case without model parallelism.
./DeepSpeed/deepspeed/runtime/pipe/topology.py:         TODO: it would be nice if topology had bcast semantics to avoid this branching
./DeepSpeed/deepspeed/runtime/pipe/topology.py:         case?
./DeepSpeed/deepspeed/runtime/pipe/topology.py:    returns the global rank of the process with the provided stage id
./DeepSpeed/deepspeed/runtime/pipe/topology.py:    which has the same data_parallel_id as caller process
./DeepSpeed/deepspeed/runtime/pipe/topology.py:     MPU functions for DeepSpeed integration
./DeepSpeed/deepspeed/runtime/pipe/topology.py:     These are model parallel groups across all types of model parallelism.
./DeepSpeed/deepspeed/runtime/pipe/topology.py:     Deepspeed uses them to detect overflow, etc.
./DeepSpeed/deepspeed/runtime/pipe/topology.py:     For Megatron-style tensor slicing
./DeepSpeed/deepspeed/runtime/pipe/module.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/pipe/module.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/pipe/module.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/pipe/module.py:         Setup world info
./DeepSpeed/deepspeed/runtime/pipe/module.py:         Construct communicators for pipeline topology
./DeepSpeed/deepspeed/runtime/pipe/module.py:         Initialize partition information
./DeepSpeed/deepspeed/runtime/pipe/module.py:         Offset the random seed by the stage ID.
./DeepSpeed/deepspeed/runtime/pipe/module.py:        newseed = get_accelerator().initial_seed() + self._grid.get_stage_id()
./DeepSpeed/deepspeed/runtime/pipe/module.py:        ds_utils.set_random_seed(newseed)
./DeepSpeed/deepspeed/runtime/pipe/module.py:        with torch.random.fork_rng(devices=[get_accelerator().current_device_name()]):
./DeepSpeed/deepspeed/runtime/pipe/module.py:         if configuration use_reentrant = False, self.activation_checkpoint_func will be set to ``checkpointing.non_reentrant_checkpoint``
./DeepSpeed/deepspeed/runtime/pipe/module.py:             Recursively build PipelineModule objects
./DeepSpeed/deepspeed/runtime/pipe/module.py:             LayerSpec objects contain an nn.Module that should be allocated now.
./DeepSpeed/deepspeed/runtime/pipe/module.py:             TiedLayerSpec objects contain an nn.Module that should be allocated now.
./DeepSpeed/deepspeed/runtime/pipe/module.py:                 Build and register the module if we haven't seen it before.
./DeepSpeed/deepspeed/runtime/pipe/module.py:                     Just use forward()
./DeepSpeed/deepspeed/runtime/pipe/module.py:                     User specified fn with args (module, input)
./DeepSpeed/deepspeed/runtime/pipe/module.py:             LayerSpec objects contain an nn.Module that should be allocated now.
./DeepSpeed/deepspeed/runtime/pipe/module.py:             Last option: layer may be a functional (e.g., lambda). We do nothing in
./DeepSpeed/deepspeed/runtime/pipe/module.py:             that case and just use it in forward()
./DeepSpeed/deepspeed/runtime/pipe/module.py:         All pipeline parameters should be considered as model parallel in the context
./DeepSpeed/deepspeed/runtime/pipe/module.py:         of our FP16 optimizer
./DeepSpeed/deepspeed/runtime/pipe/module.py:         We need to offset the seed by the microbatch ID. Save it in a local var to
./DeepSpeed/deepspeed/runtime/pipe/module.py:         ensure it is preserved in the closure. Otherwise checkpointed forward funcs
./DeepSpeed/deepspeed/runtime/pipe/module.py:         will see a different offset.
./DeepSpeed/deepspeed/runtime/pipe/module.py:                 Single tensor inputs need to be unwrapped
./DeepSpeed/deepspeed/runtime/pipe/module.py:                 Since we either pass tensors or tuples of tensors without unpacking, we
./DeepSpeed/deepspeed/runtime/pipe/module.py:                 need to be careful not to double-wrap tensors with tuple.
./DeepSpeed/deepspeed/runtime/pipe/module.py:         Each stage gets a simple uniform number of layers.
./DeepSpeed/deepspeed/runtime/pipe/module.py:         Print some information on the partitioning.
./DeepSpeed/deepspeed/runtime/pipe/module.py:             Find the layers that the tied module appears in
./DeepSpeed/deepspeed/runtime/pipe/module.py:             Find all stages with this tied module
./DeepSpeed/deepspeed/runtime/pipe/module.py:             TODO: Would be nice to remove the nested data/model parallelism loops and
./DeepSpeed/deepspeed/runtime/pipe/module.py:             TODO: instead generalize in some way, since we really just care about the
./DeepSpeed/deepspeed/runtime/pipe/module.py:             TODO: stage that owns the tied layer. Then loop over each (dp, mp, ...)
./DeepSpeed/deepspeed/runtime/pipe/module.py:             TODO: fiber to generate process groups.
./DeepSpeed/deepspeed/runtime/pipe/module.py:                     Record this tied module if we own a local copy of it.
./DeepSpeed/deepspeed/runtime/pipe/module.py:                             Only count the tied module once in the eyes of the FP16 optimizer
./DeepSpeed/deepspeed/runtime/pipe/module.py:         All checkpoint files start with this
./DeepSpeed/deepspeed/runtime/pipe/module.py:         Data parallelism is omitted from the naming convention because we are agnostic
./DeepSpeed/deepspeed/runtime/pipe/module.py:         to this in the checkpoint.
./DeepSpeed/deepspeed/runtime/pipe/module.py:         Processes having the same model parallel rank on different data parallel instances
./DeepSpeed/deepspeed/runtime/pipe/module.py:         have identical layer weights.  We can distribute the task of saving the layer weights
./DeepSpeed/deepspeed/runtime/pipe/module.py:         among the data parallel ranks.  For example, if a pipeline stage has 9 layers and
./DeepSpeed/deepspeed/runtime/pipe/module.py:         if there are 2 data parallel instances, rank 0 will save the first 5 layers and
./DeepSpeed/deepspeed/runtime/pipe/module.py:         rank 1 will save the last 4.
./DeepSpeed/deepspeed/runtime/pipe/module.py:             spread layers evenly across data parallel ranks
./DeepSpeed/deepspeed/runtime/pipe/module.py:             data parallel rank 0 writes all layers
./DeepSpeed/deepspeed/runtime/pipe/module.py:             Functions, etc. will not have state_dicts
./DeepSpeed/deepspeed/runtime/pipe/module.py:             get all checkpoint files for the layer.
./DeepSpeed/deepspeed/runtime/pipe/module.py:             if self._grid.data_parallel_id == 0:
./DeepSpeed/deepspeed/runtime/pipe/module.py:                 logger.info(
./DeepSpeed/deepspeed/runtime/pipe/module.py:                     f'RANK={self.global_rank} Loaded layer={idx+self._local_start} file={load_path}'
./DeepSpeed/deepspeed/runtime/pipe/module.py:                 )
./DeepSpeed/deepspeed/runtime/pipe/module.py:             This hook excludes the embedding layer
./DeepSpeed/deepspeed/runtime/pipe/module.py:             because only non_reentrant_checkpoint can accept inputs with requires_grad=False
./DeepSpeed/deepspeed/runtime/pipe/module.py:             otherwise, the backward of the embedding layer won't receive gradients.
./DeepSpeed/deepspeed/runtime/pipe/schedule.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/pipe/schedule.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/pipe/schedule.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/pipe/schedule.py:             Alternate send/recv buffers
./DeepSpeed/deepspeed/runtime/pipe/schedule.py:             Map the step of the pipeline to the micro-batch id and also whether it is a
./DeepSpeed/deepspeed/runtime/pipe/schedule.py:             forward or backward pass step.
./DeepSpeed/deepspeed/runtime/pipe/schedule.py:             Exchange activations
./DeepSpeed/deepspeed/runtime/pipe/schedule.py:             First/last stage loads
./DeepSpeed/deepspeed/runtime/pipe/schedule.py:             Computation
./DeepSpeed/deepspeed/runtime/pipe/schedule.py:             Model step at the end of the batch
./DeepSpeed/deepspeed/runtime/pipe/schedule.py:             Prepare state for next time
./DeepSpeed/deepspeed/runtime/pipe/schedule.py: IO
./DeepSpeed/deepspeed/runtime/pipe/schedule.py: Compute
./DeepSpeed/deepspeed/runtime/pipe/schedule.py: Communication
./DeepSpeed/deepspeed/runtime/config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/config.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/config.py: extra optimizer parameters for adam/adamw
./DeepSpeed/deepspeed/runtime/config.py: default to adamw logic for adam/adamw optimizers unless user explicitly opts out
./DeepSpeed/deepspeed/runtime/config.py:     The torch dtype must always be the first value (so we return torch.dtype)
./DeepSpeed/deepspeed/runtime/config.py:     Copied from https://stackoverflow.com/a/43210118
./DeepSpeed/deepspeed/runtime/config.py:     Allows us to use multiple values for each Enum index and returns first
./DeepSpeed/deepspeed/runtime/config.py:     listed value when Enum is called
./DeepSpeed/deepspeed/runtime/config.py:         first value is canonical value
./DeepSpeed/deepspeed/runtime/config.py:         If elastic-mode enabled, update compute + update _param_dict
./DeepSpeed/deepspeed/runtime/config.py:             Ensure the resource scheduler saw the same elastic config we are using at runtime
./DeepSpeed/deepspeed/runtime/config.py:             micro_bsz * world_size * gas = total_batch_size
./DeepSpeed/deepspeed/runtime/config.py:             gas = total_batch_size // (micro_bsz * world_size)
./DeepSpeed/deepspeed/runtime/config.py:         Pass a copy so that user json is unmodified, e.g. for logging
./DeepSpeed/deepspeed/runtime/config.py:        print(f"beginning get_train_batch_size = {get_train_batch_size}")
./DeepSpeed/deepspeed/runtime/config.py:        print(f"train_batch = {train_batch}, micro_batch={micro_batch}")
./DeepSpeed/deepspeed/runtime/config.py:         all values are provided nothing needs to be set
./DeepSpeed/deepspeed/runtime/config.py:         global_accumulation_steps needs to be set
./DeepSpeed/deepspeed/runtime/config.py:         micro_batch_per_gpu needs to be set
./DeepSpeed/deepspeed/runtime/config.py:         train_batch_size needs to be set
./DeepSpeed/deepspeed/runtime/config.py:         gradient_accumulation_steps and micro_batch_per_gpus is set
./DeepSpeed/deepspeed/runtime/config.py:         train_batch_size and gradient_accumulation_step is set
./DeepSpeed/deepspeed/runtime/config.py:         either none of the three parameters are provided or just gradient_accumulation_step is provided
./DeepSpeed/deepspeed/runtime/lr_schedules.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/lr_schedules.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/lr_schedules.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/lr_schedules.py:     LR scheduler
./DeepSpeed/deepspeed/runtime/lr_schedules.py:     Learning rate range test
./DeepSpeed/deepspeed/runtime/lr_schedules.py:     OneCycle schedule
./DeepSpeed/deepspeed/runtime/lr_schedules.py:     1Cycle LR
./DeepSpeed/deepspeed/runtime/lr_schedules.py:     1Cycle Momentum
./DeepSpeed/deepspeed/runtime/lr_schedules.py:     Warmup LR
./DeepSpeed/deepspeed/runtime/lr_schedules.py:     WarmUP cos LR
./DeepSpeed/deepspeed/runtime/lr_schedules.py:     1Cycle LR params
./DeepSpeed/deepspeed/runtime/lr_schedules.py:     1Cycle MOM params
./DeepSpeed/deepspeed/runtime/lr_schedules.py:     LR range test params
./DeepSpeed/deepspeed/runtime/lr_schedules.py:     1Cycle params
./DeepSpeed/deepspeed/runtime/lr_schedules.py:     WarmupLR params
./DeepSpeed/deepspeed/runtime/lr_schedules.py:     Warmup LR
./DeepSpeed/deepspeed/runtime/lr_schedules.py:         Initialize cycle shape
./DeepSpeed/deepspeed/runtime/lr_schedules.py:         Initialize cycle lr
./DeepSpeed/deepspeed/runtime/lr_schedules.py:         Initialize cyclic momentum
./DeepSpeed/deepspeed/runtime/lr_schedules.py:         Initialize batch iteration tracker
./DeepSpeed/deepspeed/runtime/lr_schedules.py:     Configure cycle shape
./DeepSpeed/deepspeed/runtime/lr_schedules.py:     Configure lr schedule
./DeepSpeed/deepspeed/runtime/lr_schedules.py:     Configure momentum schedule
./DeepSpeed/deepspeed/runtime/lr_schedules.py:         Currently only support linear and log function
./DeepSpeed/deepspeed/runtime/quantize.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/quantize.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/quantize.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/quantize.py:         Temporary disabled functionality
./DeepSpeed/deepspeed/runtime/quantize.py:         Random number generator (Uniform)
./DeepSpeed/deepspeed/runtime/quantize.py:         fixing the quantization bits based on the training steps
./DeepSpeed/deepspeed/runtime/quantize.py:         when reducing 1 bit at each period, we increase the period
./DeepSpeed/deepspeed/runtime/quantize.py:         to go slowly toward the target quantization bits
./DeepSpeed/deepspeed/runtime/quantize.py:         the period and starting bit can be configured
./DeepSpeed/deepspeed/runtime/weight_quantizer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/weight_quantizer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/weight_quantizer.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/constants.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/constants.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/constants.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/constants.py:############################################
./DeepSpeed/deepspeed/runtime/constants.py: Routes
./DeepSpeed/deepspeed/runtime/constants.py:############################################
./DeepSpeed/deepspeed/runtime/constants.py:############################################
./DeepSpeed/deepspeed/runtime/constants.py: Batch size
./DeepSpeed/deepspeed/runtime/constants.py:############################################
./DeepSpeed/deepspeed/runtime/constants.py:############################################
./DeepSpeed/deepspeed/runtime/constants.py: Sparse attention
./DeepSpeed/deepspeed/runtime/constants.py:############################################
./DeepSpeed/deepspeed/runtime/constants.py:############################################
./DeepSpeed/deepspeed/runtime/constants.py: Optimizer and lr scheduler
./DeepSpeed/deepspeed/runtime/constants.py:############################################
./DeepSpeed/deepspeed/runtime/constants.py:############################################
./DeepSpeed/deepspeed/runtime/constants.py: Optimizer and lr scheduler
./DeepSpeed/deepspeed/runtime/constants.py:############################################
./DeepSpeed/deepspeed/runtime/constants.py: Steps
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Training micro batch size per GPU
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Batch size for one training step. This is used when the
./DeepSpeed/deepspeed/runtime/constants.py: TRAIN_BATCH_SIZE cannot fit in GPU memory to determine
./DeepSpeed/deepspeed/runtime/constants.py: the number of gradient accumulation steps. By default, this
./DeepSpeed/deepspeed/runtime/constants.py: is set to None. Users can configure in ds_config.json as below example:
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Gradient Accumulation
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Gradient accumulation feature. By default, this feature is not enabled.
./DeepSpeed/deepspeed/runtime/constants.py: Users can configure in ds_config.json as below example:
./DeepSpeed/deepspeed/runtime/constants.py: DeepSpeed CSR gradient sparsity
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: BFLOAT16 support
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: BFLOAT16 feature. By default, this feature is not enabled.
./DeepSpeed/deepspeed/runtime/constants.py: Users can configure in ds_config.json as below example:
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: FP16 support
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: FP16 feature. By default, this feature is not enabled.
./DeepSpeed/deepspeed/runtime/constants.py: Users can configure in ds_config.json as below example:
./DeepSpeed/deepspeed/runtime/constants.py: FP16 loss scale, zero means using dynamic scaling
./DeepSpeed/deepspeed/runtime/constants.py: FP16 initial dynamic scale loss power
./DeepSpeed/deepspeed/runtime/constants.py: FP16 loss scale window
./DeepSpeed/deepspeed/runtime/constants.py: FP16 hysteresis
./DeepSpeed/deepspeed/runtime/constants.py: FP16 consecutive hysteresis
./DeepSpeed/deepspeed/runtime/constants.py: FP16 min loss scale
./DeepSpeed/deepspeed/runtime/constants.py: FP16 master and grads
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Apex AMP support
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Use Apex AMP for mixed precision support, all parameters (other than 'enabled') will be passed to
./DeepSpeed/deepspeed/runtime/constants.py: amp.initialize(model, optimizer, **amp_params)
./DeepSpeed/deepspeed/runtime/constants.py: See apex documentation for supported parameters/features: https://nvidia.github.io/apex/amp.html#apex.amp.initialize
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Gradient clipping
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Gradient clipping. By default, this feature is not enabled.
./DeepSpeed/deepspeed/runtime/constants.py: Users can configure in ds_config.json as below example:
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Communication data type
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Supported types: ['none', 'fp16', 'fp32']
./DeepSpeed/deepspeed/runtime/constants.py: By default, this feature is not enabled ('none' value)
./DeepSpeed/deepspeed/runtime/constants.py: Users can configure in ds_config.json as below example:
./DeepSpeed/deepspeed/runtime/constants.py:##########################################################
./DeepSpeed/deepspeed/runtime/constants.py: Gradient communication data type for sequence parallelism
./DeepSpeed/deepspeed/runtime/constants.py:##########################################################
./DeepSpeed/deepspeed/runtime/constants.py: Supported types: ['fp16', 'bf16','fp32']
./DeepSpeed/deepspeed/runtime/constants.py: Default value is fp32
./DeepSpeed/deepspeed/runtime/constants.py: Users can configure in ds_config.json as below example:
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Scale/predivide gradients before allreduce
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Prescale gradients. By default, this feature is not enabled.
./DeepSpeed/deepspeed/runtime/constants.py: Users can configure in ds_config.json as below example:
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Disable AllGather
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Disable AllGather. By default, this feature is not enabled.
./DeepSpeed/deepspeed/runtime/constants.py: Users can configure in ds_config.json as below example:
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Dump DeepSpeed state
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Dump State. By default, this feature is not enabled.
./DeepSpeed/deepspeed/runtime/constants.py: Users can configure in ds_config.json as below example:
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Vocabulary size
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Vocabulary size.
./DeepSpeed/deepspeed/runtime/constants.py: Users can configure in ds_config.json as below example:
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Wall block breakdown
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Wall clock breakdown. By default, this feature is not enabled.
./DeepSpeed/deepspeed/runtime/constants.py: Users can configure in ds_config.json as below example:
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Eigenvalue
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Eigenvalue computation. By default, this feature is not enabled.
./DeepSpeed/deepspeed/runtime/constants.py: Users can configure in ds_config.json as below example:
./DeepSpeed/deepspeed/runtime/constants.py: Tensorboard enable signal
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Progressive Layer Drop (PLD)
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: PLD enable signal
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Validation modes
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Checkpoint config params
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: "checkpoint": {
./DeepSpeed/deepspeed/runtime/constants.py:   tag_validation=["Ignore"|"Warn"|"Fail"]
./DeepSpeed/deepspeed/runtime/constants.py:   load_universal=false
./DeepSpeed/deepspeed/runtime/constants.py:   use_node_local_storage=false
./DeepSpeed/deepspeed/runtime/constants.py:   parallel_write: {
./DeepSpeed/deepspeed/runtime/constants.py:     pipeline_stage: [True|False]
./DeepSpeed/deepspeed/runtime/constants.py:   }
./DeepSpeed/deepspeed/runtime/constants.py: }
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Data types config params
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: "data_types": {
./DeepSpeed/deepspeed/runtime/constants.py:   grad_accum_dtype=["bf16"|"fp16"|"fp32"]
./DeepSpeed/deepspeed/runtime/constants.py:   }
./DeepSpeed/deepspeed/runtime/constants.py: }
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: Drop the last incomplete Batch
./DeepSpeed/deepspeed/runtime/constants.py: #########################################
./DeepSpeed/deepspeed/runtime/constants.py: dataloader_drop_last. By default, this feature is not enabled.
./DeepSpeed/deepspeed/runtime/constants.py: Users can configure in ds_config.json as below example:
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: PIPELINE PARALLELISM
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: DATA PARALLELISM
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/constants.py: EXPERT-DATA PARALLELISM TOPO Config
./DeepSpeed/deepspeed/runtime/constants.py:########################################
./DeepSpeed/deepspeed/runtime/swap_tensor/optimizer_utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/swap_tensor/optimizer_utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/swap_tensor/optimizer_utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/swap_tensor/optimizer_utils.py:         NVMe swap management
./DeepSpeed/deepspeed/runtime/swap_tensor/optimizer_utils.py:         Read/Write alignment for each thread during Intra-request parallelism
./DeepSpeed/deepspeed/runtime/swap_tensor/optimizer_utils.py:         Swap buffer management
./DeepSpeed/deepspeed/runtime/swap_tensor/optimizer_utils.py:         Timers
./DeepSpeed/deepspeed/runtime/swap_tensor/optimizer_utils.py:         Print exclusion list
./DeepSpeed/deepspeed/runtime/swap_tensor/optimizer_utils.py:             Split into two by making remainder a tensor
./DeepSpeed/deepspeed/runtime/swap_tensor/optimizer_utils.py:             remainder tensor
./DeepSpeed/deepspeed/runtime/swap_tensor/optimizer_utils.py:         It should be safe to discard unswapped gradient partitions
./DeepSpeed/deepspeed/runtime/swap_tensor/pipelined_optimizer_swapper.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/swap_tensor/pipelined_optimizer_swapper.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/swap_tensor/pipelined_optimizer_swapper.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/swap_tensor/pipelined_optimizer_swapper.py:         Overlap gradient swap out
./DeepSpeed/deepspeed/runtime/swap_tensor/constants.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/swap_tensor/constants.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/swap_tensor/constants.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/swap_tensor/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/swap_tensor/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/swap_tensor/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:     Partitioned parameters are present and ready for use
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:     partitioned params are in some non-memory device
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:     partitioned params are being read from some non-memory device.
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:        set swap buffers, create aio handles
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:        mapping from param id to path
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:        mapping from pram_id to buffer id
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:         mapping from param_id to swap buffer
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:        number of elements in the param
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:        keep track of async swap in params and buffers
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:        keep track of available params
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:         for swapping out from partitioned fp32 params
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:         Read/Write alignment for each thread during Intra-request parallelism
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:    Check if partitioned param or numel in a tensor is swappable or not
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:    waits for inflight nvme write to complete
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:    waits for inflight nvme reads to complete
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:    Removes the memory assignment and releases the buffers
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:    Should only be executed after swapping out the tensors
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:    writes from in memory to nvme. Does not release the buffers
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:    blocking swap out followed by releasing the memory buffers
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:     book keeping function for inflight swap in
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:    assigns an in memory buffer and swaps in from nvme
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:     Enables swapping into buffer that is out the control of swapper. This is always synchronous
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:             Release swap buffer memory assignment. Note, this will mark the parameter not available.
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py:    assign a buffer to a param and return the buffer
./DeepSpeed/deepspeed/runtime/swap_tensor/utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/swap_tensor/utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/swap_tensor/utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/swap_tensor/async_swapper.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/swap_tensor/async_swapper.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/swap_tensor/async_swapper.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_optimizer_swapper.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_optimizer_swapper.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_optimizer_swapper.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/swap_tensor/partitioned_optimizer_swapper.py:         Overlap swapping out
./DeepSpeed/deepspeed/runtime/swap_tensor/aio_config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/swap_tensor/aio_config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/swap_tensor/aio_config.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/comm/nccl.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/comm/nccl.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/comm/nccl.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/comm/nccl.py:         all_start_time = time.time()
./DeepSpeed/deepspeed/runtime/comm/nccl.py:         cupy_recvbuf_scale = cupy.zeros([self.size, 1], dtype=cupy_worker_scale.dtype)
./DeepSpeed/deepspeed/runtime/comm/nccl.py:         worker_scale = self.compression_backend.cupy2torch(cupy_worker_scale)
./DeepSpeed/deepspeed/runtime/comm/nccl.py:        recvbuf_scale = self.compression_backend.cupy2torch(cupy_recvbuf_scale)
./DeepSpeed/deepspeed/runtime/comm/nccl.py:         communication phase 1
./DeepSpeed/deepspeed/runtime/comm/nccl.py:         gather_start = time.time()
./DeepSpeed/deepspeed/runtime/comm/nccl.py:         Alltoall for sign
./DeepSpeed/deepspeed/runtime/comm/nccl.py:         Allgather for scale
./DeepSpeed/deepspeed/runtime/comm/nccl.py:         gather_end = time.time()
./DeepSpeed/deepspeed/runtime/comm/nccl.py:         cupy_sign_list_packed, sign_list_packed, cupy_worker_scale, worker_scale = None, None, None, None
./DeepSpeed/deepspeed/runtime/comm/nccl.py:        cupy_recvbuf_scale = self.compression_backend.torch2cupy(torch.stack(recvbuf_scale))
./DeepSpeed/deepspeed/runtime/comm/nccl.py:         cupy_server_scale = self.compression_backend.torch2cupy(server_scale)
./DeepSpeed/deepspeed/runtime/comm/nccl.py:         cupy_recvbuf_sign, recvbuf_sign = None, None
./DeepSpeed/deepspeed/runtime/comm/nccl.py:         server_scale = self.compression_backend.cupy2torch(cupy_server_scale)
./DeepSpeed/deepspeed/runtime/comm/nccl.py:         cupy_recvbuf_scale, recvbuf_scale = None, None
./DeepSpeed/deepspeed/runtime/comm/nccl.py:         Communication Phase 2
./DeepSpeed/deepspeed/runtime/comm/nccl.py:         need to convert from a tensor list to a single tensor
./DeepSpeed/deepspeed/runtime/comm/nccl.py:         dist.all_gather only provides a tensor list as the recv/output buffer
./DeepSpeed/deepspeed/runtime/comm/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/comm/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/comm/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/comm/mpi.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/comm/mpi.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/comm/mpi.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/comm/mpi.py:         We do in-place operations on cupy buffers so we do not return any buffers
./DeepSpeed/deepspeed/runtime/comm/mpi.py:         In-place operations are not possible for newly created cupy arrays
./DeepSpeed/deepspeed/runtime/comm/mpi.py:         so we need to return the new buffers
./DeepSpeed/deepspeed/runtime/comm/mpi.py:         1. convert from cupy to numpy
./DeepSpeed/deepspeed/runtime/comm/mpi.py:         2. use numpy buffers for communication
./DeepSpeed/deepspeed/runtime/comm/mpi.py:         3. Convert back from numpy to cupy
./DeepSpeed/deepspeed/runtime/comm/mpi.py:         1. Convert cupy to numpy
./DeepSpeed/deepspeed/runtime/comm/mpi.py:         2. Communicate numpy buffers
./DeepSpeed/deepspeed/runtime/comm/mpi.py:         3. Convert numpy back to cupy
./DeepSpeed/deepspeed/runtime/comm/mpi.py:         Communication Phase 1
./DeepSpeed/deepspeed/runtime/comm/mpi.py:         cupy_sign_list_packed, cupy_worker_scale, worker_scale = None, None, None
./DeepSpeed/deepspeed/runtime/comm/mpi.py:         cupy_recvbuf_sign, cupy_recvbuf_scale = None, None
./DeepSpeed/deepspeed/runtime/comm/mpi.py:         Communication Phase 2
./DeepSpeed/deepspeed/runtime/comm/mpi.py:         cupy_server_sign_packed, cupy_server_scale, server_scale = None, None, None
./DeepSpeed/deepspeed/runtime/comm/mpi.py:         cupy_recvbuf_sign_server, cupy_recvbuf_scale_server = None, None
./DeepSpeed/deepspeed/runtime/comm/coalesced_collectives.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/comm/coalesced_collectives.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/comm/coalesced_collectives.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/comm/coalesced_collectives.py: NOTE: Use torch.distributed's ProcessGroup class until we have our own.
./DeepSpeed/deepspeed/runtime/comm/coalesced_collectives.py:         if there's only one tensor being reduced and we don't need to pad
./DeepSpeed/deepspeed/runtime/comm/coalesced_collectives.py:         we have an opportunity to avoid a memory allocation
./DeepSpeed/deepspeed/runtime/comm/coalesced_collectives.py:         interleave tensor partitions such that the correct reduced partitions of each tensor
./DeepSpeed/deepspeed/runtime/comm/coalesced_collectives.py:         end up at each rank
./DeepSpeed/deepspeed/runtime/comm/coalesced_collectives.py:                 add tensor content
./DeepSpeed/deepspeed/runtime/comm/coalesced_collectives.py:                 add padding if necessary
./DeepSpeed/deepspeed/runtime/comm/coalesced_collectives.py:     batched reduce-scatter call
./DeepSpeed/deepspeed/runtime/comm/coalesced_collectives.py:     reverse procedure of the interleaving done previously, done on the
./DeepSpeed/deepspeed/runtime/comm/coalesced_collectives.py:     result of the batched reduce-scatter
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:         Use torch (un)flatten ops
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:        align nccl all-gather send buffers to 4-bye boundary
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:         Build BF16/FP32 groups
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:         Maintain different fp32 gradients views for convenience
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:             grab the original list
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:             create flat bf16 params
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:             Make bf16 params point to flat tensor storage
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:             divide flat weights into equal sized partitions
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:             create fp32 params partition
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:             create fp32 gradients
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:             track individual fp32 gradients for entire model
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:             flat tensor corresponding to actual fp32 gradients (i.e., minus alignment padding)
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:             flat tensor corresponding to gradient partition
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:             track fp32 gradient updates
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:             Record padding required for alignment
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:             update optimizer param groups to reference fp32 params partition
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:         Need optimizer states initialized before linking lp to optimizer state
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:             Link bf16 and fp32 params in partition
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:                 clear gradients
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:             print_rank_0(f'update_lp_params {i=} {partition_id=}', force=True)
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:             if i == 0:
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:                 print_rank_0(f'{fp32_partition[:10]=}', force=True)
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:     Restore base optimizer fp32 weights bfloat16 weights
./DeepSpeed/deepspeed/runtime/bf16_optimizer.py:                    print(f"Loading {self.param_names[lp]} {tp_rank=} {tp_world_size=}")
./DeepSpeed/deepspeed/runtime/eigenvalue.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/eigenvalue.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/eigenvalue.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/eigenvalue.py:     Replace all nan/pos-inf/neg-inf to zero
./DeepSpeed/deepspeed/runtime/eigenvalue.py:     TODO: Pytorch new version may add this function, replace this one by then.
./DeepSpeed/deepspeed/runtime/eigenvalue.py:             We found this randn() has obvious accuracy impact in some cases, save/recover random state here.
./DeepSpeed/deepspeed/runtime/eigenvalue.py:             Disable eigenvalue if the model doesn't support second order gradients computation,
./DeepSpeed/deepspeed/runtime/eigenvalue.py:             e.g. when enabling DS transformer kernel.
./DeepSpeed/deepspeed/runtime/eigenvalue.py:                Hv = [hv.float() for hv in Hv]
./DeepSpeed/deepspeed/runtime/eigenvalue.py:         {param_id: (eigenvalue, layer_id)}
./DeepSpeed/deepspeed/runtime/eigenvalue.py:     1. Map all eigenvalues to [0, 1.0].
./DeepSpeed/deepspeed/runtime/eigenvalue.py:     2. Some layers can't generate valid eigenvalues on fp16 precision, use 1.0 instead.
./DeepSpeed/deepspeed/runtime/engine.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/engine.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/engine.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/engine.py:     Fail silently so we don't spam logs unnecessarily if user isn't using amp
./DeepSpeed/deepspeed/runtime/engine.py:         for debug purposes - can then debug print: debug_get_module_name(module)
./DeepSpeed/deepspeed/runtime/engine.py:         needed for zero_to_fp32 weights reconstruction to remap nameless data to state_dict
./DeepSpeed/deepspeed/runtime/engine.py:         Configure distributed model
./DeepSpeed/deepspeed/runtime/engine.py:         Configure wall clock timers
./DeepSpeed/deepspeed/runtime/engine.py:         Throughput timer
./DeepSpeed/deepspeed/runtime/engine.py:         Configure optimizer and scheduler
./DeepSpeed/deepspeed/runtime/engine.py:         If no parameters given by init default to module parameters
./DeepSpeed/deepspeed/runtime/engine.py:         Convert model parameters from generator to list
./DeepSpeed/deepspeed/runtime/engine.py:             no optim selected but zero is enabled
./DeepSpeed/deepspeed/runtime/engine.py:         Hook optimizer for snip_momentum pruning
./DeepSpeed/deepspeed/runtime/engine.py:         Bookkeeping for sparse support
./DeepSpeed/deepspeed/runtime/engine.py:         if self.sparse_gradients_enabled():
./DeepSpeed/deepspeed/runtime/engine.py:         Engine timers
./DeepSpeed/deepspeed/runtime/engine.py:         Use torch (un)flatten ops
./DeepSpeed/deepspeed/runtime/engine.py:                 since user code might call deepspeed.zero.Init() before deepspeed.initialize(), need to check the attribute to check if the parameter is partitioned in zero 3 already or not
./DeepSpeed/deepspeed/runtime/engine.py:            print(f'{train_batch_size=} {self.train_micro_batch_size_per_gpu()=} {self.dp_world_size=}')
./DeepSpeed/deepspeed/runtime/engine.py:         overwrite config
./DeepSpeed/deepspeed/runtime/engine.py:         overwrite config
./DeepSpeed/deepspeed/runtime/engine.py:             Add code for finding number of GPUs per node automatically
./DeepSpeed/deepspeed/runtime/engine.py:            self.lr_scheduler = lr_schedules.WarmupLayerTokenDecayLR(self.optimizer, self.random_ltd_scheduler)
./DeepSpeed/deepspeed/runtime/engine.py:         First check for scheduler in json configuration
./DeepSpeed/deepspeed/runtime/engine.py:         only the first data parallel process needs to store the model checkpoint
./DeepSpeed/deepspeed/runtime/engine.py:         if you want to use node local storage this must be done by rank 0 on each
./DeepSpeed/deepspeed/runtime/engine.py:         node
./DeepSpeed/deepspeed/runtime/engine.py:             Only the first parameter parallel process needs to store the
./DeepSpeed/deepspeed/runtime/engine.py:             optimizer state checkpoints for zero
./DeepSpeed/deepspeed/runtime/engine.py:     Configure based on command line arguments
./DeepSpeed/deepspeed/runtime/engine.py:         After the distributed backend is initialized we are guaranteed the LOCAL_RANK
./DeepSpeed/deepspeed/runtime/engine.py:         environment variable is set. We must align args.local_rank to this value for
./DeepSpeed/deepspeed/runtime/engine.py:         backwards compatibility with scripts relying on [args|self].local_rank containing
./DeepSpeed/deepspeed/runtime/engine.py:         the correct local rank info. _do_args_sanity_check will ensure this is the case.
./DeepSpeed/deepspeed/runtime/engine.py:     Validate command line arguments
./DeepSpeed/deepspeed/runtime/engine.py:             fairseq optims are not torch.optim objects
./DeepSpeed/deepspeed/runtime/engine.py:     Validate configuration based on command line arguments
./DeepSpeed/deepspeed/runtime/engine.py:         Detect invalid combinations of client optimizer and client scheduler
./DeepSpeed/deepspeed/runtime/engine.py:             Broadcast the model for different parameters
./DeepSpeed/deepspeed/runtime/engine.py:         register client model in _modules so that nn.module methods work correctly
./DeepSpeed/deepspeed/runtime/engine.py:         register module attribute in engine but avoid getattr
./DeepSpeed/deepspeed/runtime/engine.py:         zero.Init() handles device placement of model
./DeepSpeed/deepspeed/runtime/engine.py:         MoE related initialization
./DeepSpeed/deepspeed/runtime/engine.py:         Pass the mpu from here to groups. For subsequent use, just query groups
./DeepSpeed/deepspeed/runtime/engine.py:         Set deepspeed parallelism spec. for the model including expert parallelism
./DeepSpeed/deepspeed/runtime/engine.py:         Query the groups module to get information about various parallel groups
./DeepSpeed/deepspeed/runtime/engine.py:     check if parameters are duplicated in optimizer param_groups
./DeepSpeed/deepspeed/runtime/engine.py:         config based assertions
./DeepSpeed/deepspeed/runtime/engine.py:                 If apex/amp is available it will be imported above
./DeepSpeed/deepspeed/runtime/engine.py:         data type checks
./DeepSpeed/deepspeed/runtime/engine.py:             else optimizer_wrapper = None
./DeepSpeed/deepspeed/runtime/engine.py:     Configure optimizer
./DeepSpeed/deepspeed/runtime/engine.py:             TODO: maybe need to broadcast experts differently?
./DeepSpeed/deepspeed/runtime/engine.py:         print(optimizer_parameters.keys())
./DeepSpeed/deepspeed/runtime/engine.py:             Optimizer name of Adam forces AdamW logic unless adam_w_mode is explicitly set
./DeepSpeed/deepspeed/runtime/engine.py:             Overlap and contiguous grads are meaningless in stage 1 and are ignored
./DeepSpeed/deepspeed/runtime/engine.py:                 Non-MoE requires contiguous grads to be disabled w. stage 1
./DeepSpeed/deepspeed/runtime/engine.py:         Currently we only use timer in train route
./DeepSpeed/deepspeed/runtime/engine.py:         If mpu is provided, forward world size and parallel rank to sampler.
./DeepSpeed/deepspeed/runtime/engine.py:         used to check quantization happens at step 0!
./DeepSpeed/deepspeed/runtime/engine.py:             TODO: The above if condition is a HACK since for PipelineEngine
./DeepSpeed/deepspeed/runtime/engine.py:             it's difficult to inject argument in forward pass.
./DeepSpeed/deepspeed/runtime/engine.py:             Enable automated discovery of external parameters by indicating that
./DeepSpeed/deepspeed/runtime/engine.py:             we are in a forward pass.
./DeepSpeed/deepspeed/runtime/engine.py:             Disable automated discovery of external parameters
./DeepSpeed/deepspeed/runtime/engine.py:            logger.info(f"Individual TopK gate time: {gate.gate_time:.2f} ms")
./DeepSpeed/deepspeed/runtime/engine.py:            logger.info(f"MoE layer; total: {l.time_moe:.2f} ms, first alltoall: {l.time_falltoall:.2f}, second alltoall: {l.time_salltoall:.2f}")
./DeepSpeed/deepspeed/runtime/engine.py:         TODO: Allreduce/average them across ranks for more accurate timing.
./DeepSpeed/deepspeed/runtime/engine.py:         if deepspeed.comm.get_rank() == 0:
./DeepSpeed/deepspeed/runtime/engine.py:         Pass (PP) gas boundary flag to optimizer (required for zero)
./DeepSpeed/deepspeed/runtime/engine.py:         ZeRO stage >= 2 communicates during non gradient accumulation boundaries as well
./DeepSpeed/deepspeed/runtime/engine.py:         Communicate only at gradient accumulation boundaries
./DeepSpeed/deepspeed/runtime/engine.py:         scale loss w.r.t. gradient accumulation if needed
./DeepSpeed/deepspeed/runtime/engine.py:         Log training loss
./DeepSpeed/deepspeed/runtime/engine.py:             AMP requires delaying unscale when inside gradient accumulation boundaries
./DeepSpeed/deepspeed/runtime/engine.py:             https://nvidia.github.io/apex/advanced.html#gradient-accumulation-across-iterations
./DeepSpeed/deepspeed/runtime/engine.py:             Traditional code path that allreduces the module parameter grads
./DeepSpeed/deepspeed/runtime/engine.py:             loss.data = None
./DeepSpeed/deepspeed/runtime/engine.py:                 AMP's recommended way of doing clipping
./DeepSpeed/deepspeed/runtime/engine.py:                 https://nvidia.github.io/apex/advanced.html#gradient-clipping
./DeepSpeed/deepspeed/runtime/engine.py:         Quantize the updated parameter if there is no overflow
./DeepSpeed/deepspeed/runtime/engine.py:         zero grad in basic optimizer could be unreliable and may not exhibit
./DeepSpeed/deepspeed/runtime/engine.py:         the behavior that we want
./DeepSpeed/deepspeed/runtime/engine.py:             TODO: Temporary until bf16_optimizer and zero_optimizer are integrated
./DeepSpeed/deepspeed/runtime/engine.py:         Check overflow here since in DS fp16 optimizer, the overflow is updated in above step() function.
./DeepSpeed/deepspeed/runtime/engine.py:                     XXX Hack to work with Megatron 2.0 and DeepSpeed pipelines.
./DeepSpeed/deepspeed/runtime/engine.py:                     We don't currently have a way to specify lr_kwargs from
./DeepSpeed/deepspeed/runtime/engine.py:                     pipe_engine.train_batch()
./DeepSpeed/deepspeed/runtime/engine.py:         Check early because self.global_steps is incremented at some point here.
./DeepSpeed/deepspeed/runtime/engine.py:         TODO: Delay self.global_steps increment until very end of this function.
./DeepSpeed/deepspeed/runtime/engine.py:         Update the model when we reach gradient accumulation boundaries
./DeepSpeed/deepspeed/runtime/engine.py:         Log learning rate
./DeepSpeed/deepspeed/runtime/engine.py:         Check flops profiling
./DeepSpeed/deepspeed/runtime/engine.py:             Log micro timing and reset
./DeepSpeed/deepspeed/runtime/engine.py:             Log global timing and reset
./DeepSpeed/deepspeed/runtime/engine.py:                 In cases where there is an imbalance of empty grads across
./DeepSpeed/deepspeed/runtime/engine.py:                 ranks we must create empty grads, this will ensure that every
./DeepSpeed/deepspeed/runtime/engine.py:                 rank is reducing the same size. In some cases it may make
./DeepSpeed/deepspeed/runtime/engine.py:                 sense in the future to support the ability to average not
./DeepSpeed/deepspeed/runtime/engine.py:                 w.r.t. world size but with a different value.
./DeepSpeed/deepspeed/runtime/engine.py:                 Call param.grad without data to avoid problem with setting of updated grads
./DeepSpeed/deepspeed/runtime/engine.py:                     Separate between diff groups
./DeepSpeed/deepspeed/runtime/engine.py:         Densify sparse tensor and copy back to original location
./DeepSpeed/deepspeed/runtime/engine.py:         Remove frozen parameter weights from state_dict if specified
./DeepSpeed/deepspeed/runtime/engine.py:                 Updating global -> local expert ids
./DeepSpeed/deepspeed/runtime/engine.py:                     loop all local_experts
./DeepSpeed/deepspeed/runtime/engine.py:                         print(expert_state_dict.keys())
./DeepSpeed/deepspeed/runtime/engine.py:                         Updating global -> local expert ids
./DeepSpeed/deepspeed/runtime/engine.py:             Used to support old checkpoint loading
./DeepSpeed/deepspeed/runtime/engine.py:             Used to support new checkpoint loading
./DeepSpeed/deepspeed/runtime/engine.py:         It is required that (checkpoints_path, tag) are consistent among all ranks.
./DeepSpeed/deepspeed/runtime/engine.py:             Prepare for checkpoint load by ensuring all parameters are partitioned
./DeepSpeed/deepspeed/runtime/engine.py:             Pipeline parallelism uses this to load its own checkpoint files.
./DeepSpeed/deepspeed/runtime/engine.py:             print(checkpoint.keys())
./DeepSpeed/deepspeed/runtime/engine.py:         When use loading checkpoint serial, checkpoint loading start from local rank 0,
./DeepSpeed/deepspeed/runtime/engine.py:         all other local rank would be paused, waiting for its rank-1 peer ready and its notification.
./DeepSpeed/deepspeed/runtime/engine.py:                 transparently handle the old file pattern for optim_states
./DeepSpeed/deepspeed/runtime/engine.py:             Fully load state for current rank
./DeepSpeed/deepspeed/runtime/engine.py:                 Warn if loading checkpoint of different bit16 type
./DeepSpeed/deepspeed/runtime/engine.py:             Custom preparation for checkpoint save, if applicable
./DeepSpeed/deepspeed/runtime/engine.py:         This is to make sure the checkpoint names are created without collision
./DeepSpeed/deepspeed/runtime/engine.py:         There seems to be issue creating them in parallel
./DeepSpeed/deepspeed/runtime/engine.py:         Ensure save_dir directory exists
./DeepSpeed/deepspeed/runtime/engine.py:         Ensure tag is a string
./DeepSpeed/deepspeed/runtime/engine.py:         Ensure checkpoint tag is consistent across ranks
./DeepSpeed/deepspeed/runtime/engine.py:         We distribute the task of saving layer checkpoint files among
./DeepSpeed/deepspeed/runtime/engine.py:         data parallel instances, so all procs should call _save_checkpoint.
./DeepSpeed/deepspeed/runtime/engine.py:         All procs then call module_state_dict(), but only procs of data
./DeepSpeed/deepspeed/runtime/engine.py:         parallel rank 0 save the general model params.
./DeepSpeed/deepspeed/runtime/engine.py:         Save latest checkpoint tag
./DeepSpeed/deepspeed/runtime/engine.py:         A hack to save the checkpointing directory. Pipeline parallelism overrides
./DeepSpeed/deepspeed/runtime/engine.py:         module_state_dict() and uses this path to save the model. module_state_dict()
./DeepSpeed/deepspeed/runtime/engine.py:         then instead just returns None.
./DeepSpeed/deepspeed/runtime/engine.py:         Using layer_#_export_# to save the model's expert state_dict
./DeepSpeed/deepspeed/runtime/engine.py:                 print(expp_rank, exp_dp_rank)
./DeepSpeed/deepspeed/runtime/engine.py:                 get all moe parameters
./DeepSpeed/deepspeed/runtime/engine.py:                 print(moe_state_dict.keys()) # until now, everything is fine. So the bug happens at next few lines
./DeepSpeed/deepspeed/runtime/engine.py:                 Reorder the moe name rank, so that each checkpoint only has one expert
./DeepSpeed/deepspeed/runtime/engine.py:                     truncating extra tensor (shared) storage
./DeepSpeed/deepspeed/runtime/engine.py:                 let save the moe parameters
./DeepSpeed/deepspeed/runtime/engine.py:                     save the moe parameters
./DeepSpeed/deepspeed/runtime/engine.py:         In the case of E + D parallelism, only the
./DeepSpeed/deepspeed/runtime/engine.py:         first expert parallel group should save the expert weights
./DeepSpeed/deepspeed/runtime/engine.py:         since each expert parallel group is a copy of the model's experts
./DeepSpeed/deepspeed/runtime/engine.py:         Save optimizer states. They are different across each exp parallel rank.
./DeepSpeed/deepspeed/runtime/engine.py:         TODO: why use BufferedWriter not the path
./DeepSpeed/deepspeed/runtime/engine.py:         get non-moe parameters
./DeepSpeed/deepspeed/runtime/engine.py:             TODO: update num experts info,.. in checkpoint
./DeepSpeed/deepspeed/runtime/engine.py:         zero checkpoint files are created sequentially
./DeepSpeed/deepspeed/runtime/engine.py:         A hack to save the checkpointing directory. Pipeline parallelism overrides
./DeepSpeed/deepspeed/runtime/engine.py:         module_state_dict() and uses this path to save the model. module_state_dict()
./DeepSpeed/deepspeed/runtime/engine.py:         then instead just returns None.  The module_state_dict() implementation in
./DeepSpeed/deepspeed/runtime/engine.py:         PipelineEngine expects the save path to be set in self._curr_ckpt_path.
./DeepSpeed/deepspeed/runtime/engine.py:         we save buffer names so that we could extract later the real buffers from the saved
./DeepSpeed/deepspeed/runtime/engine.py:         state_dict["module"] in the non-zero checkpoint - the buffers are already there but they
./DeepSpeed/deepspeed/runtime/engine.py:         are intermixed with param placeholders
./DeepSpeed/deepspeed/runtime/engine.py:         have to traverse the tree to be able to skip non-persistent buffers
./DeepSpeed/deepspeed/runtime/engine.py:         zero2 started using a round_robin_bit16_groups which is a shuffled version of bit16_groups -
./DeepSpeed/deepspeed/runtime/engine.py:         if we don't use it, we get parameters ordered incorrectly
./DeepSpeed/deepspeed/runtime/engine.py:                 uncomment to debug zero_to_fp32.py problems
./DeepSpeed/deepspeed/runtime/engine.py:                 if self.global_rank == 0: print(f"saving param {name} {shape} (numel={shape.numel()})")
./DeepSpeed/deepspeed/runtime/engine.py:         if self.global_rank == 0: print(f"Total saved {numel} numels in {cnt} params")
./DeepSpeed/deepspeed/runtime/engine.py:             handle params
./DeepSpeed/deepspeed/runtime/engine.py:                 When weights are manged by stage 3, we can't rely on param.data_ptr() as it will be reused
./DeepSpeed/deepspeed/runtime/engine.py:                 as weights get gathered and reduced, but param.ds_id is unique across all zero weights
./DeepSpeed/deepspeed/runtime/engine.py:                 (and shared params will have the same param.ds_id)
./DeepSpeed/deepspeed/runtime/engine.py:                     shared weights
./DeepSpeed/deepspeed/runtime/engine.py:                    print(f"`{key}` is shared with `{shared_index[param_id]}`")
./DeepSpeed/deepspeed/runtime/engine.py:        logger.info(f"creating recovery script {dst}")
./DeepSpeed/deepspeed/runtime/engine.py:         make executable (safeguard for file shares - Azure as example)
./DeepSpeed/deepspeed/runtime/engine.py:            this message is used in unit test TestZeRONonDistributed
./DeepSpeed/deepspeed/runtime/engine.py:             gather one layer at a time to be memory-efficient
./DeepSpeed/deepspeed/runtime/engine.py:             must use modifier_rank=0 to release GPU memory after each layer gathered
./DeepSpeed/deepspeed/runtime/engine.py:            see_memory_usage("before GatheredParameters", force=True)
./DeepSpeed/deepspeed/runtime/engine.py:                     handle params
./DeepSpeed/deepspeed/runtime/engine.py:                         can't rely on param.data_ptr() as it will be reused as weights gets
./DeepSpeed/deepspeed/runtime/engine.py:                         gathered and reduced, but param.ds_id is unique across all zero weights
./DeepSpeed/deepspeed/runtime/engine.py:                         (and shared params will have the same param.ds_id)
./DeepSpeed/deepspeed/runtime/engine.py:                             shared weights
./DeepSpeed/deepspeed/runtime/engine.py:                            print(f"`{key}` is shared with `{shared_params[param.ds_id]}`")
./DeepSpeed/deepspeed/runtime/engine.py:                        print(f"param {param.ds_id} {param.shape} {key} ")
./DeepSpeed/deepspeed/runtime/engine.py:                     now buffers - not sure if need to take care of potentially shared weights here
./DeepSpeed/deepspeed/runtime/engine.py:            see_memory_usage("after GatheredParameters", force=True)
./DeepSpeed/deepspeed/runtime/engine.py:         Prepare for checkpoint save by ensuring all parameters are partitioned
./DeepSpeed/deepspeed/runtime/engine.py:                 consolidation is expensive in time and memory and therefore isn't a default
./DeepSpeed/deepspeed/runtime/engine.py:                 the model will be bogus if not consolidated so don't confuse the user by saving it
./DeepSpeed/deepspeed/runtime/zero/config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/zero/config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/zero/config.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/zero/config.py: ZeRO optimization. By default, this optimization is not enabled.
./DeepSpeed/deepspeed/runtime/zero/config.py: Users have to configure the desired optimization (0 means disabled) in params.json as below example:
./DeepSpeed/deepspeed/runtime/zero/config.py:     Validators
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:     https://stackoverflow.com/a/17795199
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py: ensure we only warn once, otherwise every iteration will trigger a warning
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:             namedtuples require a slightly different syntax.
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:         apply inplace to avoid recreating dict inherited objects
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:         this also applies to torch.Tensor's subclasses like torch.nn.parameter.Parameter
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:         restore zero param attributes if those get stripped by `backward_function`
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:for each tensor in outputs run the forward_function and register backward_function as hook
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:         Params can be registered as None (e.g., bias)
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:        print(f"After Forward: {ctx.module.__class__.__name__}")
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:        print(f"Before Backward: {ctx.module.__class__.__name__}")
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:            TODO SOME TIMES post backward does not seem to be triggered debug in detail
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:            Should only cause increase in memory not correctness issue
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:            if output.grad_fn.__class__.__name__ == 'ViewBackward':
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:                ctx.view=True
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:                print(f"Warning view tensor for input to module : {module.__class__.__name__}. Backward hooks may not trigger properly")
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:            assert len(module.parameters(recurse=False)), "The input tensor to the module is a view, and autograd Function or register_hook is not triggered with view tensors."
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:            if module.ds_grads_remaining == 0:
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:                print(f"Before Forward: {ctx.module.__class__.__name__}")
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:            print(f"After Backward: {ctx.module.__class__.__name__}")
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:             we need two registries, one for training and one for eval. They will be used when creating PartitionedParameterCoordinator
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:        reset step if in inference mode
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:        likely one of them should be enough but just to be safe
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:         Add top module to stack trace
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:        print(f"{module.__class__} : {module.id}")
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:                    print(f'got UNKNOWN type {type(output)}')
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:                     It's possible that the parameter was already external to the completed module. If so, remove it the
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:                     registration as it will be covered by the outer module instead.
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:                 some models (e.g. Albert) may run multiple forwards on the same layer in a loop
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:                 before doing backwards, so each backward will need a pre-fetch - using reference
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:                 counting to support this scenario
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:                print(f"COUNTER before: {sub_module.applied_pre_backward_ref_cnt}")
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:                print(f"COUNTER after: {sub_module.applied_pre_backward_ref_cnt}")
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:        This is an alternate to doing _post_backward_module_hook
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:        it uses tensor.register_hook instead of using torch.autograd.Function
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:            print(f"Before Forward {module.__class__.__name__}")
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:                    print(f"After backward {module.__class__.__name__}")
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:         Pre forward hook
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:         Post forward hook
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:         Pre backward hook
./DeepSpeed/deepspeed/runtime/zero/parameter_offload.py:         post backward hook
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py: Toggle this to true to enable correctness test
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py: with gradient partitioning and without
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         The fused optimizer does all the work. We need this layer for two reason:
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         1. maintain same user API from apex.fp16_utils
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         2. keep common stuff here in case we need to add ne552w fused optimizer later
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         differences from apex.fp16_utils:
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         - assume all model params in fp16
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         - assume all params requires grad
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         - flat by groups, not keeping state. TODO: remove state explicitly?
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         - master grad and unflat master weight never exist. TODO: a way to save out unflat master?
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         Use torch (un)flatten ops
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         ZeRO stage 1 (False) or 2 (True)
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:        expert parallel group
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:        data parallel group for experts
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:        data parallel size for non-experts
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:        For MoE models this maybe different for different param group
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:        It will be modified during MoE setup later in the init
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         CPU-Offload requires contiguous gradients
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         param flattened by groups
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         param partitioned by data parallel degree
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         this will contain a list of equal sized tensors
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         each of which will be updated by a different process
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         a single 32-bit partition of the parallel partitioned parameters
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         that this process will update
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         param partition info
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         These are the parameters in each group that will not be updated by this process directly
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         These are the parameters that will be updated by this process directly
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         Offset from the first parameter in the self.params_in_partition
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         the parameter boundaries may not align with partition boundaries
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         so we need to keep track of the offset
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         number of elements per partition in each group
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         align nccl all-gather send buffers to 4-byte boundary
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         Use different parallel to do all_to_all_reduce related things
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         padding on each partition for alignment purposes
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         loop to deal with groups
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             push this group to list before modify
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             TODO: Explore simplification that avoids the extra book-keeping by pushing the reordered group
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             not sure why apex was cloning the weights before flattening
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             removing cloning here
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             move all the parameters to cpu to free up GPU space for creating flat buffer
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             Reorder group parameters for load balancing of gradient partitioning during backward among ranks.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             This ensures that gradients are reduced in a fashion such that ownership round robins among the ranks.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             For example, rather than 3 gradients (g_n+2, g_n+1, g_n) that are reduced consecutively belonging
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             to the same rank, instead they will belong to 3 ranks (r_m+2, r_m+1, r_m).
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             create flat buffer in CPU and move to GPU
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             Record padding required for alignment
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             set model bit16 weight to slices of flattened buffer
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             divide the flat weights into near equal partition equal to the data parallel degree
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             each process will compute on a different part of the partition
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             verify that data partition start locations are 4-byte aligned
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             A partition of the fp32 master weights that will be updated by this process.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             Note that the params in single_partition_of_fp32_groups is cloned and detached
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             from the origin params of the model.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             Set local optimizer to have flat params of its own partition.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             After this, the local optimizer will only contain its own partition of params.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             In that case, the local optimizer only saves the states(momentum, variance, etc.) related to its partition's params(zero stage1).
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:        self.copy_grad_stream = get_accelerator().Stream()
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         map between param_id and bool to specify if a param is in this partition
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         simplified param id
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:        interesting code: unique ids being assigned to individual parameters
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         mapping from parameter to partition that it belongs to
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         stores if a partition has been reduced in this step
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         number of grads in partition that still need to be computed
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         total number of grads in partition
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         stores if a grad in a partition has been computed or not
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         stores the offset at which a parameter gradient needs to be inserted in a partition
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         the offset in the gradient at which it must be inserted at the beginning of the partition
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         will store the averaged gradients required by this partition
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         For cpu_offload, will store the averaged gradients required by this partition
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         store index of first parameter in each partition
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         initializes all data structures for implementing gradient partitioning
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         resets the data structure value for the next backward propagation
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         creates backward hooks for gradient partitioning
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         we may have a way of fusing dynamic scale. Do not support for now
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             Only fp16 should use dynamic loss scaling
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             Link bit16 and fp32 params in partition
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         if we're using ZeRO stage 2, ensure contiguous gradients are used
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         NOTE: To run ZeRO stage 1 with MoE, we need to set self.contiguous_gradients to True or ignore the assertion
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         set model fp16 weight to slices of reordered flattened buffer
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         disable round robin if need to debug something
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         return tensor_list, list(range(len(tensor_list)))
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         Initialize the optimizer states with the flattened fp32 partition.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         State initialization for the Adagrad optimizer occurs at construction as opposed to other optimizers
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         which do lazy initialization of the state at the first call to step.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:    ########################################################################
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:    ################### ZeRO Stage 1 - reduce gradients ####################
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:    ########################################################################
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         with PP we must create ipg buffer, since backward is handled outside zero
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         reduce any pending grads in either hook/non-hook case
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:    ########################################################################
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:    ########################ZeRO Partition Gradients########################
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:    ########################################################################
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         if dist.get_rank() == 0:
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:            logger.info("Params already reduced %s", self.params_already_reduced)
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             It is safe to clear previously reduced grads of other partitions
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         No need to keep the gradients anymore.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         All gradients required by the step
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         are in self.averaged_gradients
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     resets all partition to no reduced
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     sets remaining grads to the total number of grads in each partition
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     set is grad computed to false for all grads in partition
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     Clear the tensor the reduction gradient attribute is pointing to
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     create a flat tensor aligned at the alignment boundary
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:    ############## Independent Partition Gradient ########################
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 Swap ipg_index between 0 and 1
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 keeping the gradients contiguous to prevent memory fragmentation, and avoid flattening
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:        make sure the average tensor function knows how to average the gradients
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             Accumulate destination ranks and bucket offsets for each gradient slice.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             Note: potential future optimization, record access pattern of parameters
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             in backward pass and partition gradients w.r.t. access pattern so that our
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             bucket is guaranteed to be contiguous w.r.t. ranks
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             count = 0
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                Averages gradients at parameter level if ipg has a moe param
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                Otherwise averaging is done at the entire buffer level at the end of the loop
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 MoE param have different groups
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 Get all partition ids + their offsets
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 Calculate rank and offsets for grad slices
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                     if dist.get_rank() == 0 and count < 100:
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                         print(f"Rank {dist.get_rank()} rank offset id {idx} calculated dp size {dist.get_world_size(group=process_group)} real dp size {dist.get_world_size(self.real_dp_process_group[i])} and dst: {partition_id}")
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                     count += 1
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                     Calculate numel for grad slice depending on partition location
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                         Last partition_id uses its own offset
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                         Set numel to next partition's offset
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                     Merge bucket ranges if they belong to the same rank
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:    #############################################################################
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:    ############################ CPU Offload Methods#############################
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:    #############################################################################
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             we need to offset to get to the right element
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             we dont need all elements of the tensor
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         copy to a preexisiting buffer to avoid memory allocation penalty
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:        buffer for storing gradients for this parameter in CPU
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:        accumulate gradients into param.grad_accum or parts of it that belongs to this partition
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:        move accumulated gradients back to CPU
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         at the boundary we will send 32bit directly
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             Pipeline parallelism may replicate parameters. Avoid multi-counting.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 as some model have trainable parameters but skipped in training,
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 their backward hooks in self.create_reduce_and_remove_grad_hooks() will not run,
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 so they have no norm_for_param_grads
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                     As unused parameters in modules may not be expected sometimes,
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                     add an explicit error msg when it occurred and an option to
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                     avoid the error
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         Sum across all model parallel GPUs.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:    ###########################################################################################
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:        print(f"ID {self.get_param_id(param)} grad norm {param.grad.norm()}")
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         The allreduce buffer will be rewritten. Copy the gradients in partition to a new buffer
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:        print(f"Grad norm after copy to contiguous_buffer {param.grad.data.norm()}")
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             TODO: copy_grad_stream is disabled because of race with reduce. This hurts perf and should be fixed.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                        get_accelerator().synchronize()
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                        stream = self.copy_grad_stream
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                             Clear grads of other partitions during the next reduction
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                             to avoid clearing them before the reduction is complete.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:        ####################################################################
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:    #####################Reduction Related Methods##############################
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                "All Reducing"
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     if rank is specified do a reduction instead of an allreduce
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             It is safe to clear the previously reduced grads of other partitions
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     allows using reduction of gradients instead of using all_reduce
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:    ############################################################################
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:    ############################################################################
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:    ############################################################################
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     views the tensor as multiple partitions and returns
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     those partitions
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         dp_id = dist.get_rank(group=self.real_dp_process_group[group_id])
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         FP32 grad should never exist.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         For speed, set model fp16 grad to None by default
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         zero all pointers to grad tensors
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             Take max across all GPUs.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             if dist.get_rank() == 0:
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                logger.info(f"Total Norm beginning {total_norm}")
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 Pipeline parallelism may replicate parameters. Avoid multi-counting.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             Sum across all model parallel GPUs.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     creates a flat fused tensor from the tensor list starting at the first_offset
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     in the first tensor of the list. If there are not enough elements in the tensor
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     list then the flat tensor will be padded with zeros
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             we need to offset to get to the right element
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             we dont need all elements of the tensor
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             we need a narrow view of the tensor based on the tensor offset and number of elements that
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             we need from this tensor
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         this means its the last partition and does not align with the dp boundary. We need to pad before flattening
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         note that the get_global_norm function only supports l2 norm
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         Disabling this as the C++ side copy & synchronize is not working correctly
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:        from deepspeed.ops.adam import DeepSpeedCPUAdam
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:        if type(self.optimizer) == DeepSpeedCPUAdam and self.dtype == torch.half:
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:            self.optimizer.step(fp16_param_groups=[self.get_bit16_param_group(group_no)])
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:        else:
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:            self.optimizer.step()
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         First compute norm for all group so we know if there is overflow
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         Step 1:- Calculate gradient norm using bit-16 grads
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         Step 2:- run optimizer and upscaling simultaneously
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 Disabled, this is not currently working
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                from deepspeed.ops.adam import DeepSpeedCPUAdam
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                if not (type(self.optimizer) == DeepSpeedCPUAdam and self.dtype == torch.half):
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                    bit16_partitions = self.parallel_partitioned_bit16_groups[i]
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                    fp32_partition = self.single_partition_of_fp32_groups[i]
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                    bit16_partitions[partition_id].data.copy_(fp32_partition.data)
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 free gradients for all the parameters that are not updated by this process(ZeRO stage2)
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 create a flat gradients for parameters updated by this process
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 If we are last partition, ensure we have same size grads and partition size, if not pad with zero tensors
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 release all the gradient since we have already created a necessary copy in dp_grad_partition(ZeRO stage2)
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 Step 3:- run the optimizer if no offloading
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 Step 4:- get rid of the fp32 gradients. Not needed anymore
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         Gather the updated weights from everyone.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         Then all partitions of the model parameters are updated and ready for next round forward.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         TODO: we probably don't need this? just to be safe
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             print_rank_0(f'update_lp_params {i=} {partition_id=}', force=True)
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             if i == 0:
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 print_rank_0(f'{fp32_partition[:10]=}', force=True)
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         compute combined scale factor for this group
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             norm is in fact norm*scale
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     `params` is a list / generator of torch.Variable
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         Since each model parallel GPU carries only part of the model,
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         make sure overflow flag is synced across all the model parallel GPUs
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     `x` is a torch.Tensor
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             if x is half, the .float() incurs an additional deep copy, but it's necessary if
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             Pytorch's .sum() creates a one-element tensor of the same type as x
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             (which is true for some recent version of pytorch).
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             More efficient version that can be used if .sum() returns a Python scalar
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             cpu_sum = float(x.sum())
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             We want to check if inst is actually an overflow exception.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             RuntimeError could come from a different error.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             If so, we still want the exception to propagate.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             Use double buffers to avoid data access conflict when overlap_comm is enabled.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         Only for Stage 1, Mode 2
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     Promote state so it can be retrieved or set via "fp16_optimizer_instance.state"
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     Promote param_groups so it can be retrieved or set via "fp16_optimizer_instance.param_groups"
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     (for example, to adjust the learning rate)
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     Promote loss scale so it can be retrieved or set via "fp16_optimizer_instance.loss_scale"
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     Return group tensor after removing paddings that are added for alignment to DP world size.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     This method works on the assumption that each group contains a single flattened tensor.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     Return optimizer state after removing paddings that are added for alignment.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     Return base optimizer states.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     This method assumes that each param group contains a single flattened tensor.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 Assuming "step" is the only item that changes through training iterations
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         Remove paddings for DP alignment to enable loading for other alignment values
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     Restore base optimizer fp32 weights from elastic checkpoint by:
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     1) Merging fp32 weights from checkpoints of all partitions
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     2) Extracting fp32 weights for current partition from merged weights
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     3) Using extracted weights to update base optimizer weights directly.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     Restore base optimizer fp32 weights from ZeRO fp16 or bfloat16 weights
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     Refresh the fp32 master params from the fp16 or bfloat16 copies.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     Extract optimizer state for current partition from merged states of all partitions
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             Assume non-tensor states are not partitioned and equal across ranks, so return first one
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     Restore base optimizer state from elastic checkpoint by
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     1) Merging optimizer state from checkpoints of all partitions
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     2) Extracting optimizer state for current partition from the merged state
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     3) Using the extracted value to directly update the base optimizer.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         Restore step
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                    print(f"Loading {self.param_names[lp]} {tp_rank=} {tp_world_size=}")
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         zero stage 1 mode
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         I think it should actually be ok to reload the optimizer before the model.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         padding is always at the last rank/partition
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         if DP=1024 and param-group elems=16 -> padding will be 1024-16 across all but one rank
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         scenario-1 (shrink): saving w. 4 gpus -> loading w. 2 gpus
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         scenario-2 (expand): saving w. 2 gpus -> loading w. 4 gpus
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         if load_optimizer_states:
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             if new_dp_size:
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 self.strip_padding()
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 self.add_padding_w_new_dp_size()
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             self.optimizer.load_state_dict(current_rank_sd[BASE_OPTIMIZER_STATE])
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 loading rigid ckpt into either rigid or elastic exec
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                     loading elastic into elastic exec
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                     loading an elastic checkpoint into rigid exec
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         At this point, the optimizer's references to the model's fp32 parameters are up to date.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         The optimizer's hyperparameters and internal buffers are also up to date.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         However, the fp32 master copies of the model's fp16 params stored by the optimizer are still
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         out of date.  There are two options.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         1:  Refresh the master params from the model's fp16 params.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         This requires less storage but incurs precision loss.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         2:  Save and restore the fp32 master copies separately.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         We choose option 1 if changing DP degree and option 2 otherwise.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:        
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         Pytorch Optimizer.load_state_dict casts saved buffers (e.g. momentum) to the type and device
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         of their associated parameters, because it's possible those buffers might not exist yet in
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         the current optimizer instance.  In our case, as long as the current FP16_Optimizer has been
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         constructed in the same way as the one whose state_dict we are loading, the same master params
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:         are guaranteed to exist, so we can just copy_() from the saved master params.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             option 2 from above
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:                 For non-elastic checkpoint, simply copying from saved weights of current rank is sufficient.
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:             option 1 from above
./DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:     shared params calculated only once
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:     other variations
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:     - print for all ranks w/o interleaving
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:     printflock(f"[{rank}] {message}")
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:     - print to log file per rank
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:     log_rank_file(rank, message)
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:    . Register a weight that is used in another module's forward pass (line 6).
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                     self.layer1.weight is required by self.layer2.forward
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:     same as regular pytorch parameters
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:     parameters are partitioned across data parallel process
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:     the parameter is held with a unique process rank
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:     and is not available on all other process
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:     parameters are fully present and ready for use on all processes
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:     parameters are either partitioned or remote in some or all process
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:     parameters are being gathered.
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py: https://stackoverflow.com/a/63851681/9201239
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         need to make sure that we don't free the parameter while it is still
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         being used for computation
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:     param.data doesn't store anything meaningful in partitioned state
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py: Inserts _post_init_method at the end of init method
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py: for all sub classes of torch.nn.Module
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         Exiting the top level context
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         Now that we cleaned up the metaclass injection, raise the exception.
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:     To be implemented by inheriting classes
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                     TODO Delay error checking for dangling partitioned parameters to post module init
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                     raise RuntimeError(f"not all parameters for {module_to_apply_fn_to.__class__.__name__}, "
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                                        f"were zero params, is it possible that the parameters were "
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                                        f"overwritten after they were initialized? "
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                                        f"params: {[p for p in module_to_apply_fn_to.parameters(recurse=False)]} ")
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 important logic: We want to run post_init only after child's __init__ is
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 completed, and do nothing after __init__ of any of its parents and grandparents in
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 the inheritance ancestry. This way the partitioning will need to happen only once
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 when the whole object is ready to be partitioned and not before. This is because
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 often the child module will need to tweak the weights - for example running a
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 custom weights init function. So if a parent created the weights param, the child
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 won't need to gather it in order to tweak it
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                     child's __init__ was called, since parents all see the same object they can now skip post_init
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                     child's __init__ is done, now we can run a single post_init on the child object
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         Replace .__init__() for all existing subclasses of torch.nn.Module recursively
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         holding onto some methods so we can put them back the way they were in __exit__
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         Replace .__init__() for future subclasses of torch.nn.Module
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             putting methods back the way we found them
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         split the single tensor out into individual tensors
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:     a placeholder object to store all quant related vars used in handles
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py: Replaces all parameters in module with Scattered Parameters
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:        . allocates tensors to either GPU or CPU memory or NVMe
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:        . converts floating point tensors to half precision
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:        . immediately partitions tensors among the group of data-parallel devices
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:        . (*optional*) replaces ``torch.nn.functional.linear`` with a more
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:        . Allocate a model and partition it among all processes:
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:        . Allocate a model in pinned CPU memory and partition it among a subgroup of processes:
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:        . Partition an already-allocated model in CPU memory:
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         Local device is the device where the parameters are consumed, must be default device.
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         It is the device where parameters are fully instantiated using allgather
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         Remote device is the device where parameter partitions are stored
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         It can be same as local_device or it could be CPU or NVMe.
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         Enable fp16 param swapping to NVMe
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         If we are provided an already-allocated module to prepare.
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:        see_memory_usage(f"Before converting params in {module.__class__.__name__}", force=False)
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         Partitioned, Normal, Remote
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         Replicated vs Partitioned vs Inflight
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         Stores the shape of the original tensor
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         Stores the number of elements in the original parameter without padding
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         Stores the partitioned copy of the tensor
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         Keeps track of how many active sub-modules need this param at any given point in time
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         If this flag is true, then the parameters are replicated throughput training
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         And only partitioned before the step
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         The group that the parameter is scattered across.
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         Stores the secondary partitioned copy of the tensor
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:        Process group for secondary partition all (group) gather
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         This is set to the Async Param swapper if remote device is nvme
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         else this is set to None
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         DeepSpeed Param ID
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:            Fix get_partition_dp_group(params[0]))
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             fetches from nvme if the partition is not available and in nvme
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:            use appropriate all gather process group
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:            pprint(dir(ds_process_group))
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             ensure that each rank has params in same order. the allgather
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             is done by flattening the parameter list into a single tensor that
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             can be allgathered in a single call - this means that if each rank
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             gives a list of the same parameters in a different order we will
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             silently get incorrect parameter values, and have very difficult
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             to debug correctness issues.
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 ensure that same list (with same ordering) of parameters are
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 being allgathered across all ranks, otherwise could mix
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 data between tensors.
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 ensure that tensors from each rank agree on the same ds_numel
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 otherwise could mix data between tensors.
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 have an opportunity to avoid some intermediate memory allocations
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         Collectives for gathering and partitioning parameters
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         Collective for averaging gradients
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         Partitioning size utilities
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         fetches from nvme if the partition is not available and in nvme
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         note: param_list may contain params that are already in flight / aviailable. So we need to use all_gather_list
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 _allgather_params_coalesced always return None
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             if param.ds_tensor is not None:
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                assert id(param.data) == id(param.ds_tensor.data), \
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                "After the parameters are initially partitioned, make sure we are not recreating the partition."
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:            print_rank_0(f"After Partitioning Param {param.ds_id} {param.ds_tensor.size()} {param.ds_tensor}",force=False)
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             if reuse_buffers and False:
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 numel = buffer.numel()
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 buffer = param.data.view(-1)
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 print_rank_0(
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                     "Returning buffer for param {param.ds_id} with numel {param.ds_numel} to empty buffers",
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                     force=False)
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 if numel in empty_buffers:
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                     empty_buffers[numel].append(buffer)
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             if deepspeed.comm.get_rank():
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                print(f"Releasing {param.data.numel()}")
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                print_rank_0(f"Param  {param.ds_id} pri {param.ds_tensor.size()}  loc? {param.ds_tensor.final_location}", force=True)
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                param.data = param.ds_tensor.data
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 param.data does not store anything meaningful in partitioned state
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                     quantize the tensor if it's not trainable
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                     make sure param.ds_tensor requires_grad always be false,
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                     otherwise, torch tracer will complain.
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                partitioned_tensor = src_tensor.clone().detach().to(self.remote_device)
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 partitioned_tensor = torch.zeros(partition_size,
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                                                  dtype=param.dtype,
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                                                  device=self.remote_device )
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                         make sure param.ds_tensor requires_grad always be false,
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                         otherwise, torch tracer will complain.
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:            print(f"Remote device {self.remote_device}")
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:            param.ds_tensor = partitioned_tensor
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:            param.data = param.ds_tensor.data
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             param.data does not store anything meaningful in partitioned state
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:        #support for NVME secondary param offload
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:        print_rank_0(f"SEC Param id {param.ds_id} status is {param.ds_status}", force=True)
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:            check padding
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             quantize the tensor if it's not trainable
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:            use rank in group for secondary tensor
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                if not flat_tensor.numel() > 100000:
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                    replicated_tensor = flat_tensor.narrow(0,
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                                                           0,
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                                                           param.ds_numel).view(param.ds_shape)
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                    param.data = replicated_tensor.data
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                    return None
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         collect local tensors and partition sizes
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         allocate memory for allgather params
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         launch
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 try the _all_gather_base from Pytorch master
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 back to old all_gather function
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         Wait ensures the operation is enqueued, but not necessarily complete.
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         assign to param.data (not copy)
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         guarantee the communication to be completed
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:            param_offset += param.data.numel()
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:        print_rank_0([param.grad for param in param_list])
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:        assert any([param.grad is None for param in param_list]), "None gradients cannot be reduce scattered"
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             some ranks may have partitions that are padded to go beyond the grad size.
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             For these ranks the output of reduce scatter is a separate buffer and needs
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             to be copied in
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:            print_rank_0("REduce scatter was executed for param {param.ds_id}")
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:        output = torch.empty(partition_size, dtype=param.dtype, device=param.device)
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:            print("before reduce scatter gradients")
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:            print("after reduce scatter gradients")
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:        import pdb;pdb.set_trace()
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         param.grad=None
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         param.grad.test()
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:        print("before partition gradients")
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             just copy the grad partition to the buffer
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             if source and destination are on same device,
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             add to the provided buffer
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             if source and destination are on different device, copy first to src
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             then add and move back to the destination. This seems to run faster
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             when src is gpu and dest is cpu
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             adding directly to cpu is very slow
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             partition_buffer.view(-1).narrow(
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 0,
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 0,
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 elements).copy_(param.grad.view(-1).narrow(0,
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                                                         start,
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                                                         elements))
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:        print("after partition gradients")
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:        . Allocate a partitioned module, initialize its weight on rank 0, and update all
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:        . Collect a partitioned weight to pass to another module during
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                     self.layer1.weight is required by self.layer2.forward
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:        . Pretrained model loading
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                     because zero3 puts placeholders in model params, this context
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                     manager gathers (unpartitions) the params of the current layer, then loads from
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                     the state dict and then re-partitions them again
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             deal with generators like model.parameters()
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             must convert to list to be able to iterate more than once if we get a generator
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             single param
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:         enable if at least one is zero-param, otherwise a noop
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:                 A group was specified; convert DP rank to global rank
./DeepSpeed/deepspeed/runtime/zero/partition_parameters.py:             is a no-op if already registered
./DeepSpeed/deepspeed/runtime/zero/linear.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/zero/linear.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/zero/linear.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/zero/linear.py:Linear Module to use with ZeRO Stage 3 to allow for parameter memory release
./DeepSpeed/deepspeed/runtime/zero/linear.py:after the module execution during forward
./DeepSpeed/deepspeed/runtime/zero/linear.py:Instead of saving variables using save_for_backward, we save variable ids
./DeepSpeed/deepspeed/runtime/zero/linear.py:Allowing us to retrieve the variable without creating pointer to it
./DeepSpeed/deepspeed/runtime/zero/linear.py:Which allows for underlying tensor to be garbage collected
./DeepSpeed/deepspeed/runtime/zero/linear.py:When partitioned as needed by the Zero Stage 3 optimizer
./DeepSpeed/deepspeed/runtime/zero/linear.py:TODO instead of patching Linear module, we could patch the ctx.save_for_backward
./DeepSpeed/deepspeed/runtime/zero/linear.py:ctx.saved_tensors so that this approach works for all nn modules that are built upon
./DeepSpeed/deepspeed/runtime/zero/linear.py:torch.nn.function. However the issue is that many modules uses C++ implementations
./DeepSpeed/deepspeed/runtime/zero/linear.py:which does not have pytorch implementation. Eg torch.addmm which acts as a functional
./DeepSpeed/deepspeed/runtime/zero/linear.py:when implemented outside of torch.autograd.Function
./DeepSpeed/deepspeed/runtime/zero/linear.py:     Note that both forward and backward are @staticmethods
./DeepSpeed/deepspeed/runtime/zero/linear.py:     bias is an optional argument
./DeepSpeed/deepspeed/runtime/zero/linear.py:             fused op is marginally faster
./DeepSpeed/deepspeed/runtime/zero/linear.py:     This function has only a single output, so it gets only one gradient
./DeepSpeed/deepspeed/runtime/zero/linear.py:         This is a pattern that is very convenient - at the top of backward
./DeepSpeed/deepspeed/runtime/zero/linear.py:         unpack saved_tensors and initialize all gradients w.r.t. inputs to
./DeepSpeed/deepspeed/runtime/zero/linear.py:         None. Thanks to the fact that additional trailing Nones are
./DeepSpeed/deepspeed/runtime/zero/linear.py:         ignored, the return statement is simple even when the function has
./DeepSpeed/deepspeed/runtime/zero/linear.py:         optional inputs.
./DeepSpeed/deepspeed/runtime/zero/linear.py:        print(f"backward shaped grad_output {grad_output.shape}, input {input.shape}, weight {weight.shape} and bias {bias.shape if bias is not None else None}")
./DeepSpeed/deepspeed/runtime/zero/linear.py:         These needs_input_grad checks are optional and there only to
./DeepSpeed/deepspeed/runtime/zero/linear.py:         improve efficiency. If you want to make your code simpler, you can
./DeepSpeed/deepspeed/runtime/zero/linear.py:         skip them. Returning gradients for inputs that don't require it is
./DeepSpeed/deepspeed/runtime/zero/linear.py:         not an error.
./DeepSpeed/deepspeed/runtime/zero/linear.py:            print(f"Computing grad input weight {weight.shape} grad_output {grad_output.shape}")
./DeepSpeed/deepspeed/runtime/zero/linear.py:            print(f"Computed grad input {grad_input.shape}")
./DeepSpeed/deepspeed/runtime/zero/linear.py:            print("Computing grad weight")
./DeepSpeed/deepspeed/runtime/zero/linear.py:            print(f"Computed grad weight grad_weight {grad_weight.shape}")
./DeepSpeed/deepspeed/runtime/zero/linear.py:            print("Computing grad bias")
./DeepSpeed/deepspeed/runtime/zero/linear.py:            print("Done computing grad bias")
./DeepSpeed/deepspeed/runtime/zero/linear.py:            print("needs bias")
./DeepSpeed/deepspeed/runtime/zero/linear.py:        print(f"backward shaped grad_input {grad_input.shape}, grad_weight {grad_weight.shape}, grad_bias {grad_bias.shape if grad_bias is not None else None}")
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_profiler.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_profiler.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_profiler.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_profiler.py:                f'{event_ctr.name}: time = {self._log_timers()},count = {event_ctr.count}, numel = {event_ctr.num_elem}',
./DeepSpeed/deepspeed/runtime/zero/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/zero/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/zero/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/zero/mics_utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/zero/mics_utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/zero/mics_utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/zero/mics_utils.py: Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
./DeepSpeed/deepspeed/runtime/zero/mics_utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/zero/mics_utils.py:     env var for debugging purpose
./DeepSpeed/deepspeed/runtime/zero/mics_utils.py:     full size of the world
./DeepSpeed/deepspeed/runtime/zero/mics_utils.py:     global rank
./DeepSpeed/deepspeed/runtime/zero/mics_utils.py:     for simplicity
./DeepSpeed/deepspeed/runtime/zero/mics_utils.py:     create shard groups
./DeepSpeed/deepspeed/runtime/zero/mics_utils.py:     create replicate groups
./DeepSpeed/deepspeed/runtime/zero/mics_utils.py:     assign shard group size as world size
./DeepSpeed/deepspeed/runtime/zero/mics_utils.py:         create hierarchy inter-node, intra-node groups
./DeepSpeed/deepspeed/runtime/zero/mics_utils.py:         n_span_nodes = config['shard_span']
./DeepSpeed/deepspeed/runtime/zero/mics_utils.py:         create communicators
./DeepSpeed/deepspeed/runtime/zero/test.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/zero/test.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/zero/test.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/zero/test.py:    print(f"a4:{a4}")
./DeepSpeed/deepspeed/runtime/zero/test.py:    print(f"a7:{a7}")
./DeepSpeed/deepspeed/runtime/zero/test.py:    print(f"a8:{a8}")
./DeepSpeed/deepspeed/runtime/zero/test.py:    print(f"a10:{a10}")
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py:        address to contiguous size available
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py:        tensor id to its address
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py:        tensor address to its size
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py:        tensor address to ids
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py:        id to tensors
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py:        id to params. Maps each tensor buffer to list of parameters that uses it
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py:    create a tensor of size from the pre-allocated buffer
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py:    if not enough free space will fail
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py:    if not enough contiguous space, will defragment and allocate
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py:            set the param data to the new tensor buffer locations
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py:    assigns the tensor data to the param data and keeps track of the assignment
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py:    any change the underlying buffer from defragmentation will cause a
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py:    reassignment of the param data
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py:    deletes the tensor and frees up the underlying buffer
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py:    shows the current memory allocation at specified resolution
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py:    to be called after defragmentation that moves the tensor buffers
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py:    this call reassigns the data of all the parameters using the tensor buffers
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py:        consolidate next buffer
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py:        consolidate previous buffer
./DeepSpeed/deepspeed/runtime/zero/contiguous_memory_allocator.py:                    print_rank_0(f'empty addr : {empty_addr}, empty size {empty_size} tensor addr {tensor_addr} tensor size {tensor_size}')
./DeepSpeed/deepspeed/runtime/zero/utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/zero/utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/zero/utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/zero/utils.py: Add apex FusedAdam to supported list if apex is installed
./DeepSpeed/deepspeed/runtime/zero/utils.py:         device=get_accelerator().current_device_name(),
./DeepSpeed/deepspeed/runtime/zero/mics.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/zero/mics.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/zero/mics.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/zero/mics.py: Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
./DeepSpeed/deepspeed/runtime/zero/mics.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/zero/mics.py:         let the current stream to op
./DeepSpeed/deepspeed/runtime/zero/mics.py:        . Allocate a model and partition it among all processes:
./DeepSpeed/deepspeed/runtime/zero/mics.py:                 the config_dict_or_path is required to let the context manager know
./DeepSpeed/deepspeed/runtime/zero/mics.py:                 how partition the parameters.
./DeepSpeed/deepspeed/runtime/zero/mics.py:                 The configuration has to include the field ``mics_shard_size``
./DeepSpeed/deepspeed/runtime/zero/mics.py:        . Allocate a model in pinned CPU memory and partition it among a subgroup of processes:
./DeepSpeed/deepspeed/runtime/zero/mics.py:        . Partition an already-allocated model in CPU memory:
./DeepSpeed/deepspeed/runtime/zero/mics.py:         attach communication groups to every param
./DeepSpeed/deepspeed/runtime/zero/mics.py:         record existing all_gather_coalesced implementation
./DeepSpeed/deepspeed/runtime/zero/mics.py:         so that we can fallback later
./DeepSpeed/deepspeed/runtime/zero/mics.py:         change the all_gather_coalesced method
./DeepSpeed/deepspeed/runtime/zero/mics.py:         fetches from nvme if the partition is not available and in nvme
./DeepSpeed/deepspeed/runtime/zero/mics.py:         ensure that each rank has params in same order. the allgather
./DeepSpeed/deepspeed/runtime/zero/mics.py:         is done by flattening the parameter list into a single tensor that
./DeepSpeed/deepspeed/runtime/zero/mics.py:         can be allgathered in a single call - this means that if each rank
./DeepSpeed/deepspeed/runtime/zero/mics.py:         gives a list of the same parameters in a different order we will
./DeepSpeed/deepspeed/runtime/zero/mics.py:         silently get incorrect parameter values, and have very difficult
./DeepSpeed/deepspeed/runtime/zero/mics.py:         to debug correctness issues.
./DeepSpeed/deepspeed/runtime/zero/mics.py:         must have to change the status of the param
./DeepSpeed/deepspeed/runtime/zero/mics.py:         and ensure they are on the device
./DeepSpeed/deepspeed/runtime/zero/mics.py:         inter node all-gather
./DeepSpeed/deepspeed/runtime/zero/mics.py:         sync enqueue
./DeepSpeed/deepspeed/runtime/zero/mics.py:         intra node all-gather
./DeepSpeed/deepspeed/runtime/zero/mics.py:             partition param into multiple chunks for allgather
./DeepSpeed/deepspeed/runtime/zero/mics.py:             because inter-node all-gather outputs are in a continues memory
./DeepSpeed/deepspeed/runtime/zero/mics.py:             while in param memory, those inter-node data are placed in different
./DeepSpeed/deepspeed/runtime/zero/mics.py:             location.
./DeepSpeed/deepspeed/runtime/zero/mics.py:             each chunk is an intra-node output
./DeepSpeed/deepspeed/runtime/zero/mics.py:         overload the dp_process_group and partition_count
./DeepSpeed/deepspeed/runtime/zero/mics.py:         perform all-reduce among replication groups
./DeepSpeed/deepspeed/runtime/zero/mics.py:         the function will perform accumulation boundary check
./DeepSpeed/deepspeed/runtime/zero/mics.py:         TODO: improve the condition check
./DeepSpeed/deepspeed/runtime/zero/mics.py:             manually coalescing all-reduce
./DeepSpeed/deepspeed/runtime/zero/tiling.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/zero/tiling.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/zero/tiling.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/zero/tiling.py:     Get the size and dimension.
./DeepSpeed/deepspeed/runtime/zero/tiling.py:     Split.
./DeepSpeed/deepspeed/runtime/zero/tiling.py:     Note: torch.split does not create contiguous tensors by default.
./DeepSpeed/deepspeed/runtime/zero/tiling.py:         global, not necessarily local
./DeepSpeed/deepspeed/runtime/zero/tiling.py:         Build partition-lists. These are CSR-style splits [0, part0, part1, ..., features]
./DeepSpeed/deepspeed/runtime/zero/tiling.py:         For example, row_parts[p] gives the start of partition p and row_parts[p+1]
./DeepSpeed/deepspeed/runtime/zero/tiling.py:         is the exclusive end.
./DeepSpeed/deepspeed/runtime/zero/tiling.py:                if input_size is split, we only need one bias
./DeepSpeed/deepspeed/runtime/zero/tiling.py:         Optionally initialize with a known tensor
./DeepSpeed/deepspeed/runtime/zero/tiling.py:             no splits
./DeepSpeed/deepspeed/runtime/zero/tiling.py:            this clone is necessary to preserve auto grad
./DeepSpeed/deepspeed/runtime/zero/tiling.py:            there is some issue with inplace update for outputs that are views
./DeepSpeed/deepspeed/runtime/zero/tiling.py:         stack output tensors
./DeepSpeed/deepspeed/runtime/zero/tiling.py:         stack biases if applicable
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:     Record trace of the network during a single forward+backward (for training) or forward (for inference)
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:     Use recorded network trace to optimize current forward+backward or forward
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:     Recorded trace does not match current forward+backward or forward pass.
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         mapping of param -> handle for each param that is currently in flight
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         keeps track of the number of submodules invoked so far.
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         network tracing mode
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         sequence of submodules/parameters in forward pass + backward pass
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         number of available params, and max number of available params
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         max distance between two use of the module beyond which module is released
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         queue for parameters to fetch. parameters will be popped off the left
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         side of the dequeue as they are fetched
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         stream that will be used for allgather operations
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         limit the number of fetch events that can be queued at once
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         otherwise, what happens is memory is allocated by the host thread at the
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         time of the call, but not used until later by the asynchronous cuda stream.
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         allowing an infinite number of these to queue up causes a lot of memory
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         pressure that then becomes detrimental to performance.
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         this is a much less elegant way of fixing this vs something like using
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         cudaMallocAsync/cudaFreeAsync. Choosing to not expose this to the user now
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         because ideally in the future its replaced by an async allocation
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         mechanism which doesn't require any configuration by the user.
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         TODO. make this configurable via JSON
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:             sub_module must match expectation else invalidate trace cache
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:             Make sure that recorded submodule orders are identical across ranks
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:                 Successfully recorded a trace
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:                 Make sure that recorded parameter orders are identical across ranks
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:                 Enable trace recording for next forward/backward pass
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:             kick off all gather for params in the immediately required submodule
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:            for param in params_to_fetch:
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         wait for parameters in the immediately needed submodule to become available
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         kick off parameter prefetches for upcoming modules
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         don't prefetch if we dont have a completed model trace
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:             go through the parameters we need for the current module and pop them
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:             off the fetch queue so that they aren't prefetched later.
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:             if params have already been popped off the fetch queue by earlier
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:             prefetches we won't look for them here
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:             kick off all gather for params in the next few submodules (prefetch)
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:                         nvme prefetch is handled elsewhere. Need to break here to preserve fetch order
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:                         Avoid duplicates
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:             TODO. make this throw if if there are still active submodules. currently
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:             there's a hook execution issue
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:             Release swap buffers for persisted params on nvme since they will never be partitioned or evicted from GPU
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         Problem: When prefetcher scans the param trace, it skips AVAILABLE params.
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         This creates issues if those params are released before the skipped uses:
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         1) It hurts performance as the skipped uses are never prefetched.
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         2) For nvme params, we run out of swap buffers because the prefetch order
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         diverges from the trace.
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         Solution: Don't release params whose reuse was skipped by prefetch. This is
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         possible because we detect such skips during prefetch and mark those params.
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         examine all modules within `max_reuse_dist_in_numel` of the current step,
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         if we see any of the candidate parameters to be released reoccur while
./DeepSpeed/deepspeed/runtime/zero/partitioned_param_coordinator.py:         doing this, remove them from the set of parameters to release.
./DeepSpeed/deepspeed/runtime/zero/offload_config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/zero/offload_config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/zero/offload_config.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/zero/stage3.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/zero/stage3.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/zero/stage3.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/zero/stage3.py: Toggle this to true to enable correctness test
./DeepSpeed/deepspeed/runtime/zero/stage3.py: with gradient partitioning and without
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     other variations
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     - print for all ranks w/o interleaving
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     printflock(f"[{rank}] {message}")
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     - print to log file per rank
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     log_rank_file(rank, message)
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         The fused optimizer does all the work. We need this layer for two reason:
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         1. maintain same user API from apex.fp16_utils
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         2. keep common stuff here in case we need to add ne552w fused optimizer later
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         differences from apex.fp16_utils:
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         - assume all model params in fp16
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         - assume all params requires grad
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         - flat by groups, not keeping state. TODO: remove state explicitly?
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         - master grad and unflat master weight never exist. TODO: a way to save out unflat master?
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         Use torch (un)flatten ops
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        num of ranks in a ZeRO param partitioning group
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         backup fused_adam optimizer init
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             Multiple param_groups configs for back-up optimizer
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        ## streams used for overlapping computation with communication
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        ###########################################################################
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        -------------Stage 3 Setup-------------------#
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         Holds the mode parameter
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         The param.data may not hold any meaningful data
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         when param's status is NOT_AVAILABLE or IN_FLGHT
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         Hold partitioned parameters
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         Holds a fused and flattened copy of the parameters
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        defragmented pinned memory
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        a single 32-bit partition of the parallel partitioned parameters
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        that this process will update
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         number of elements per partition in each group
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         padding on each partition for alignment purposes
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         Trainable parameters
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         Optimizer tensor swapping
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         TODO. make this configurable via JSON
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         map between param_id and bool to specify if a param is in this partition
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         simplified param id
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        Largest partitioned param
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         stores if a partition has been reduced in this step
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         stores if a grad in a partition has been computed or not
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         will store the averaged gradients required by this partition
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        creates backward hooks for gradient partitioning
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        ##Calls all gather param
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        exit(0)
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         we may have a way of fusing dynamic scale. Do not support for now
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         To support pipelined optimizer swapping
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         IPG
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     TODO. factor out to a utility outside of stage3
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             move the tensor from device memory to host memory
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             record some data so we can restore the device tensor later
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         copy tensors (now flattened and contiguous) back to GPU
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         restore device tensors
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        ##################### offload optimizer setup ##################################
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        ##################### offload param setup ##################################
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             this dst buffer is on NVMe, so skip this
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             Final location must be gpu/cpu in this case
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         bookkeeping related to param groups
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                 record sub group and partitions
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                 record sub group -> group mapping
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                 record total elements of parameter partitions in sub group
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                 record padding required to align group to world size (only applies to last rank)
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         move parameters to flattened buffer
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             move parameter partitions into a single contiguous flat buffer
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             setup flat buffers per subgroup, these are each just sections of the
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             contiguous flat buffer for all parameters that we created earlier
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             create a flat CPU memory allocation for each param group
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                    Flat buffer may not be available for parameters that reside in NVME
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         if necessary, create a pinned memory buffer to be used for swapping out
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         params to NVME after optimizer step
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         Assign portion of subgroup to cpu, the other to gpu.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             print(f"Partial offload sub_group_size is {sub_group_size}, ratio is {self.partial_offload}\n")
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             a partition of the fp32 master weights that will be updated by this process
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         Clear for on-the-fly population before the optimizer step
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         State initialization for the Adagrad optimizer occurs at construction as opposed to other optimizers
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         which do lazy initialization of the state at the first call to step.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             Initialize the optimizer states with the flattened fp32 partition.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         Initialize the optimizer states with the flattened fp32 partition.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         Reset steps
./DeepSpeed/deepspeed/runtime/zero/stage3.py:    ########################################################################
./DeepSpeed/deepspeed/runtime/zero/stage3.py:    ########################ZeRO Partition Gradients########################
./DeepSpeed/deepspeed/runtime/zero/stage3.py:    ########################################################################
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        in case of cpu offload, averaged gradients are already in fp32_partitioned_groups_flat.grad
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        TODO: use a similar code path for both cpu_offload and non-cpu offload
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                TODO: This is redundant
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         this method gets called after every backward. need to increment
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         here because if it gets incremented in backward() the micro step
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         id will be off by one when we do the reduce and partition at the.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         start of this method.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         TODO. make this less error prone
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                    print_rank_0(f" Before all gather {param.device}, {param.shape}")
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                     The hook must be created in un-partitioned parameter
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                    print(f"After all gather {param.device}, {param.shape}")
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                    print(f"param grad fn {param.expand_as(param).grad_fn}")
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                     Partition the parameter after creating the hook
./DeepSpeed/deepspeed/runtime/zero/stage3.py:    ##############Independent Partition Gradient ########################
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        print_rank_0(f"Inside reduce ipg buckets. {debug_param2name_id_shape(param)}, ipg elements {self.elements_in_ipg_bucket}, reduce bucket size {self.reduce_bucket_size}", force=True)
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         Because the ipg bucket is initialized with a random place holder tensor, we must
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         explicitly check that the bucket has any real data in it (self.elements_in_ipg_bucket >
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         0). Otherwise if the incoming param.ds_numel is large, this branch may get triggered on a
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         garbage data and `self.average_tensor()` will crash because its params_to_reduce will be
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         empty, while reduction_list will have that garbage data.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             move the gradient to a contiguous buffer
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                 move the parameter's gradient to the contiguous flat buffer
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                print(f"param id {param_id} i:{i}, ds_tensor {num_elements} numel {param.numel()}")
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        self.norm_for_param_grads[param_id] = param.grad.data.double().norm(2)
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        Using a more memory efficient version
./DeepSpeed/deepspeed/runtime/zero/stage3.py:            print(f"src_tensor {src_tensor.size()} and fp32 grad {fp32_grad_tensor.size()}")
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         Sum across all model parallel GPUs.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                 this grad partition is empty - don't need to do anything
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             move or accumulate gradient partition to target buffer
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                 ensure grad buffer is a CUDA buffer to speed up the next few
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                 operations and so it can be used asynchronously
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                 if dst is CPU, copy first to src device, do the addition
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                 there, then move back to dst. adding directly to cpu is very slow
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                 ensure grad buffer is a CUDA buffer to speed up the next few
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                 operations and so it can be used asynchronously
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             offload the gradient partition if applicable
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             free the gradient
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        print_rank_0(f"Backward {debug_param2name_id_shape(param)}", force=True)
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                 skip small parameters
./DeepSpeed/deepspeed/runtime/zero/stage3.py:    #####################Reduction Related Methods##############################
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                "All Reducing"
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     if rank is specified do a reduction instead of an allreduce
./DeepSpeed/deepspeed/runtime/zero/stage3.py:    ############################################################################
./DeepSpeed/deepspeed/runtime/zero/stage3.py:    ############################################################################
./DeepSpeed/deepspeed/runtime/zero/stage3.py:    ############################################################################
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     views the tensor as multiple partitions and returns
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     those partitions
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         FP32 grad should never exist.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         For speed, set model fp16 grad to None by default
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             Take max across all GPUs.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             if dist.get_rank() == 0:
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                logger.info(f"Total Norm beginning {total_norm}")
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             Sum across all model parallel GPUs.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                 FIX https://github.com/microsoft/DeepSpeed/issues/3564
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     creates a flat fused tensor from the tensor list starting at the first_offset
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     in the first tensor of the list. If there are not enough elements in the tensor
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     list then the flat tensor will be padded with zeros
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             we need to offset to get to the right element
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             we dont need all elements of the tensor
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             we need a narrow view of the tensor based on the tensor offset and number of elements that
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             we need from this tensor
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         this means its the last partition and does not align with the dp boundary. We need to pad before flattening
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         release all the gradient since we have already created a necessary copy in dp_grad_partition
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         get rid of the fp32 gradients. Not needed anymore
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     create a flat tensor aligned at the alignment boundary
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         get rid of the fp32 gradients. Not needed anymore
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         First compute norm for all group so we know if there is overflow
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        loss scaling related computation
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        Gathering persisting parameters
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         self.invalidate_secondary_tensor() # given that we want hpz in forward pass when no_grad is set, we need to keep the secondary tensor
./DeepSpeed/deepspeed/runtime/zero/stage3.py:            unflatten fp16 parameter subgroup
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        checks for overflow, adjust the loss scale accordingly
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         Stash unscaled gradient norm
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        update parameters one sub group at a time
./DeepSpeed/deepspeed/runtime/zero/stage3.py:            prepare optimizer states, gradients and fp32 parameters for update
./DeepSpeed/deepspeed/runtime/zero/stage3.py:            scale the fp32 gradients
./DeepSpeed/deepspeed/runtime/zero/stage3.py:            apply the optimizer step on the sub group and copy fp32 parameters to fp16
./DeepSpeed/deepspeed/runtime/zero/stage3.py:            put fp16 parameters in appropriate location
./DeepSpeed/deepspeed/runtime/zero/stage3.py:            release memory or swap out optimizer states of fp32 parameters
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         warn user about caching allocator flushes
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         Dump gradient norms for debugging
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         Dump gradient norms for debugging
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         compute combined scale factor for this group
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             norm is in fact norm*scale
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     `params` is a list / generator of torch.Variable
./DeepSpeed/deepspeed/runtime/zero/stage3.py:                     logical_or_ not available in older versions of pytorch
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         Since each model parallel GPU carries only part of the model,
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         make sure overflow flag is synced across all the model parallel GPUs
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     `x` is a torch.Tensor
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             if x is half, the .float() incurs an additional deep copy, but it's necessary if
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             Pytorch's .sum() creates a one-element tensor of the same type as x
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             (which is true for some recent version of pytorch).
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             More efficient version that can be used if .sum() returns a Python scalar
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             cpu_sum = float(x.sum())
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             We want to check if inst is actually an overflow exception.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             RuntimeError could come from a different error.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             If so, we still want the exception to propagate.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:    ## Local API START ###
./DeepSpeed/deepspeed/runtime/zero/stage3.py:    ## Local API END ###
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     Promote state so it can be retrieved or set via "fp16_optimizer_instance.state"
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     Promote param_groups so it can be retrieved or set via "fp16_optimizer_instance.param_groups"
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     (for example, to adjust the learning rate)
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     Promote loss scale so it can be retrieved or set via "fp16_optimizer_instance.loss_scale"
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         Remove paddings from flattened tensor
./DeepSpeed/deepspeed/runtime/zero/stage3.py:        logger.info(f'rank {dist.get_rank()}: lean_tensors = {[t.numel() for t in lean_tensors]}')
./DeepSpeed/deepspeed/runtime/zero/stage3.py:    TODO REVISIT this for stage 3
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         Return optimizer states after removing paddings.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         This method assumes that each param group contains a single flattened tensor.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         Return group tensor after removing paddings added for alignment to DP world size.
./DeepSpeed/deepspeed/runtime/zero/stage3.py: Restore base optimizer fp32 weights from checkpoint by:
./DeepSpeed/deepspeed/runtime/zero/stage3.py: 1) Merging fp32 weights from checkpoints of all partitions
./DeepSpeed/deepspeed/runtime/zero/stage3.py: 2) Extracting fp32 weights for current partition from merged weights
./DeepSpeed/deepspeed/runtime/zero/stage3.py: 3) Using extracted weights to update base optimizer weights directly.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     Restore base optimizer fp32 weights from ZeRO fp16 weights
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     Refresh the fp32 master params from the fp16 copies.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     Extract flattened partition for current rank from all partitions
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         Assume non-tensor states are not partitioned and equal across ranks, so return first one
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     Restore base optimizer state from checkpoint by
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     1) Merging optimizer state from checkpoints of all partitions
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     2) Extracting optimizer state for current partition from the merged state
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     3) Using the extracted value to directly update the base optimizer.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         I think it should actually be ok to reload the optimizer before the model.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         restore fp32 partitions
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         restore fp16 partitions from fp32
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         update fp16 unflattened params
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     TODO: Support different/changing load/save DP degree.
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         when use loading checkpoint serial, after finish loading, we need to
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         delete the temp state_dict_list variable to save memory, then trigger
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         the next rank's loading
./DeepSpeed/deepspeed/runtime/zero/stage3.py:             self.persistent_parameters[0].all_gather(self.persistent_parameters) # this will be done in checkpoint_event_epilogue() so remove it to prevent double all_gather
./DeepSpeed/deepspeed/runtime/zero/stage3.py:     shared params calculated only once
./DeepSpeed/deepspeed/runtime/zero/stage3.py:         assuming no shared params within a single layer
./DeepSpeed/deepspeed/runtime/utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/utils.py:         No model parallelism in easy :)
./DeepSpeed/deepspeed/runtime/utils.py:         New Megatron and DeepSpeed convention (post pipeline-parallelism release)
./DeepSpeed/deepspeed/runtime/utils.py:         Some DeepSpeed + pipeline parallelism versions
./DeepSpeed/deepspeed/runtime/utils.py:         Deprecated Megatron and DeepSpeed convention
./DeepSpeed/deepspeed/runtime/utils.py:         TODO: I don't think reduce_overflow is needed if mpu is None
./DeepSpeed/deepspeed/runtime/utils.py:             In this case, we need to do an all_reduce across
./DeepSpeed/deepspeed/runtime/utils.py:             the expert_parallel_group, so that if there was
./DeepSpeed/deepspeed/runtime/utils.py:             an overflow due to expert weights, we detect it
./DeepSpeed/deepspeed/runtime/utils.py:             Only need to check groups.get_largest_expert_parallel_group()
./DeepSpeed/deepspeed/runtime/utils.py:     `params` is a list / generator of torch.Variable
./DeepSpeed/deepspeed/runtime/utils.py:         Since each model parallel GPU carries only part of the model,
./DeepSpeed/deepspeed/runtime/utils.py:         make sure overflow flag is synced across all the model parallel GPUs
./DeepSpeed/deepspeed/runtime/utils.py:         deepspeed.comm.all_reduce(overflow_gpu,
./DeepSpeed/deepspeed/runtime/utils.py:                                     op=deepspeed.comm.ReduceOp.MAX,
./DeepSpeed/deepspeed/runtime/utils.py:                                     group=mpu.get_model_parallel_group())
./DeepSpeed/deepspeed/runtime/utils.py:             All reduce this across expert_parallel_group, so that if an expert
./DeepSpeed/deepspeed/runtime/utils.py:             overflows, we detect it here
./DeepSpeed/deepspeed/runtime/utils.py:     `x` is a torch.Tensor
./DeepSpeed/deepspeed/runtime/utils.py:             if x is half, the .float() incurs an additional deep copy, but it's necessary if
./DeepSpeed/deepspeed/runtime/utils.py:             Pytorch's .sum() creates a one-element tensor of the same type as x
./DeepSpeed/deepspeed/runtime/utils.py:             (which is true for some recent version of pytorch).
./DeepSpeed/deepspeed/runtime/utils.py:             More efficient version that can be used if .sum() returns a Python scalar
./DeepSpeed/deepspeed/runtime/utils.py:             cpu_sum = float(x.sum())
./DeepSpeed/deepspeed/runtime/utils.py:             We want to check if inst is actually an overflow exception.
./DeepSpeed/deepspeed/runtime/utils.py:             RuntimeError could come from a different error.
./DeepSpeed/deepspeed/runtime/utils.py:             If so, we still want the exception to propagate.
./DeepSpeed/deepspeed/runtime/utils.py:     logger.info(f'norm_list = {norm_list} global = {sqrt(total_norm)}')
./DeepSpeed/deepspeed/runtime/utils.py:         Take max across all GPUs.
./DeepSpeed/deepspeed/runtime/utils.py:         Sum across all model parallel GPUs.
./DeepSpeed/deepspeed/runtime/utils.py:     Need to average total_norm across different GPUs due to the presence of moe params
./DeepSpeed/deepspeed/runtime/utils.py:         Take max across all GPUs.
./DeepSpeed/deepspeed/runtime/utils.py:             Pipeline parallelism may replicate parameters. Avoid multi-counting.
./DeepSpeed/deepspeed/runtime/utils.py:             Filter to avoid over-counting replicated tensors from tensor
./DeepSpeed/deepspeed/runtime/utils.py:             model parallelism
./DeepSpeed/deepspeed/runtime/utils.py:         Sum across all model parallel GPUs.
./DeepSpeed/deepspeed/runtime/utils.py:         Pipeline parallelism may replicate parameters. Avoid multi-counting.
./DeepSpeed/deepspeed/runtime/utils.py:         Filter to avoid over-counting replicated tensors from tensor
./DeepSpeed/deepspeed/runtime/utils.py:         model parallelism
./DeepSpeed/deepspeed/runtime/utils.py:     Sum across all model parallel GPUs.
./DeepSpeed/deepspeed/runtime/utils.py:         Take max across all GPUs.
./DeepSpeed/deepspeed/runtime/utils.py:             Pipeline parallelism may replicate parameters. Avoid multi-counting.
./DeepSpeed/deepspeed/runtime/utils.py:             Filter to avoid over-counting replicated tensors from tensor
./DeepSpeed/deepspeed/runtime/utils.py:             model parallelism
./DeepSpeed/deepspeed/runtime/utils.py:         Sum across all model parallel GPUs.
./DeepSpeed/deepspeed/runtime/utils.py:     First check for the trivial edge case
./DeepSpeed/deepspeed/runtime/utils.py:     initialize partitioning
./DeepSpeed/deepspeed/runtime/utils.py:         Jump to the next bucket
./DeepSpeed/deepspeed/runtime/utils.py:         Find the end index of partition p
./DeepSpeed/deepspeed/runtime/utils.py:         Nothing more to partition, return early
./DeepSpeed/deepspeed/runtime/utils.py:             See if the current partition is overweight.
./DeepSpeed/deepspeed/runtime/utils.py:         Next partition target
./DeepSpeed/deepspeed/runtime/utils.py:     Do a binary search for the best partitioning
./DeepSpeed/deepspeed/runtime/utils.py:     First check for the trivial edge case
./DeepSpeed/deepspeed/runtime/utils.py:     Find the smallest bottleneck (weight of heaviest partition)
./DeepSpeed/deepspeed/runtime/utils.py:     Now compute that partitioning
./DeepSpeed/deepspeed/runtime/utils.py:         [N, list0, ..., listN-1]
./DeepSpeed/deepspeed/runtime/utils.py:         Partition is encoded like the rowptr of a CSR matrix:
./DeepSpeed/deepspeed/runtime/utils.py:         [num_parts, rank, 0, part_1, ..., part_num_parts]
./DeepSpeed/deepspeed/runtime/utils.py:         TODO: support shuffle between different partition granularities
./DeepSpeed/deepspeed/runtime/utils.py:         Allocate the full tensor as a flat buffer.
./DeepSpeed/deepspeed/runtime/utils.py:         Prepare all-gather buffer
./DeepSpeed/deepspeed/runtime/utils.py:         Collect the full tensor
./DeepSpeed/deepspeed/runtime/utils.py:     convert to GB for printing
./DeepSpeed/deepspeed/runtime/utils.py:     python doesn't do real-time garbage collection so do it explicitly to get the correct RAM reports
./DeepSpeed/deepspeed/runtime/utils.py:     Print message except when distributed but not rank 0
./DeepSpeed/deepspeed/runtime/utils.py:     get the peak memory to report correct data, so reset the counter for the next call
./DeepSpeed/deepspeed/runtime/utils.py:         Sequential AllGather Best of both worlds
./DeepSpeed/deepspeed/runtime/utils.py:         Sequential AllGather Best of both worlds
./DeepSpeed/deepspeed/runtime/utils.py:             no groups share optimizer states
./DeepSpeed/deepspeed/runtime/utils.py:             pipeline parallel with bf16 will default call this even if dp size = 1.
./DeepSpeed/deepspeed/runtime/utils.py:         Enforce nccl/rccl alignment of start location of each shard
./DeepSpeed/deepspeed/runtime/activation_checkpointing/config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/activation_checkpointing/config.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/activation_checkpointing/config.py:########################################
./DeepSpeed/deepspeed/runtime/activation_checkpointing/config.py:  DeepSpeed Activation Checkpointing
./DeepSpeed/deepspeed/runtime/activation_checkpointing/config.py:########################################
./DeepSpeed/deepspeed/runtime/activation_checkpointing/config.py: Activation Checkpointing Allows to save memory by only keeping a select few
./DeepSpeed/deepspeed/runtime/activation_checkpointing/config.py:activations for the backpropagation.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/activation_checkpointing/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py: Parts of the code here are adapted from PyTorch
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py: repo: https://github.com/pytorch/pytorch
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py: DeepSpeed Checkpointing Enabled or Disabled
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py: MP parameters
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py: Model Parameters
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py: Checkpointing buffers
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py: optimization flags
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py: Default name for the model parallel rng tracker.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         older PyTorch
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         newer PyTorch
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         Map from a string name to the cuda rng state.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         Seeds are just for book keeping and ensure no seed is set twice.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         Check seed is not already used.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         Check that state is not already defined.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         Get the current rng state.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         Set the new state and store it.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         Reset rng state to what it was.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         Check if we have added the state
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         Store current rng state.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         Set rng state to the desired one
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         Do the stuff we wanted to do.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             Update the current rng state for later use.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             And set the state to the original state we started with.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py: RNG tracker object.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:     2718 is just for fun and any POSITIVE value will work.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:     Data parallel gets the original seed.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:     Set the default state.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:     and model parallel state.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         don't need to do all_gather if model parallel is not enabled
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:     remove the flags that are assigned to the size of the flattened tensors
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             Because the 'new_empty' returns uninitialized pages,
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             the pages need to be populated during the cudaMemcpy time
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             which increases the data copy time. To avoid this, we
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             pre-populate these pages by simply writing 0 ahead of
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             the actual cudaMemcpy operation time. Due to the
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             previously launched GPU kernels, there is a small
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             window of time here for CPUs to populate pages asynchronously.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         just in case something funky is happening such as reuse of inputs
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         Copy the rng states.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         ctx.save_for_backward(*args)
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         Tensors returned from forward() may not be differentiable.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         removing pointers to the contiguous buffer memory
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         so that they can be garbage collected once the checkpoints
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         have been used
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             frees up all the pointers to the checkpoints except for the ones
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             stored by save for backward
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             with get_accelerator().stream(transport_stream):
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         Add non tensor input args
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         Store the current states.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         Set the states to what it used to be before the forward pass.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         if PARTITION_ACTIVATIONS:
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             current_stream=get_accelerator().current_stream()
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             current_stream.wait_stream(transport_stream)
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         Set the states back to what it was at the start of this function.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         Filter out non tensor outputs
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         Construct arguments to autograd.backward().
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         This is usually just outputs and grads, but forward() can return tensors that
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         are not differentiable.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         Force clear our stashed tensors to prevent a memory leak in certain scenarios
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:     just in case something funky is happening such as reuse of inputs
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:     Copy the rng states.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:     weakref seems utilized to discover the tensor deletion before a whole
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:     forward backward pair loop finished
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         if this is a leaf tensor, save it for backward progression trace
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         leaf tensor used to be input or parameters, which is not activations and
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         has no memory overhead
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         if this is the first step of backward probagation, recompute the graph and save
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         all the activations with the same order as `checkpoint_pack` does
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             removing pointers to the contiguous buffer memory
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             so that they can be garbage collected once the checkpoints
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             have been used
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:                 frees up all the pointers to the checkpoints except for the ones
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:                 stored by save for backward
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             gather inputs which is partitioned or checkpointed before first forward
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:                 with get_accelerator().stream(transport_stream):
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             Add non tensor input args
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             Store the current states.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             Set the states to what it used to be before the forward pass.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:             Set the states back to what it was at the start of this function.
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         frees up all the pointers to the checkpoints except for the ones
./DeepSpeed/deepspeed/runtime/activation_checkpointing/checkpointing.py:         stored by save for backward
./DeepSpeed/deepspeed/runtime/sparse_tensor.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/sparse_tensor.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/sparse_tensor.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/compression/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/compression/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/compression/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/compression/cupy.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/compression/cupy.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/compression/cupy.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/dataloader.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/dataloader.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/dataloader.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/dataloader.py: DataLoader([(torch.randn(3, 3), torch.tensor(i % 2)) for i in range(10)], batch_size=2))
./DeepSpeed/deepspeed/runtime/progressive_layer_drop.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/progressive_layer_drop.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/progressive_layer_drop.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/state_dict_factory.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/state_dict_factory.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/state_dict_factory.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/state_dict_factory.py:            logger.info(f'rank: {mp_rank} loading checkpoint: {load_path}')
./DeepSpeed/deepspeed/runtime/state_dict_factory.py:        logger.info(f'checkpoint file list: {self.ckpt_list}')
./DeepSpeed/deepspeed/runtime/state_dict_factory.py:         check checkpoint count is same with saved mp_world_size
./DeepSpeed/deepspeed/runtime/state_dict_factory.py:        # Q/K/V data need special processing
./DeepSpeed/deepspeed/runtime/state_dict_factory.py:        # merge or split on axis=0
./DeepSpeed/deepspeed/runtime/state_dict_factory.py:        # merge or split on axis=1
./DeepSpeed/deepspeed/runtime/state_dict_factory.py:        # no change required
./DeepSpeed/deepspeed/runtime/state_dict_factory.py:             [(3 * np * hn), h]
./DeepSpeed/deepspeed/runtime/state_dict_factory.py:             [(np * hn * 3), h] or [(np * 3 * hn), h]
./DeepSpeed/deepspeed/runtime/state_dict_factory.py:             [(3 * np * hn), h]
./DeepSpeed/deepspeed/runtime/state_dict_factory.py:             [(np * hn * 3), h] or [(np * 3 * hn), h]
./DeepSpeed/deepspeed/runtime/state_dict_factory.py:        self.sanity_check(self.ckpt_list[0])
./DeepSpeed/deepspeed/runtime/state_dict_factory.py:         partial_key is a sub-string of one key in the sd
./DeepSpeed/deepspeed/runtime/state_dict_factory.py:         Use 0 if version info doesn't exist
./DeepSpeed/deepspeed/runtime/checkpoint_engine/nebula_checkpoint_engine.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/checkpoint_engine/nebula_checkpoint_engine.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/checkpoint_engine/nebula_checkpoint_engine.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/checkpoint_engine/nebula_checkpoint_engine.py:         -2 means: customer needs to  explicitly tell nebula
./DeepSpeed/deepspeed/runtime/checkpoint_engine/nebula_checkpoint_engine.py:         current checkpoint is complete by commit method.
./DeepSpeed/deepspeed/runtime/checkpoint_engine/nebula_checkpoint_engine.py:             In some cases, there is the inconsistent tag between deepspeed metadata (latest file)
./DeepSpeed/deepspeed/runtime/checkpoint_engine/nebula_checkpoint_engine.py:             and nebula metadata, will lead to the failure on loading with deepspeed tag. Then we
./DeepSpeed/deepspeed/runtime/checkpoint_engine/nebula_checkpoint_engine.py:             will try to load the valid latest checkpoint from nebula(tier3 > tier1). So, in summary
./DeepSpeed/deepspeed/runtime/checkpoint_engine/nebula_checkpoint_engine.py:             when met failure loading for given tag, the loading priority would be like:
./DeepSpeed/deepspeed/runtime/checkpoint_engine/nebula_checkpoint_engine.py:                           nebula tier3 latest > nebula tier1 latest.
./DeepSpeed/deepspeed/runtime/checkpoint_engine/nebula_checkpoint_engine.py:             nebula tier3 latest
./DeepSpeed/deepspeed/runtime/checkpoint_engine/nebula_checkpoint_engine.py:                 nebula tier1 latest
./DeepSpeed/deepspeed/runtime/checkpoint_engine/nebula_checkpoint_engine.py:         nebula commit will be call when all files under give tag are ready to be persisted in the async way.
./DeepSpeed/deepspeed/runtime/checkpoint_engine/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/checkpoint_engine/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/checkpoint_engine/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/checkpoint_engine/checkpoint_engine.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/checkpoint_engine/checkpoint_engine.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/checkpoint_engine/checkpoint_engine.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/checkpoint_engine/checkpoint_engine.py:     init checkpoint engine for save/load
./DeepSpeed/deepspeed/runtime/checkpoint_engine/checkpoint_engine.py:         create checkpoint on give tag for save/load.
./DeepSpeed/deepspeed/runtime/checkpoint_engine/checkpoint_engine.py:         to tell checkpoint services if all files are ready.
./DeepSpeed/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         param flattened by groups
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         loop to deal with groups
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:             push this group to list before modify
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:             init fp16 weight buffer, flattened
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:             set model fp16 weight to slices of flattened buffer
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:             init master weight, flattened
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:             modify optimizer of have flat master weight
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         we may have a way of fusing dynamic scale. Do not support for now
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:        model parallel object
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         For speed, set model fp16 grad to None by default
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         First compute norm for all group so we know if there is overflow
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         Stash unscaled gradient norm
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         norm is in fact norm*cur_scale
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         TODO: we probably don't need this? just to be safe
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         First determine if there is overflow.
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:             Clear gradients
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         Stash unscaled gradient norm
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:        get rid of the fp32 gradients. Not needed anymore
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:        all_groups_norm_old = all_groups_norm
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         Need to allreduce (avg) the norms across different ranks because moe params will not be synced during allreduce
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:        print(f"old = {all_groups_norm_old} and new = {all_groups_norm} at rank: {deepspeed.comm.get_rank()}")
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         compute combined scale factor for this group
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:             norm is in fact norm*scale
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:                 Ensure self.scale_window updates since last overflow
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:     Promote state so it can be retrieved or set via "fp16_optimizer_instance.state"
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:     Promote param_groups so it can be retrieved or set via "fp16_optimizer_instance.param_groups"
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:     (for example, to adjust the learning rate)
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:     Refresh fp32 master params from fp16 copies
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         I think it should actually be ok to reload the optimizer before the model.
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         At this point, the optimizer's references to the model's fp32 parameters are up to date.
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         The optimizer's hyperparameters and internal buffers are also up to date.
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         However, the fp32 master copies of the model's fp16 params stored by the optimizer are still
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         out of date.  There are two options.
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         1:  Refresh the master params from the model's fp16 params.
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         This requires less storage but incurs precision loss.
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         2:  Save and restore the fp32 master copies separately.
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         We choose option 2.
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:        
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         Pytorch Optimizer.load_state_dict casts saved buffers (e.g. momentum) to the type and device
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         of their associated parameters, because it's possible those buffers might not exist yet in
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         the current optimizer instance.  In our case, as long as the current FP16_Optimizer has been
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         constructed in the same way as the one whose state_dict we are loading, the same master params
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:         are guaranteed to exist, so we can just copy_() from the saved master params.
./DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py:     Promote loss scale so it can be retrieved or set via "fp16_optimizer_instance.loss_scale"
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         param groups
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         loop to deal with groups
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:            fp16 weights that represents the actual model weights
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:            creating a fp32 copy of the weights that will be updated first then
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:            copied to fp16 weights
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:            in case the internal optimizer needs it
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:            setting the param groups in the optimizer to point to fp32
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:            note these are not the weights used by the model
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:            the model uses the fp16 version that we added to fp16_group
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         we may have a way of fusing dynamic scale. Do not support for now
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         FP32 grad should never exist outside of the step function
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         For speed, set model fp16 grad to None by default
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         First compute norm for all group so we know if there is overflow
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:                remove the fp32 grad
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:                copy data from fp32 to fp16
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:             copying gradients to fp32 to work with fp32 parameters
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:                remove the fp32 grad
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:                copy data from fp32 to fp16
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         compute combined scale factor for this group
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:             norm is in fact norm*scale
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:                 Ensure self.scale_window updates since last overflow
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:     Promote state so it can be retrieved or set via "fp16_optimizer_instance.state"
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:     Promote param_groups so it can be retrieved or set via "fp16_optimizer_instance.param_groups"
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:     (for example, to adjust the learning rate)
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:     Promote loss scale so it can be retrieved or set via "fp16_optimizer_instance.loss_scale"
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:     Refresh fp32 master params from fp16 copies
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         I think it should actually be ok to reload the optimizer before the model.
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         At this point, the optimizer's references to the model's fp32 parameters are up to date.
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         The optimizer's hyperparameters and internal buffers are also up to date.
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         However, the fp32 master copies of the model's fp16 params stored by the optimizer are still
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         out of date.  There are two options.
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         1:  Refresh the master params from the model's fp16 params.
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         This requires less storage but incurs precision loss.
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         2:  Save and restore the fp32 master copies separately.
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         We choose option 2.
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:        
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         Pytorch Optimizer.load_state_dict casts saved buffers (e.g. momentum) to the type and device
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         of their associated parameters, because it's possible those buffers might not exist yet in
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         the current optimizer instance.  In our case, as long as the current FP16_Optimizer has been
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         constructed in the same way as the one whose state_dict we are loading, the same master params
./DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py:         are guaranteed to exist, so we can just copy_() from the saved master params.
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py:         Empty initializer. Set handle based on the comm backend as follows.
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py:         backward compatibility
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py:         assuming a list/generator of parameter means single group
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py:                 State initialization
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py:                     Exponential moving average of gradient values
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py:                     Exponential moving average of squared gradient values
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py:                     Some scalars to help scale the variance update/local step policies
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py:                     Accumulation of momentum, i.e., the u variable in the 0/1 Adam paper
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py:                     self.freeze_key = True
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py:                     According to 0/1 Adam theory, a fixed variance would allow more accurate estimation of momentum
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py:                     However, in practice, we can also disable the manual freezing of variance, since the interval of
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py:                     updating variance will increase exponentially, so that it has negligible effect on the estimation.
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py:             We need to reinitialize the error buffers when local step > 1 since
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py:             the errors will be logged for different metrics (gradient vs. accumulated momentum).
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py:         Because at different stage exp_avg_mask may change (e.g.,
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py:         BERT pre-training seqlen 128 and 512 ), we don't use the exp_avg_mask
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py:         in checkpoints but always use the one user provided in training script.
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py:         (See example in DeepSpeedExamples/bing_bert/deepspeed_train.py.)
./DeepSpeed/deepspeed/runtime/fp16/onebit/zoadam.py:         Thus here we keep the exp_avg_mask unchanged when loading checkpoint
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         Empty initializer. Set handle based on the comm backend as follows.
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         backward compatibility
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         assuming a list/generator of parameter means single group
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:        remove the previous stats
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:                 Compute the scaling_coeff for each momentum at the end of warmup stage.
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:                 This is used to reduce compression error during compression stage.
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:                 State initialization
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:                     Exponential moving average of gradient values
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:                     Exponential moving average of squared gradient values
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:                     warmup stage, baseline Lamb optimization
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:                     compression stage, update each momentum locally, then
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:                     communicate based on the compressed_allreduce below
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         init fused momentum
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:                     Because 1-bit compression cannot represent exact zero, it is required to
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:                     provide a momentum mask for those params that have constant exact zeros in their
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:                     momentums, otherwise the compression error would keep accumulating.
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:                     For example, for BERT pre-training seq 128, bert.embeddings.position_embeddings.weight
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:                     always have exact zeros in its momentum for row 129 to 512, because it only
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:                     learns up to seq length 128 while the model supports up to 512 seq length.
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:                     (See example in DeepSpeedExamples/bing_bert/deepspeed_train.py about how
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:                     to add this exp_avg_mask for BERT pre-training.)
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         Because at different stage exp_avg_mask may change (e.g.,
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         BERT pre-training seqlen 128 and 512 ), we don't use the exp_avg_mask
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         in checkpoints but always use the one user provided in training script.
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         (See example in DeepSpeedExamples/bing_bert/deepspeed_train.py.)
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         Thus here we keep the exp_avg_mask unchanged when loading checkpoint
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         need to reset the fused momentum since loading states will break the linking
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         We reset the compression errors when loading checkpoints for 3 reasons:
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         1) The worker and server error at each GPU are distinct, so in current implementation
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         only rank 0's errors are saved in the checkpoint. Thus we have to reset the errors.
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         If we want to save them correctly we need O(num_gpu*model_size) memory in order to
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         gather all the error, which is a very large memory requirement. It's possible to save
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         them in a distributed way, but it will make the checkpoint saving/loading much more complicated.
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         2) Even if we are able to save the compression errors correctly, you need to have the
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         exact same number of GPUs in order to load them correctly.
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         3) We verified on BERT pre-training that occasionally resetting the compression error
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         at checkpoint loading does not affect the convergence.
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         However, please avoid frequent checkpoint loading which could break the error
./DeepSpeed/deepspeed/runtime/fp16/onebit/lamb.py:         compensation mechanism thus affect the convergence.
./DeepSpeed/deepspeed/runtime/fp16/onebit/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/fp16/onebit/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/fp16/onebit/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:         Empty initializer. Set handle based on the comm backend as follows.
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:         backward compatibility
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:         assuming a list/generator of parameter means single group
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:                 State initialization
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:                     Exponential moving average of gradient values
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:                     Exponential moving average of squared gradient values
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:                         Because 1-bit compression cannot represent exact zero, it is required to
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:                         provide a momentum mask for those params that have constant exact zeros in their
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:                         momentums, otherwise the compression error would keep accumulating.
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:                         For example, for BERT pre-training seq 128, bert.embeddings.position_embeddings.weight
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:                         always have exact zeros in its momentum for row 129 to 512, because it only
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:                         learns up to seq length 128 while the model supports up to 512 seq length.
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:                         (See example in DeepSpeedExamples/bing_bert/deepspeed_train.py.)
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:         Because at different stage exp_avg_mask may change (e.g.,
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:         BERT pre-training seqlen 128 and 512 ), we don't use the exp_avg_mask
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:         in checkpoints but always use the one user provided in training script.
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:         (See example in DeepSpeedExamples/bing_bert/deepspeed_train.py.)
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:         Thus here we keep the exp_avg_mask unchanged when loading checkpoint
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:         We reset the compression errors when loading checkpoints for 3 reasons:
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:         1) The worker and server error at each GPU are distinct, so in current implementation
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:         only rank 0's errors are saved in the checkpoint. Thus we have to reset the errors.
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:         If we want to save them correctly we need O(num_gpu*model_size) memory in order to
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:         gather all the error, which is a very large memory requirement. It's possible to save
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:         them in a distributed way, but it will make the checkpoint saving/loading much more complicated.
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:         2) Even if we are able to save the compression errors correctly, you need to have the
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:         exact same number of GPUs in order to load them correctly.
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:         3) We verified on BERT pre-training that occasionally resetting the compression error
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:         at checkpoint loading does not affect the convergence.
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:         However, please avoid frequent checkpoint loading which could break the error
./DeepSpeed/deepspeed/runtime/fp16/onebit/adam.py:         compensation mechanism thus affect the convergence.
./DeepSpeed/deepspeed/runtime/fp16/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/fp16/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/fp16/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py: item() is a recent addition, so this helps with backward compatibility.
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:         print(f'LossScalerBackward: {scaled_loss=}')
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:     `params` is a list / generator of torch.Variable
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:     `x` is a torch.Tensor
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:     `params` is a list / generator of torch.Variable
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:     `x` is a torch.Tensor
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:             if x is half, the .float() incurs an additional deep copy, but it's necessary if
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:             Pytorch's .sum() creates a one-element tensor of the same type as x
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:             (which is true for some recent version of pytorch).
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:             More efficient version that can be used if .sum() returns a Python scalar
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:             cpu_sum = float(x.sum())
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:             We want to check if inst is actually an overflow exception.
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:             RuntimeError could come from a different error.
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:             If so, we still want the exception to propagate.
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:     `overflow` is boolean indicating whether the gradient overflowed
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:             self.cur_scale /= self.scale_factor
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py: Although loss scaling is only defined for fp16, yet for backwards compatibility
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py: we still create a scaler for other dtypes (fp32, bf16) which does not perform any scaling.
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:#############################################################
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py: Example usage below here -- assuming it's in a separate file
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:#############################################################
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:     N is batch size; D_in is input dimension;
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:     H is hidden dimension; D_out is output dimension.
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:     Create random Tensors to hold inputs and outputs, and wrap them in Variables.
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:         Run backprop
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:         Check for overflow
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:         If no overflow, unscale grad and update as usual
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:         Otherwise, don't do anything -- ie, skip iteration
./DeepSpeed/deepspeed/runtime/fp16/loss_scaler.py:         Update loss scale for next iteration
./DeepSpeed/deepspeed/runtime/config_utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/runtime/config_utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/runtime/config_utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/runtime/config_utils.py:         Get information about the deprecated field
./DeepSpeed/deepspeed/runtime/config_utils.py:             Check if there is a new param and if it should be set with a value
./DeepSpeed/deepspeed/runtime/config_utils.py:                 Remove the deprecate field if there is a replacing field
./DeepSpeed/deepspeed/runtime/config_utils.py:                 Set new param value
./DeepSpeed/deepspeed/runtime/config_utils.py:                     If the new param exists in a subconfig, we need to get
./DeepSpeed/deepspeed/runtime/config_utils.py:                     the fields set for that subconfig
./DeepSpeed/deepspeed/runtime/config_utils.py:                 A custom function for converting the old param value to new param value can be provided
./DeepSpeed/deepspeed/runtime/config_utils.py: adapted from https://stackoverflow.com/a/50701137/9201239
./DeepSpeed/deepspeed/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/__init__.py: Export version information
./DeepSpeed/deepspeed/__init__.py: Set to torch's distributed package or deepspeed.comm based inside DeepSpeedEngine init
./DeepSpeed/deepspeed/__init__.py:     Disable zero.Init context if it's currently enabled
./DeepSpeed/deepspeed/__init__.py:     Set config using config_params for backwards compat
./DeepSpeed/deepspeed/__init__.py:     Check for deepscale_config for backwards compat
./DeepSpeed/deepspeed/__init__.py:     Check that we have only one config passed
./DeepSpeed/deepspeed/__init__.py:     Restore zero.Init context if necessary
./DeepSpeed/deepspeed/__init__.py:     Case 1: user provides no config and no kwargs. Default config will be used.
./DeepSpeed/deepspeed/__init__.py:     Case 2: user provides a config and no kwargs. User supplied config will be used.
./DeepSpeed/deepspeed/__init__.py:     Case 3: user provides no config and uses keyword arguments (kwargs) only.
./DeepSpeed/deepspeed/__init__.py:     Case 4: user provides config and keyword arguments (kwargs). Both config and kwargs are merged and kwargs take precedence.
./DeepSpeed/deepspeed/__init__.py:     Load config_dict from config first
./DeepSpeed/deepspeed/__init__.py:     Update with values from kwargs, ensuring no conflicting overlap between config and kwargs
./DeepSpeed/deepspeed/__init__.py:     If there is overlap, error out if values are different
./DeepSpeed/deepspeed/comm/backend.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/comm/backend.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/comm/backend.py: DeepSpeed Team
./DeepSpeed/deepspeed/comm/backend.py: Custom DS Backends -- Direct C/Ops
./DeepSpeed/deepspeed/comm/backend.py: DS backend wrapper for torch.distributed [DEFAULT]
./DeepSpeed/deepspeed/comm/backend.py: Backend is the base class
./DeepSpeed/deepspeed/comm/backend.py:         The world size and rank of the world process group
./DeepSpeed/deepspeed/comm/backend.py:         Single process group (pg) implementation for now but keep a list for future
./DeepSpeed/deepspeed/comm/backend.py:         create a new pg and add it to pg list
./DeepSpeed/deepspeed/comm/backend.py:         subclasses will initialize them fully
./DeepSpeed/deepspeed/comm/backend.py:         - initialize a default world process group and add it to pg list
./DeepSpeed/deepspeed/comm/config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/comm/config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/comm/config.py: DeepSpeed Team
./DeepSpeed/deepspeed/comm/comm.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/comm/comm.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/comm/comm.py: DeepSpeed Team
./DeepSpeed/deepspeed/comm/comm.py: Current deepspeed.comm backend (cdb) global object for simple access by client code
./DeepSpeed/deepspeed/comm/comm.py: Create global timer for ops
./DeepSpeed/deepspeed/comm/comm.py: Maintain objects of all initialized ds backends and assign them using the API functions in this file
./DeepSpeed/deepspeed/comm/comm.py: This should be set here so all rank/size information from the launcher can be propagated
./DeepSpeed/deepspeed/comm/comm.py: Logging wrapper for timing ops
./DeepSpeed/deepspeed/comm/comm.py:         Add enabled flag so that overhead to each comm op is two if conditions at most
./DeepSpeed/deepspeed/comm/comm.py:                 Need func args for their defaults
./DeepSpeed/deepspeed/comm/comm.py:         Return the op, then stop the op's timer
./DeepSpeed/deepspeed/comm/comm.py:                 Need to make op blocking for accurate logging
./DeepSpeed/deepspeed/comm/comm.py:                 If we're using MPI, we can't simply sync the stream
./DeepSpeed/deepspeed/comm/comm.py:                     need temp var since 'elapsed' resets events
./DeepSpeed/deepspeed/comm/comm.py: For compatibility with torch distributed's init_process_group, we shall retain the signature from PyTorch code.
./DeepSpeed/deepspeed/comm/comm.py: DeepSpeed NCCL/MPI backend may not need all these params as we will have our own implementation.
./DeepSpeed/deepspeed/comm/comm.py: Please read full torch.distributed API docs from https://pytorch.org/docs/stable/distributed.html
./DeepSpeed/deepspeed/comm/comm.py: UNUSED: Future helper function to initialize DS backends
./DeepSpeed/deepspeed/comm/comm.py:    assert cdb is not None, 'DeepSpeed backend not set, please initialize it using init_process_group()'
./DeepSpeed/deepspeed/comm/comm.py:     Returns ``True`` if the deepspeed comm package is available.
./DeepSpeed/deepspeed/comm/comm.py:     TODO: load other ops. Clients including deepspeed itself should use deepspeed.comm to import
./DeepSpeed/deepspeed/comm/comm.py:     any communication related primitives from this package.
./DeepSpeed/deepspeed/comm/comm.py:     use hasattr(deepspeed.csrc.ops, "_comm") or something
./DeepSpeed/deepspeed/comm/comm.py:    if profile_comm:
./DeepSpeed/deepspeed/comm/comm.py:     context of the timers?
./DeepSpeed/deepspeed/comm/comm.py:     timers.start()
./DeepSpeed/deepspeed/comm/comm.py:     TensorBoard logging for comm calls.?
./DeepSpeed/deepspeed/comm/comm.py:    print(f'op = {op}, cdb= {cdb.name}')
./DeepSpeed/deepspeed/comm/comm.py: Main DeepSpeed Comms. public API.
./DeepSpeed/deepspeed/comm/comm.py:         The user initialized torch.dist themselves, create cdb and short-circuit
./DeepSpeed/deepspeed/comm/comm.py:         Initialize torch distributed if needed
./DeepSpeed/deepspeed/comm/comm.py:             Create a torch backend object, initialize torch distributed, and assign to cdb
./DeepSpeed/deepspeed/comm/comm.py:     Determine local rank by assuming hostnames are unique
./DeepSpeed/deepspeed/comm/comm.py:     Are we running inside an Azure Machine Learning (AML) environment?
./DeepSpeed/deepspeed/comm/comm.py:     Are we running inside an AWS SageMaker environment?
./DeepSpeed/deepspeed/comm/comm.py:     Are we running on a DLTS cluster?
./DeepSpeed/deepspeed/comm/comm.py:         Do not overwrite master port with that defined in AZ_BATCH_MASTER_NODE
./DeepSpeed/deepspeed/comm/reduce_op.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/comm/reduce_op.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/comm/reduce_op.py: DeepSpeed Team
./DeepSpeed/deepspeed/comm/constants.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/comm/constants.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/comm/constants.py: DeepSpeed Team
./DeepSpeed/deepspeed/comm/constants.py:########################################
./DeepSpeed/deepspeed/comm/constants.py: Comms Logger
./DeepSpeed/deepspeed/comm/constants.py:########################################
./DeepSpeed/deepspeed/comm/constants.py: Comms Logger. By default, this feature is not enabled.
./DeepSpeed/deepspeed/comm/constants.py: Users can configure in ds_config.json as below example:
./DeepSpeed/deepspeed/comm/constants.py: Comms logger enable signal
./DeepSpeed/deepspeed/comm/constants.py: Comms logger verbose signal
./DeepSpeed/deepspeed/comm/constants.py: comms logger profile all ops signal
./DeepSpeed/deepspeed/comm/constants.py: comms logger show all ops signal
./DeepSpeed/deepspeed/comm/constants.py: comms logger profile specific ops in list
./DeepSpeed/deepspeed/comm/ccl.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/comm/ccl.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/comm/ccl.py: DeepSpeed Team
./DeepSpeed/deepspeed/comm/ccl.py:         backend covered it
./DeepSpeed/deepspeed/comm/ccl.py:             set CCLBackend to uninitialized state if CCLCommBuilder cannot be loaded
./DeepSpeed/deepspeed/comm/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/comm/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/comm/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/comm/utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/comm/utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/comm/utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/comm/utils.py:     DeepSpeed launcher will set it so get from there
./DeepSpeed/deepspeed/comm/utils.py:     Make it a single process job and set rank to 0
./DeepSpeed/deepspeed/comm/utils.py:     DeepSpeed launcher will set it so get from there
./DeepSpeed/deepspeed/comm/utils.py:     Make it a single process job and set rank to 0
./DeepSpeed/deepspeed/comm/utils.py:     DeepSpeed launcher will set it so get from there
./DeepSpeed/deepspeed/comm/utils.py:     Make it a single process job and set size to 1
./DeepSpeed/deepspeed/comm/utils.py: We need this hacky function since torch doesn't consistently name or place the input tensor args
./DeepSpeed/deepspeed/comm/utils.py:     most colls
./DeepSpeed/deepspeed/comm/utils.py:     all_reduce_coalesced coll
./DeepSpeed/deepspeed/comm/utils.py:     reduce scatter coll
./DeepSpeed/deepspeed/comm/utils.py:     all_to_all and torch multiGPU colls
./DeepSpeed/deepspeed/comm/utils.py:     3 cases:
./DeepSpeed/deepspeed/comm/utils.py:       - tensor arg is in args
./DeepSpeed/deepspeed/comm/utils.py:       - tensor arg is in kwargs
./DeepSpeed/deepspeed/comm/utils.py:       - tensor arg is not present (e.g. barrier)
./DeepSpeed/deepspeed/comm/utils.py:     check if tensor arg is in args
./DeepSpeed/deepspeed/comm/utils.py:     check if tensor arg is in kwargs
./DeepSpeed/deepspeed/comm/utils.py:     if tensor arg is not present, no data is being transmitted
./DeepSpeed/deepspeed/comm/utils.py:         Sum of tensor sizes for list colls such as torch's all_to_all
./DeepSpeed/deepspeed/comm/utils.py:         NOTE: msg_size for list colls will not be the actual size transmitted by a given MPI/NCCL call within the coll op. Instead, it's the total amount of data transmitted.
./DeepSpeed/deepspeed/comm/torch.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/comm/torch.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/comm/torch.py: DeepSpeed Team
./DeepSpeed/deepspeed/comm/torch.py:#Utilities to turn comm off
./DeepSpeed/deepspeed/comm/torch.py:#TODO: move to base comm (wrapper)
./DeepSpeed/deepspeed/comm/torch.py:assumption: all_gather and reduce scatter
./DeepSpeed/deepspeed/comm/torch.py:# are what we care about
./DeepSpeed/deepspeed/comm/torch.py:         Future functionality to support ds.initialize() on a single GPU
./DeepSpeed/deepspeed/comm/torch.py:         The idea is to fake that dist backend is initialized even when
./DeepSpeed/deepspeed/comm/torch.py:         it is not so we can run on a single GPU without doing any init_process_group
./DeepSpeed/deepspeed/comm/torch.py:             customized PyTorch
./DeepSpeed/deepspeed/comm/torch.py: This will become a light-weight wrapper around torch.distributed functions
./DeepSpeed/deepspeed/comm/torch.py: TODO: create some example to show how this wrapper can help profile communication
./DeepSpeed/deepspeed/comm/torch.py: TODO: make sure there is no performance regression with this approach
./DeepSpeed/deepspeed/comm/torch.py: TODO: explore monkey-patching if this does not work
./DeepSpeed/deepspeed/utils/comms_logging.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/utils/comms_logging.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/utils/comms_logging.py: DeepSpeed Team
./DeepSpeed/deepspeed/utils/comms_logging.py: Helper function to pretty-print message sizes
./DeepSpeed/deepspeed/utils/comms_logging.py: Helper function to calculate algbw and busbw.
./DeepSpeed/deepspeed/utils/comms_logging.py: See https://gist.github.com/jeffra/b5e80466b4c86be00ea3b6f130fb7a36 and https://github.com/NVIDIA/nccl-tests/blob/master/doc/PERFORMANCE.md
./DeepSpeed/deepspeed/utils/comms_logging.py:     convert to Gbps
./DeepSpeed/deepspeed/utils/comms_logging.py:     There are three settings for the op profiler:
./DeepSpeed/deepspeed/utils/comms_logging.py:     - Global profiling (profile all comms)
./DeepSpeed/deepspeed/utils/comms_logging.py:     - Op-type profiling (e.g. profile all all_reduce comms)
./DeepSpeed/deepspeed/utils/comms_logging.py:     - Op profiling (e.g. profile a specific all_reduce op)
./DeepSpeed/deepspeed/utils/comms_logging.py:     E.g. start_profiling_op('all_reduce')
./DeepSpeed/deepspeed/utils/comms_logging.py:     Add log entry
./DeepSpeed/deepspeed/utils/comms_logging.py:             If this comm_op has already been logged with this message size, just add to existing record
./DeepSpeed/deepspeed/utils/comms_logging.py:             If this is a new message size for this comm_op, add new record under existing comm_op
./DeepSpeed/deepspeed/utils/comms_logging.py:             Create entirely new record
./DeepSpeed/deepspeed/utils/comms_logging.py:         If verbose, print every comm op
./DeepSpeed/deepspeed/utils/comms_logging.py:         TODO: Add to tensorboard
./DeepSpeed/deepspeed/utils/comms_logging.py:     Print summary at end of iteration, epoch, or training
./DeepSpeed/deepspeed/utils/comms_logging.py:                 vals[0] is the count for each msg size
./DeepSpeed/deepspeed/utils/comms_logging.py:                 vals[1] is a list of latency records for each msg size
./DeepSpeed/deepspeed/utils/comms_logging.py:                 vals[2] and vals[3] are the lists of algbw and busbw, respectively
./DeepSpeed/deepspeed/utils/comms_logging.py:                 Get rid of outliers when we print
./DeepSpeed/deepspeed/utils/comms_logging.py:                     vals[0] is the count for each msg size
./DeepSpeed/deepspeed/utils/comms_logging.py:                     vals[1] is a list of latency records for each msg size
./DeepSpeed/deepspeed/utils/logging.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/utils/logging.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/utils/logging.py: DeepSpeed Team
./DeepSpeed/deepspeed/utils/init_on_device.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/utils/init_on_device.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/utils/init_on_device.py: DeepSpeed Team
./DeepSpeed/deepspeed/utils/timer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/utils/timer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/utils/timer.py: DeepSpeed Team
./DeepSpeed/deepspeed/utils/timer.py:             If the timing in progress, end it first.
./DeepSpeed/deepspeed/utils/timer.py:             Get the elapsed time.
./DeepSpeed/deepspeed/utils/timer.py:             Reset the elapsed time
./DeepSpeed/deepspeed/utils/timer.py:             If timing was in progress, set it back.
./DeepSpeed/deepspeed/utils/timer.py:             training samples per second
./DeepSpeed/deepspeed/utils/timer.py:     Account for edge case of empty list
./DeepSpeed/deepspeed/utils/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/utils/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/utils/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/utils/__init__.py:from .distributed import init_distributed
./DeepSpeed/deepspeed/utils/__init__.py: TODO: Move tensor fragment and mixed precision to zero utils
./DeepSpeed/deepspeed/utils/types.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/utils/types.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/utils/types.py: DeepSpeed Team
./DeepSpeed/deepspeed/utils/groups.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/utils/groups.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/utils/groups.py: DeepSpeed Team
./DeepSpeed/deepspeed/utils/groups.py: The file has been adapted from https://github.com/NVIDIA/Megatron-LM and retains the following license from the original file
./DeepSpeed/deepspeed/utils/groups.py: Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.
./DeepSpeed/deepspeed/utils/groups.py:
./DeepSpeed/deepspeed/utils/groups.py: Licensed under the Apache License, Version 2.0 (the "License");
./DeepSpeed/deepspeed/utils/groups.py: you may not use this file except in compliance with the License.
./DeepSpeed/deepspeed/utils/groups.py: You may obtain a copy of the License at
./DeepSpeed/deepspeed/utils/groups.py:
./DeepSpeed/deepspeed/utils/groups.py:     http://www.apache.org/licenses/LICENSE-2.0
./DeepSpeed/deepspeed/utils/groups.py:
./DeepSpeed/deepspeed/utils/groups.py: Unless required by applicable law or agreed to in writing, software
./DeepSpeed/deepspeed/utils/groups.py: distributed under the License is distributed on an "AS IS" BASIS,
./DeepSpeed/deepspeed/utils/groups.py: WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
./DeepSpeed/deepspeed/utils/groups.py: See the License for the specific language governing permissions and
./DeepSpeed/deepspeed/utils/groups.py: limitations under the License.
./DeepSpeed/deepspeed/utils/groups.py: Expert parallel group that the current rank belongs to.
./DeepSpeed/deepspeed/utils/groups.py: Expert data parallel group that the current rank belongs to.
./DeepSpeed/deepspeed/utils/groups.py: dist world group needs to be cloned for some cases
./DeepSpeed/deepspeed/utils/groups.py: ZeRO parameter  partitioning group that the current rank belongs to.
./DeepSpeed/deepspeed/utils/groups.py: global object to maintain mpu object if passed by a Megatron client
./DeepSpeed/deepspeed/utils/groups.py: global object that stores tensor parallel world size for experts
./DeepSpeed/deepspeed/utils/groups.py: All to All quantized graident communication groups
./DeepSpeed/deepspeed/utils/groups.py: Deprecated groups initialize function.
./DeepSpeed/deepspeed/utils/groups.py: Not currently used. Helper function to create a model (tensor) parallel group.
./DeepSpeed/deepspeed/utils/groups.py:     Get world size and rank. Ensure some consistencies.
./DeepSpeed/deepspeed/utils/groups.py:     Build the data parallel groups.
./DeepSpeed/deepspeed/utils/groups.py:     Build the model parallel groups.
./DeepSpeed/deepspeed/utils/groups.py:     Build the expert data parallel groups.
./DeepSpeed/deepspeed/utils/groups.py:     Only create group if it does not already exist
./DeepSpeed/deepspeed/utils/groups.py:     Build the expert parallel groups.
./DeepSpeed/deepspeed/utils/groups.py:     Only create group if it does not already exist
./DeepSpeed/deepspeed/utils/groups.py:     Generate data parallel groups
./DeepSpeed/deepspeed/utils/groups.py:                 [0, 4, 8, 12, 16, 20, 24, 28, 2, 6, 10, 14, 18, 22, 26, 30]
./DeepSpeed/deepspeed/utils/groups.py:                 [1, 5, 9, 13, 17, 21, 25, 29, 3, 7, 11, 15, 19, 23, 27, 31]
./DeepSpeed/deepspeed/utils/groups.py:         partition of expert parallel groups, e.g. [0,2,4,6], [8,10,12,14]
./DeepSpeed/deepspeed/utils/groups.py:         zip part_ep_groups get expert data parallel ranks, e.g [0,8],[2,10],[4,12],[6,14]
./DeepSpeed/deepspeed/utils/groups.py:     Get world size and rank. Ensure some consistencies.
./DeepSpeed/deepspeed/utils/groups.py:     Only create groups if they don't already exist
./DeepSpeed/deepspeed/utils/groups.py:     Need to check conditions outside the group creation loop because of the way torch.dist group creation works
./DeepSpeed/deepspeed/utils/groups.py:         index 2 is ep_size in the group name: ep_size_<ep_size>
./DeepSpeed/deepspeed/utils/groups.py:         If not cloned already, clone the world group
./DeepSpeed/deepspeed/utils/groups.py:     Return the clone of dist world group
./DeepSpeed/deepspeed/utils/groups.py:     When sequence parallelism is enabled, the process group for zero sharding and
./DeepSpeed/deepspeed/utils/groups.py:     gradient allreduce must be across both dimensions of data and sequence parallelism.
./DeepSpeed/deepspeed/utils/groups.py:     Only create group if it does not already exist
./DeepSpeed/deepspeed/utils/groups.py:     Build the ZeRO param intra parallel groups.
./DeepSpeed/deepspeed/utils/groups.py:    assert _ZERO_PARAM_INTRA_PARALLEL_GROUP is not None, \
./DeepSpeed/deepspeed/utils/groups.py:        'ZeRO parameter partitioning group is not initialized'
./DeepSpeed/deepspeed/utils/groups.py:    TODO: Add warning
./DeepSpeed/deepspeed/utils/groups.py:    ##TODO: assert that MPU is not set
./DeepSpeed/deepspeed/utils/debug.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/utils/debug.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/utils/debug.py: DeepSpeed Team
./DeepSpeed/deepspeed/utils/debug.py: For lazy import with printflock()
./DeepSpeed/deepspeed/utils/debug.py: for debug purposes map module and param objects to their fully qualified names
./DeepSpeed/deepspeed/utils/debug.py:     extract the fully qualified names as soon as the model is acquired
./DeepSpeed/deepspeed/utils/debug.py:     XXX: can probably make a map of param2module and vice-versa
./DeepSpeed/deepspeed/utils/exceptions.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/utils/exceptions.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/utils/exceptions.py: DeepSpeed Team
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:!/usr/bin/env python
./DeepSpeed/deepspeed/utils/zero_to_fp32.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/utils/zero_to_fp32.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/utils/zero_to_fp32.py: DeepSpeed Team
./DeepSpeed/deepspeed/utils/zero_to_fp32.py: This script extracts fp32 consolidated weights from a zero 1, 2 and 3 DeepSpeed checkpoints. It gets
./DeepSpeed/deepspeed/utils/zero_to_fp32.py: copied into the top level checkpoint dir, so the user can easily do the conversion at any point in
./DeepSpeed/deepspeed/utils/zero_to_fp32.py: the future. Once extracted, the weights don't require DeepSpeed and can be used in any
./DeepSpeed/deepspeed/utils/zero_to_fp32.py: application.
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:
./DeepSpeed/deepspeed/utils/zero_to_fp32.py: example: python zero_to_fp32.py . pytorch_model.bin
./DeepSpeed/deepspeed/utils/zero_to_fp32.py: while this script doesn't use deepspeed to recover data, since the checkpoints are pickled with
./DeepSpeed/deepspeed/utils/zero_to_fp32.py: DeepSpeed data structures it has to be available in the current python environment.
./DeepSpeed/deepspeed/utils/zero_to_fp32.py: load to cpu
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     there should be only one file
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     XXX: need to test that this simple glob rule works for multi-node setup too
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         recover just the buffers while restoring them to fp32 if they were saved in fp16
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         collect parameters that are included in param_shapes
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         update with frozen parameters
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         handle shared params
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         immediately discard the potentially huge 2 optimizer states as we only care for fp32 master weights
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         and also handle the case where it was already removed by another helper script
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     For ZeRO-2 each param group can have different partition_count as data parallelism for expert
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     parameters can be different from data parallelism for non-expert parameters. So we can just
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     use the max of the partition_count to get the dp world_size.
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     the groups are named differently in each stage
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         if there is more than one param group, there will be multiple flattened tensors - one
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         flattened tensor per group - for simplicity merge them into a single tensor
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:        
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         XXX: could make the script more memory efficient for when there are multiple groups - it
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         will require matching the sub-lists of param_shapes for each param group flattened tensor
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     Reconstruction protocol:
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:    
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     XXX: document this
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     XXX: memory usage doubles here (zero2)
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         not asserting if there is a mismatch due to possible padding
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     params
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     XXX: for huge models that can't fit into the host's RAM we will have to recode this to support
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     out-of-core computing solution
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         Z2 started to align to 2*world_size to improve nccl performance. Therefore both offset and
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         avail_numel can differ by anywhere between 0..2*world_size. Due to two unrelated complex
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         paddings performed in the code it's almost impossible to predict the exact numbers w/o the
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         live optimizer object, so we are checking that the numbers are within the right range
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         Sanity check
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     buffers
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     recover shared parameters
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     Reconstruction protocol: For zero3 we need to zip the partitions together at boundary of each
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     param, re-consolidating each param, while dealing with padding if any
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     merge list of dicts, preserving order
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         not asserting if there is a mismatch due to possible padding
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     params
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     XXX: for huge models that can't fit into the host's RAM we will have to recode this to support
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     out-of-core computing solution
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         XXX: memory usage doubles here
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     Sanity check
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     buffers
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:     recover shared parameters
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         do the training and checkpoint saving
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         submit to model hub or save the model to share with others
./DeepSpeed/deepspeed/utils/zero_to_fp32.py:         submit to model hub or save the model to share with others
./DeepSpeed/deepspeed/utils/tensor_fragment.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/utils/tensor_fragment.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/utils/tensor_fragment.py: DeepSpeed Team
./DeepSpeed/deepspeed/utils/tensor_fragment.py:     ZeRO stage 3 param
./DeepSpeed/deepspeed/utils/tensor_fragment.py:     ZeRO stage 1, 2, and bf16_optimizer params
./DeepSpeed/deepspeed/utils/tensor_fragment.py:     ZeRO stage 3 param
./DeepSpeed/deepspeed/utils/tensor_fragment.py:     ZeRO stage 1, 2, and bf16_optimizer params
./DeepSpeed/deepspeed/utils/tensor_fragment.py:     ZeRO stage 3 param
./DeepSpeed/deepspeed/utils/tensor_fragment.py:     ZeRO stage 1, 2, and bf16_optimizer params
./DeepSpeed/deepspeed/utils/tensor_fragment.py:     ZeRO stage 3 param
./DeepSpeed/deepspeed/utils/tensor_fragment.py:     ZeRO stage 1, 2, and bf16_optimizer params
./DeepSpeed/deepspeed/utils/tensor_fragment.py: TODO: Figure out the correct return dtype
./DeepSpeed/deepspeed/utils/tensor_fragment.py:     ZeRO stage 3 param
./DeepSpeed/deepspeed/utils/tensor_fragment.py:     ZeRO stage 1, 2, and bf16_optimizer params
./DeepSpeed/deepspeed/utils/tensor_fragment.py:## Local API  START ###
./DeepSpeed/deepspeed/utils/tensor_fragment.py:     ZeRO stage 3 param
./DeepSpeed/deepspeed/utils/tensor_fragment.py:     ZeRO stage 3 param
./DeepSpeed/deepspeed/utils/tensor_fragment.py:     ZeRO stage 3 param
./DeepSpeed/deepspeed/utils/tensor_fragment.py:     ZeRO stage 3 param
./DeepSpeed/deepspeed/utils/tensor_fragment.py:     ZeRO stage 3 param
./DeepSpeed/deepspeed/utils/tensor_fragment.py:## Local API  END ###
./DeepSpeed/deepspeed/utils/tensor_fragment.py: TODO: Implement API for setting ZeRO partitioned gradients
./DeepSpeed/deepspeed/utils/nvtx.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/utils/nvtx.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/utils/nvtx.py: DeepSpeed Team
./DeepSpeed/deepspeed/utils/mixed_precision_linkage.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/utils/mixed_precision_linkage.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/utils/mixed_precision_linkage.py: DeepSpeed Team
./DeepSpeed/deepspeed/utils/mixed_precision_linkage.py:         lp_param overlaps with partition if both are true
./DeepSpeed/deepspeed/utils/mixed_precision_linkage.py:         1) current_offset < partition_end,
./DeepSpeed/deepspeed/utils/mixed_precision_linkage.py:         2) current_offset + lp_param.numel() >= partition_start
./DeepSpeed/deepspeed/utils/mixed_precision_linkage.py:             Indices for params in this partition/GPU
./DeepSpeed/deepspeed/utils/numa.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/utils/numa.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/utils/numa.py: DeepSpeed Team
./DeepSpeed/deepspeed/utils/numa.py: return a list of list for cores to numa mapping
./DeepSpeed/deepspeed/utils/numa.py: [
./DeepSpeed/deepspeed/utils/numa.py:     [ cores for numa 0 ]
./DeepSpeed/deepspeed/utils/numa.py:     [ cores belong to numa 1 ]
./DeepSpeed/deepspeed/utils/numa.py:     ...
./DeepSpeed/deepspeed/utils/numa.py: ]
./DeepSpeed/deepspeed/utils/numa.py: return a list of list for cores to numa mapping
./DeepSpeed/deepspeed/utils/numa.py: [
./DeepSpeed/deepspeed/utils/numa.py:     [ cores for numa 0 ]
./DeepSpeed/deepspeed/utils/numa.py:     [ cores belong to numa 1 ]
./DeepSpeed/deepspeed/utils/numa.py:     ...
./DeepSpeed/deepspeed/utils/numa.py: ]
./DeepSpeed/deepspeed/utils/numa.py:         value is not a single number
./DeepSpeed/deepspeed/utils/numa.py: parse comma and dash separated range list into list
./DeepSpeed/deepspeed/utils/numa.py: i.e. "0,2-4,6" --> [0, 2, 3, 4, 6]
./DeepSpeed/deepspeed/utils/numa.py: rules:
./DeepSpeed/deepspeed/utils/numa.py: 1. Range list number be comma separated, each item are either a single number,
./DeepSpeed/deepspeed/utils/numa.py:    or a range marked by two numbers (both number are included in the range)
./DeepSpeed/deepspeed/utils/numa.py: 2. Sub ranges must be in ascend order and not overlap with each other
./DeepSpeed/deepspeed/utils/numa.py: 3. No space in the range expression
./DeepSpeed/deepspeed/utils/numa.py:     check if all cores belong to same numa, if true, bind process to that numa domain with -m parameter
./DeepSpeed/deepspeed/utils/numa.py:         look for empty numa which is HBM numa
./DeepSpeed/deepspeed/utils/numa.py:             check for fakenuma
./DeepSpeed/deepspeed/utils/numa.py:                    first duplication, add previous node into list
./DeepSpeed/deepspeed/utils/numa.py:                 the following construct break the outer loop if inner loop breaks
./DeepSpeed/deepspeed/env_report.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/env_report.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/env_report.py: DeepSpeed Team
./DeepSpeed/deepspeed/profiling/config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/profiling/config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/profiling/config.py: DeepSpeed Team
./DeepSpeed/deepspeed/profiling/constants.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/profiling/constants.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/profiling/constants.py: DeepSpeed Team
./DeepSpeed/deepspeed/profiling/constants.py:########################################
./DeepSpeed/deepspeed/profiling/constants.py: flops profiler
./DeepSpeed/deepspeed/profiling/constants.py:########################################
./DeepSpeed/deepspeed/profiling/constants.py: Flops profiler. By default, this feature is not enabled.
./DeepSpeed/deepspeed/profiling/constants.py: Users can configure in ds_config.json as below example:
./DeepSpeed/deepspeed/profiling/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/profiling/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/profiling/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/profiling/flops_profiler/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/profiling/flops_profiler/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/profiling/flops_profiler/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py: DeepSpeed Team
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:             if computing the flops of a module directly
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:             if computing the flops of the functionals in a module
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:                 number of expert parameters taking into account other expert parallel groups
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:         estimation
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     estimation
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     estimation
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     estimation
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     Re-map equation so that same equation with different alphabet
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     representations will look the same.
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     FC
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     convolutions
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     conv transposed
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     activations
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     Normalizations
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     poolings
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     upsample
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     softmax
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     embedding
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     torch.nn.functional does not support importlib.reload()
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     matrix matrix mult ih state and internal state
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     matrix matrix mult hh state and internal state
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:         add both operations
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:         hadamard of r
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:         adding operations from both states
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:         last two hadamard _product and add
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:         adding operations from both states
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:         two hadamard _product and add for C state
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:         final hadamard
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     input is a tuple containing a sequence to process and (optionally) hidden state
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     RNN
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     can not iterate over all submodules using self.model.modules()
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     since modules() returns duplicate modules only once
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     iterate over immediate children modules
./DeepSpeed/deepspeed/profiling/flops_profiler/profiler.py:     iterate over immediate children modules
./DeepSpeed/deepspeed/compression/config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/compression/config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/compression/config.py: DeepSpeed Team
./DeepSpeed/deepspeed/compression/config.py:    
./DeepSpeed/deepspeed/compression/config.py:     shared parameters
./DeepSpeed/deepspeed/compression/config.py:     each sub-groups
./DeepSpeed/deepspeed/compression/config.py:     shared parameters
./DeepSpeed/deepspeed/compression/config.py:     each sub-groups
./DeepSpeed/deepspeed/compression/config.py:     shared parameters
./DeepSpeed/deepspeed/compression/config.py:     each sub-groups
./DeepSpeed/deepspeed/compression/config.py:     shared parameters
./DeepSpeed/deepspeed/compression/config.py:     each sub-groups
./DeepSpeed/deepspeed/compression/config.py:     shared parameters
./DeepSpeed/deepspeed/compression/config.py:     each sub-groups
./DeepSpeed/deepspeed/compression/config.py:     shared parameters
./DeepSpeed/deepspeed/compression/config.py:     each sub-groups
./DeepSpeed/deepspeed/compression/basic_layer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/compression/basic_layer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/compression/basic_layer.py: DeepSpeed Team
./DeepSpeed/deepspeed/compression/basic_layer.py:             Initialization
./DeepSpeed/deepspeed/compression/basic_layer.py:             if do not need momentum, please set self.act_range_momentum = 0
./DeepSpeed/deepspeed/compression/basic_layer.py:             for embedding, we always use token-wise quantization
./DeepSpeed/deepspeed/compression/basic_layer.py:         Here, we support two cases: L1 norm based pruning and topk based pruning
./DeepSpeed/deepspeed/compression/basic_layer.py:         Here, we support two cases: L1 norm based pruning and topk based pruning
./DeepSpeed/deepspeed/compression/basic_layer.py:             compute the l1 norm of each column
./DeepSpeed/deepspeed/compression/basic_layer.py:         Here, we support only topk based pruning
./DeepSpeed/deepspeed/compression/basic_layer.py:         This function is used for row/col pruning
./DeepSpeed/deepspeed/compression/basic_layer.py:         particularly, if we have two back-to-back layers, F1 and F2; when
./DeepSpeed/deepspeed/compression/basic_layer.py:         we remove rows from F1, we also need to remove columns from F2
./DeepSpeed/deepspeed/compression/basic_layer.py:         However, if we only have one layer, F1, then we only need to mask pruned
./DeepSpeed/deepspeed/compression/basic_layer.py:         rows as 0 in F1
./DeepSpeed/deepspeed/compression/basic_layer.py:             this is generally for column pruning
./DeepSpeed/deepspeed/compression/basic_layer.py:         similar as row/col pruning, head pruning also needs to prune QKV which is associated with O matrix
./DeepSpeed/deepspeed/compression/basic_layer.py:             used for mpu linear layers
./DeepSpeed/deepspeed/compression/basic_layer.py:         Here, we support two cases: L1 norm based pruning and topk based pruning
./DeepSpeed/deepspeed/compression/basic_layer.py:             compute the l1 norm of each conv2d kernel (the last three dimension)
./DeepSpeed/deepspeed/compression/basic_layer.py:     Bypass the function if we are using only 1 GPU.
./DeepSpeed/deepspeed/compression/basic_layer.py:     All-reduce.
./DeepSpeed/deepspeed/compression/basic_layer.py:     Get the size and dimension.
./DeepSpeed/deepspeed/compression/basic_layer.py:     Split.
./DeepSpeed/deepspeed/compression/basic_layer.py:     Note: torch.split does not create contiguous tensors by default.
./DeepSpeed/deepspeed/compression/basic_layer.py:     Bypass the function if we are using only 1 GPU.
./DeepSpeed/deepspeed/compression/basic_layer.py:     Split along last dimension.
./DeepSpeed/deepspeed/compression/basic_layer.py:     Note: torch.split does not create contiguous tensors by default.
./DeepSpeed/deepspeed/compression/basic_layer.py:     Bypass the function if we are using only 1 GPU.
./DeepSpeed/deepspeed/compression/basic_layer.py:     Size and dimension.
./DeepSpeed/deepspeed/compression/basic_layer.py:     Note: torch.cat already creates a contiguous tensor.
./DeepSpeed/deepspeed/compression/basic_layer.py: -----------------
./DeepSpeed/deepspeed/compression/basic_layer.py: Helper functions.
./DeepSpeed/deepspeed/compression/basic_layer.py: -----------------
./DeepSpeed/deepspeed/compression/basic_layer.py:         Keep input parameters
./DeepSpeed/deepspeed/compression/basic_layer.py:         Divide the weight matrix along the last dimension.
./DeepSpeed/deepspeed/compression/basic_layer.py:         Set up backprop all-reduce.
./DeepSpeed/deepspeed/compression/basic_layer.py:         Matrix multiply.
./DeepSpeed/deepspeed/compression/basic_layer.py:             All-gather across the partitions.
./DeepSpeed/deepspeed/compression/basic_layer.py:         Keep input parameters
./DeepSpeed/deepspeed/compression/basic_layer.py:         Divide the weight matrix along the last dimension.
./DeepSpeed/deepspeed/compression/basic_layer.py:         Set up backprop all-reduce.
./DeepSpeed/deepspeed/compression/basic_layer.py:         Matrix multiply.
./DeepSpeed/deepspeed/compression/basic_layer.py:         All-reduce across all the partitions.
./DeepSpeed/deepspeed/compression/constants.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/compression/constants.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/compression/constants.py: DeepSpeed Team
./DeepSpeed/deepspeed/compression/constants.py:########################################
./DeepSpeed/deepspeed/compression/constants.py: Compression Methods
./DeepSpeed/deepspeed/compression/constants.py: It has several sub-components
./DeepSpeed/deepspeed/compression/constants.py: #########################################
./DeepSpeed/deepspeed/compression/constants.py: COMPRESSION_TRAINING_ENABLED = "enabled"
./DeepSpeed/deepspeed/compression/constants.py: COMPRESSION_TRAINING_ENABLED_DEFAULT = False
./DeepSpeed/deepspeed/compression/constants.py:###
./DeepSpeed/deepspeed/compression/constants.py: Layer Reduction
./DeepSpeed/deepspeed/compression/constants.py:###
./DeepSpeed/deepspeed/compression/constants.py:###
./DeepSpeed/deepspeed/compression/constants.py: Weight Quantization
./DeepSpeed/deepspeed/compression/constants.py:###
./DeepSpeed/deepspeed/compression/constants.py: maybe deleted for a cleaner version
./DeepSpeed/deepspeed/compression/constants.py:##
./DeepSpeed/deepspeed/compression/constants.py: Activation Quantization
./DeepSpeed/deepspeed/compression/constants.py:##
./DeepSpeed/deepspeed/compression/constants.py:##
./DeepSpeed/deepspeed/compression/constants.py: Sparse Pruning
./DeepSpeed/deepspeed/compression/constants.py:##
./DeepSpeed/deepspeed/compression/constants.py:##
./DeepSpeed/deepspeed/compression/constants.py: Row Pruning
./DeepSpeed/deepspeed/compression/constants.py:##
./DeepSpeed/deepspeed/compression/constants.py:##
./DeepSpeed/deepspeed/compression/constants.py: Head Pruning
./DeepSpeed/deepspeed/compression/constants.py:##
./DeepSpeed/deepspeed/compression/constants.py:##
./DeepSpeed/deepspeed/compression/constants.py: Channel Pruning
./DeepSpeed/deepspeed/compression/constants.py:##
./DeepSpeed/deepspeed/compression/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/compression/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/compression/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/compression/utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/compression/utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/compression/utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/compression/utils.py:         Get the subnetwork by sorting the inputs and using the top threshold
./DeepSpeed/deepspeed/compression/utils.py:         flat_out and mask access the same memory.
./DeepSpeed/deepspeed/compression/compress.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/compression/compress.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/compression/compress.py: DeepSpeed Team
./DeepSpeed/deepspeed/compression/compress.py:                 logger.warning
./DeepSpeed/deepspeed/compression/compress.py:     extract the compression module for each method in compress_methods
./DeepSpeed/deepspeed/compression/compress.py:         for loop different methods, i.e., weight quantization, activation quantization etc
./DeepSpeed/deepspeed/compression/compress.py:             for loop different groups, i.e., weight quantization group 1, weight quantization group 2 etc
./DeepSpeed/deepspeed/compression/compress.py:                 this is used for head/row/channel pruning, if users provide the related module scope, we can shrink the layer dim for them
./DeepSpeed/deepspeed/compression/compress.py:                 otherwise we just mask those as zeros
./DeepSpeed/deepspeed/compression/compress.py:                         related key word can be a list, for instance the QKV for O matrix in Attention
./DeepSpeed/deepspeed/compression/compress.py:                 combine shared parameters with each group
./DeepSpeed/deepspeed/compression/compress.py:     For layer reduction
./DeepSpeed/deepspeed/compression/compress.py:     For sparse pruning snip_momentum method
./DeepSpeed/deepspeed/compression/compress.py:     sort methods
./DeepSpeed/deepspeed/compression/helper.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/compression/helper.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/compression/helper.py: DeepSpeed Team
./DeepSpeed/deepspeed/compression/helper.py:     Get the old module
./DeepSpeed/deepspeed/compression/helper.py:     Initialize the new module
./DeepSpeed/deepspeed/compression/helper.py:     Replace the old module with the new one
./DeepSpeed/deepspeed/compression/helper.py:     Here we first replace all module with our linear wrapper
./DeepSpeed/deepspeed/compression/helper.py:     Here we can make things much simpler by just replacing the module
./DeepSpeed/deepspeed/compression/scheduler.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/compression/scheduler.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/compression/scheduler.py: DeepSpeed Team
./DeepSpeed/deepspeed/compression/scheduler.py:         check weight quantization
./DeepSpeed/deepspeed/compression/scheduler.py:         check activation quantization
./DeepSpeed/deepspeed/compression/scheduler.py:         check sparse pruning
./DeepSpeed/deepspeed/compression/scheduler.py:         check head pruning
./DeepSpeed/deepspeed/compression/scheduler.py:         check row pruning
./DeepSpeed/deepspeed/compression/scheduler.py:         check channel pruning
./DeepSpeed/deepspeed/compression/scheduler.py:         check all different compression methods we have
./DeepSpeed/deepspeed/pydantic_v1.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/pydantic_v1.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/pydantic_v1.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/config.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/config.py:     The torch dtype must always be the first value (so we return torch.dtype)
./DeepSpeed/deepspeed/inference/config.py:     Copied from https://stackoverflow.com/a/43210118
./DeepSpeed/deepspeed/inference/config.py:     Allows us to use multiple values for each Enum index and returns first
./DeepSpeed/deepspeed/inference/config.py:     listed value when Enum is called
./DeepSpeed/deepspeed/inference/config.py:         first value is canonical value
./DeepSpeed/deepspeed/inference/config.py: todo: brainstorm on how to do ckpt loading for DS inference
./DeepSpeed/deepspeed/inference/config.py:    todo: refactor the following 3 into the new checkpoint_config
./DeepSpeed/deepspeed/inference/config.py:         Get the str representation of the datatype for serialization
./DeepSpeed/deepspeed/inference/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/quantization/quantization_context.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/quantization/quantization_context.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/quantization/quantization_context.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/quantization/quantization.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/quantization/quantization.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/quantization/quantization.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/quantization/quantization.py:     global quantized_weight_registry
./DeepSpeed/deepspeed/inference/quantization/quantization.py:     Return nvme swapper if exists, else return None.
./DeepSpeed/deepspeed/inference/quantization/quantization.py:     For nvme offloading we must use the same swapper here as model initialized.
./DeepSpeed/deepspeed/inference/quantization/quantization.py:         Quantize small weight first then large.
./DeepSpeed/deepspeed/inference/quantization/quantization.py:             Use popleft to timely release module's memory of replaced module after each loop iteration
./DeepSpeed/deepspeed/inference/quantization/quantization.py:             Timely recycle memory to prevent OOM on large models
./DeepSpeed/deepspeed/inference/quantization/quantization.py:     Clear registry after model construction.
./DeepSpeed/deepspeed/inference/quantization/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/quantization/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/quantization/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/quantization/utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/quantization/utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/quantization/utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/quantization/utils.py:         CPU does not support FP16 clamp
./DeepSpeed/deepspeed/inference/quantization/utils.py:         CPU does not support FP16 round
./DeepSpeed/deepspeed/inference/quantization/utils.py:         Use customized CUDA quantization kernel if possible.
./DeepSpeed/deepspeed/inference/quantization/utils.py:                 last_dimension_size = last_dimension_size // 2
./DeepSpeed/deepspeed/inference/quantization/utils.py:         We may have nested wrappers if we launch multiple initialization context.
./DeepSpeed/deepspeed/inference/quantization/utils.py:         Use state_dict_quantized flag to quantize state_dict only once
./DeepSpeed/deepspeed/inference/quantization/layers.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/quantization/layers.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/quantization/layers.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/quantization/layers.py: deal with weight sharing
./DeepSpeed/deepspeed/inference/quantization/layers.py:         A temporary hack as zero Zero3 assume all model weights has the same type. in all_gather_coalesced.get_only_unique_item
./DeepSpeed/deepspeed/inference/quantization/layers.py:         !!! Do not use torch.functional.linear(input, temp_dequantized_weight, self.bias) here as in zero3 torch.functional.linear is
./DeepSpeed/deepspeed/inference/quantization/layers.py:         replaced by LinearFunctionForZeroStage3. Which assume weight is non-temporary.
./DeepSpeed/deepspeed/inference/quantization/layers.py:         If weight is temp buffer there will be memory leak.
./DeepSpeed/deepspeed/inference/v2/kernels/ds_kernel.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ds_kernel.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ds_kernel.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/logits_gather/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/logits_gather/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/logits_gather/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/logits_gather/logits_gather.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/logits_gather/logits_gather.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/logits_gather/logits_gather.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/embed/embed.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/embed/embed.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/embed/embed.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/embed/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/embed/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/embed/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/blocked_flash/blocked_flash.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/blocked_flash/blocked_flash.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/blocked_flash/blocked_flash.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/blocked_flash/blocked_flash.py:         For testing, need to revert to 32
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/blocked_flash/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/blocked_flash/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/blocked_flash/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/top_1_gating/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/top_1_gating/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/top_1_gating/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/top_1_gating/top_1_gating.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/top_1_gating/top_1_gating.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/top_1_gating/top_1_gating.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/blocked_kv_rotary.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/blocked_kv_rotary.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/blocked_kv_rotary.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/linear_blocked_kv_copy.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/linear_blocked_kv_copy.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/linear_blocked_kv_copy.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/linear_blocked_kv_copy.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/linear_blocked_kv_copy.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/linear_blocked_kv_copy.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/blocked_trained_kv_rotary.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/blocked_trained_kv_rotary.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/blocked_trained_kv_rotary.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/blocked_trained_kv_rotary.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/blocked_trained_kv_rotary.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/blocked_trained_kv_rotary.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/moe_scatter/moe_scatter.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/moe_scatter/moe_scatter.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/moe_scatter/moe_scatter.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/moe_scatter/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/moe_scatter/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/moe_scatter/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/atom_builder/atom_builder.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/atom_builder/atom_builder.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/atom_builder/atom_builder.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/atom_builder/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/atom_builder/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/atom_builder/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/moe_gather/moe_gather.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/moe_gather/moe_gather.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/moe_gather/moe_gather.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/moe_gather/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/moe_gather/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/ragged_ops/moe_gather/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/cutlass_ops/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/cutlass_ops/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/cutlass_ops/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/cutlass_ops/moe_gemm/mixed_moe_gemm.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/cutlass_ops/moe_gemm/mixed_moe_gemm.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/cutlass_ops/moe_gemm/mixed_moe_gemm.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/cutlass_ops/moe_gemm/moe_gemm.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/cutlass_ops/moe_gemm/moe_gemm.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/cutlass_ops/moe_gemm/moe_gemm.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/cutlass_ops/moe_gemm/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/cutlass_ops/moe_gemm/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/cutlass_ops/moe_gemm/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/cutlass_ops/mixed_gemm/mixed_gemm.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/cutlass_ops/mixed_gemm/mixed_gemm.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/cutlass_ops/mixed_gemm/mixed_gemm.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/cutlass_ops/mixed_gemm/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/cutlass_ops/mixed_gemm/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/cutlass_ops/mixed_gemm/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_rms_norm/rms_norm_base.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_rms_norm/rms_norm_base.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_rms_norm/rms_norm_base.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_rms_norm/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_rms_norm/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_rms_norm/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_rms_norm/rms_pre_norm.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_rms_norm/rms_pre_norm.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_rms_norm/rms_pre_norm.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_rms_norm/rms_norm.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_rms_norm/rms_norm.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_rms_norm/rms_norm.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/gated_activations/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/gated_activations/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/gated_activations/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/gated_activations/gated_activation.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/gated_activations/gated_activation.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/gated_activations/gated_activation.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_layer_norm/cuda_pre_ln.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_layer_norm/cuda_pre_ln.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_layer_norm/cuda_pre_ln.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_layer_norm/cuda_ln.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_layer_norm/cuda_ln.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_layer_norm/cuda_ln.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_layer_norm/cuda_fp_ln_base.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_layer_norm/cuda_fp_ln_base.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_layer_norm/cuda_fp_ln_base.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_layer_norm/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_layer_norm/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_layer_norm/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_layer_norm/cuda_post_ln.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_layer_norm/cuda_post_ln.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/cuda_layer_norm/cuda_post_ln.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/blas_kernels/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/blas_kernels/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/blas_kernels/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/blas_kernels/blas_linear.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/blas_kernels/blas_linear.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/blas_kernels/blas_linear.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/bias_activations/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/bias_activations/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/bias_activations/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/bias_activations/bias_activation.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/bias_activations/bias_activation.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/kernels/core_ops/bias_activations/bias_activation.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/logging.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/logging.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/logging.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/flat_model_helpers.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/flat_model_helpers.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/flat_model_helpers.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_policy_base.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_policy_base.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_policy_base.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_policy_base.py:             Catch the ValueError here from the non_transformer_params because we are knowingly
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_policy_base.py:             calling it with something that may not match. This should allow us to raise a slightly more
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_policy_base.py:             informative error message.
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_transformer_base.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_transformer_base.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_transformer_base.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_transformer_base.py:    ######## Embedding #########
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_transformer_base.py:         Until we can do non-contiguous all-gather, we won't shard the embedding parameters.
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_transformer_base.py:    ######## Unembedding #########
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_transformer_base.py:    ######## QKV #########
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_transformer_base.py:    ######## Attention #########
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_transformer_base.py:    ######## Attention output #########
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_transformer_base.py:    ######## MLP #########
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_transformer_base.py:    ######## Norm #########
./DeepSpeed/deepspeed/inference/v2/model_implementations/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/__init__.py: Model Implementations
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/qkv.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/qkv.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/qkv.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/qkv.py:         Guaranteed to be in MHA
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/qkv.py:         32 KV heads, 16 shards for example
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/qkv.py:         8 KV heads, 16 shards for example
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/qkv.py:             This is equivalent to the original sharding algorithm since n_heads_q = C * n_heads_kv.
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/qkv.py:             If n_heads_kv % num_shards == 0, then n_heads_q % num_shards == 0.
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/qkv.py:             We will first do a sharding on the KV and Q to map to the one KV shard per group of Q.
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/embedding.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/embedding.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/embedding.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/attn_out.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/attn_out.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/attn_out.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/attn_out.py:         We will do the bias addition on the 0th rank only rather than scale the parameter and
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/attn_out.py:         implicitly reconstruct this in the distributed reduce.
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/attn_out.py:         32 KV heads, 16 shards for example
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/attn_out.py:         8 KV heads, 16 shards for example
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/attn_out.py:             Same as original sharding scenario
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/attn_out.py:             We will first do a sharding on the KV and Q to map to the one KV shard per group of Q.
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/types.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/types.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/types.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/types.py:     Inner dimension sharding corresponds to splitting the Tensor along the K-dimension
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/types.py:     of a matrix multiplication. This would be used for attention_output or MLP2.
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/types.py:     Outer dimension sharding corresponds to splitting the Tensor along the N-dimension
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/types.py:     of a matrix multiplication. This would be used for the QKV and MLP1 projections.
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/mlp.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/mlp.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/mlp.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/mlp.py:         We will do the bias addition on the 0th rank only rather than scale the parameter and
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/mlp.py:         implicitly reconstruct this in the distributed reduce.
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/utils.py:     Easier to hide this inside of the sharding logic than to add checks in every model
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/utils.py:     implementation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/utils.py:         Trivial case of no sharding.
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/utils.py:             Special case for bias parameters.
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/utils.py:             General case for weight parameters. This assumes MoE parameters are stored in the format of
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/utils.py:             [num_experts, out_features, in_features]
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/unembed.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/unembed.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/unembed.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/attn.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/attn.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/attn.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/attn.py:         MHA attention
./DeepSpeed/deepspeed/inference/v2/model_implementations/sharding/attn.py:         GQA attention
./DeepSpeed/deepspeed/inference/v2/model_implementations/llama_v2/llama_v2_containers.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/llama_v2/llama_v2_containers.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/llama_v2/llama_v2_containers.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/llama_v2/llama_v2_containers.py: Create a container object to save model-specific tensors using the policy file above.
./DeepSpeed/deepspeed/inference/v2/model_implementations/llama_v2/llama_v2_containers.py:  HF Llama model looks like this:
./DeepSpeed/deepspeed/inference/v2/model_implementations/llama_v2/llama_v2_model.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/llama_v2/llama_v2_model.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/llama_v2/llama_v2_model.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/llama_v2/llama_v2_model.py:         llama model family is special and is always gated so force gated versions of relu, gelu, silu
./DeepSpeed/deepspeed/inference/v2/model_implementations/llama_v2/llama_v2_model.py:         TODO(cmikeh2): Distribute ragged_batch_info to all modules
./DeepSpeed/deepspeed/inference/v2/model_implementations/llama_v2/llama_v2_model.py:         Should be configurable in the future
./DeepSpeed/deepspeed/inference/v2/model_implementations/llama_v2/llama_v2_model.py:             On last layer, we just need to perform the residual add. Adding into the residual
./DeepSpeed/deepspeed/inference/v2/model_implementations/llama_v2/llama_v2_model.py:             here is safe.
./DeepSpeed/deepspeed/inference/v2/model_implementations/llama_v2/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/llama_v2/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/llama_v2/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/llama_v2/llama_v2_policy.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/llama_v2/llama_v2_policy.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/llama_v2/llama_v2_policy.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/opt/policy.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/opt/policy.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/opt/policy.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/opt/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/opt/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/opt/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/opt/container.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/opt/container.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/opt/container.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/opt/container.py: Create a container object to save model-specific tensors using the policy file above.
./DeepSpeed/deepspeed/inference/v2/model_implementations/opt/container.py:  HF OPT model looks like this:
./DeepSpeed/deepspeed/inference/v2/model_implementations/opt/model.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/opt/model.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/opt/model.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/opt/model.py:         TODO(cmikeh2): Distribute ragged_batch_info to all modules
./DeepSpeed/deepspeed/inference/v2/model_implementations/opt/model.py:         Should be configurable in the future
./DeepSpeed/deepspeed/inference/v2/model_implementations/opt/model.py:             On last layer, we just need to perform the residual add. Adding into the residual
./DeepSpeed/deepspeed/inference/v2/model_implementations/opt/model.py:             here is safe.
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_model_base.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_model_base.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_model_base.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_model_base.py:         Set to None until the Policy sets the model parameters
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_model_base.py:         TODO(cmikeh2): Kind of a hack right now, but this is too verbose to use at
./DeepSpeed/deepspeed/inference/v2/model_implementations/inference_model_base.py:         TODO(cmikeh2): Kind of a hack right now, but this is too verbose to use at
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py: Currently have dependency loops for the type hints.
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:             We'll pick up all annotations on any base classes. This will allow us to
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:             to use inheritance to share common parameter groups in base classes.
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:                     This is likely a fail state. If a parent has MAPPING KEY but the child does
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:                     not, then we're guaranteed only a subset of the parameters will be mapped.
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:             If we have a mapping key at all, then we will enter the validation mode for building
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:             helpers for mapping and ensuring we have complete mapping.
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:             First we'll build a flat list of every dependency for this layer.
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:             Create static helper for doing the string processing only once.
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:             Iterate over all the mappings
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:                     Check for invalid mappings
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:                         This check is not universal (see below) if a single dependency is being
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:                         mapped to by a single row.
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:                     If we've made it this far, the dependency definitely exists.
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:                     We can do direct sets on everything but ParametrizedLists, so we'll only explicitly
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:                     handle these here.
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:                     TODO(cmikeh2): SPLIT, error if more than 1
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:                 TODO(cmikeh2): Do we want to make this a property
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:                 It might also make sense to do this in the base class __init__
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:                 but since it is tied with the changes made in __new__ it feels
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:                 to me like it should be here.
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:                 If we have an exact match, it's a direct mapping and we can
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:                 immediately set the value.
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:             If we have an exact match, it's a direct mapping and we can immediately set
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:             the value.
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:             Convert single targets to a list for consistency
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:                 Double setting doesn't set the attribute correctly, so we do a getattr then setattr
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:         Otherwise we need to map to one of the parameter lists.
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:                 We have a match, so we can set the value.
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:                 Convert single targets to a list for consistency
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:         TODO: Refactor this with the help of cmikeh2
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:         We should be able to combine this with the wildcard matching above.
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:             Convert single targets to a list for consistency
./DeepSpeed/deepspeed/inference/v2/model_implementations/layer_container_base.py:                 Double setting doesn't set the attribute correctly, so we do a getattr then setattr
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/mlp_parameters.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/mlp_parameters.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/mlp_parameters.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/mlp_parameters.py:         NOTE(cmikeh2): If we are gated but not in the format specified below, we should trigger a permutation here.
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/mlp_parameters.py:         I am not currently aware of any models that use this format (or how we should even detect it; probably should
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/mlp_parameters.py:         just be a different param entirely, but until then we'll just assume the format is correct).
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/mlp_parameters.py:         flip the order if even with the correct tokenizer we get wrong output
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/mlp_parameters.py:        fused_param = torch.cat([self.up_params, self.gate_params], dim=-1).reshape(total_neurons, -1)
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/unembed_parameters.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/unembed_parameters.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/unembed_parameters.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/moe_parameters.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/moe_parameters.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/moe_parameters.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/norm_parameters.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/norm_parameters.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/norm_parameters.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/invfreq_parameters.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/invfreq_parameters.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/invfreq_parameters.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/embedding_parameters.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/embedding_parameters.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/embedding_parameters.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/qkv_parameters.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/qkv_parameters.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/qkv_parameters.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/qkv_parameters.py:     Reshape to get the groups as the leading dimension
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/qkv_parameters.py:     Squeeze will remove extra dimension for bias
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/attn_output_parameters.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/attn_output_parameters.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/common_parameters/attn_output_parameters.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/parameter_base.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/parameter_base.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/parameter_base.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/parameter_base.py: Currently have dependency loops for the type hints.
./DeepSpeed/deepspeed/inference/v2/model_implementations/parameter_base.py:         Create properties for each of our dependencies
./DeepSpeed/deepspeed/inference/v2/model_implementations/parameter_base.py:         Initialize our dependences to None/empty `ParametrizedList`s
./DeepSpeed/deepspeed/inference/v2/model_implementations/parameter_base.py:                TODO(jeff): update assert with this, model implementation attribute does not align or missing wrt the ParametrizedList attributes
./DeepSpeed/deepspeed/inference/v2/model_implementations/parameter_base.py:     inference_model: InferenceModel
./DeepSpeed/deepspeed/inference/v2/model_implementations/parameter_base.py:    completed_components: int
./DeepSpeed/deepspeed/inference/v2/model_implementations/mistral/policy.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/mistral/policy.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/mistral/policy.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/mistral/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/mistral/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/mistral/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/mistral/container.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/mistral/container.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/mistral/container.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/mistral/container.py: Create a container object to save model-specific tensors using the policy file above.
./DeepSpeed/deepspeed/inference/v2/model_implementations/mistral/container.py:  HF Mistral model (mistralai/Mistral-7B-v0.1) looks like this:
./DeepSpeed/deepspeed/inference/v2/model_implementations/mistral/model.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/model_implementations/mistral/model.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/model_implementations/mistral/model.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/model_implementations/mistral/model.py:         TODO(cmikeh2): Distribute ragged_batch_info to all modules
./DeepSpeed/deepspeed/inference/v2/model_implementations/mistral/model.py:         Should be configurable in the future
./DeepSpeed/deepspeed/inference/v2/model_implementations/mistral/model.py:             On last layer, we just need to perform the residual add. Adding into the residual
./DeepSpeed/deepspeed/inference/v2/model_implementations/mistral/model.py:             here is safe.
./DeepSpeed/deepspeed/inference/v2/checkpoint/huggingface_engine.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/checkpoint/huggingface_engine.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/checkpoint/huggingface_engine.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/checkpoint/huggingface_engine.py:         Define this property here so we can use it in the model implementation
./DeepSpeed/deepspeed/inference/v2/checkpoint/huggingface_engine.py:         TODO(jeff): for models like llama-2 the user will have to provide an auth `token`,
./DeepSpeed/deepspeed/inference/v2/checkpoint/huggingface_engine.py:         currently coming from the ckpt engine init but maybe a catch all kwargs for other
./DeepSpeed/deepspeed/inference/v2/checkpoint/huggingface_engine.py:         snapshot download parameters would be more flexible.
./DeepSpeed/deepspeed/inference/v2/checkpoint/huggingface_engine.py:         NOTE(jeff): allow_patterns here are explicitly not using safetensors or other
./DeepSpeed/deepspeed/inference/v2/checkpoint/huggingface_engine.py:         checkpoint files that may be present. Example of all files in the llama-2-7b
./DeepSpeed/deepspeed/inference/v2/checkpoint/huggingface_engine.py:         repo here: https://huggingface.co/meta-llama/Llama-2-7b-hf/tree/main
./DeepSpeed/deepspeed/inference/v2/checkpoint/huggingface_engine.py:             We don't need any json as all such HF models will have pytorch_model.bin
./DeepSpeed/deepspeed/inference/v2/checkpoint/huggingface_engine.py:             weight_map -> { "lm_head.weight": "pytorch_model-00002-of-00002.bin", ... }
./DeepSpeed/deepspeed/inference/v2/checkpoint/huggingface_engine.py:             unique set of all checkpoint files
./DeepSpeed/deepspeed/inference/v2/checkpoint/huggingface_engine.py:             get absolute path of all unique checkpoint files
./DeepSpeed/deepspeed/inference/v2/checkpoint/huggingface_engine.py:     To test, add your auth_token here and run `python huggingface_engine.py`
./DeepSpeed/deepspeed/inference/v2/checkpoint/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/checkpoint/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/checkpoint/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/checkpoint/base_engine.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/checkpoint/base_engine.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/checkpoint/base_engine.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/checkpoint/base_engine.py:from .huggingface_engine import HuggingFaceCheckpointEngine
./DeepSpeed/deepspeed/inference/v2/checkpoint/in_memory_engine.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/checkpoint/in_memory_engine.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/checkpoint/in_memory_engine.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/ragged/manager_configs.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/ragged/manager_configs.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/ragged/manager_configs.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/ragged/manager_configs.py:         If the attributes below failed their validation they won't appear in the values dict.
./DeepSpeed/deepspeed/inference/v2/ragged/sequence_descriptor.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/ragged/sequence_descriptor.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/ragged/sequence_descriptor.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/ragged/sequence_descriptor.py:     Padded list of KV-cache IDs for the sequence.
./DeepSpeed/deepspeed/inference/v2/ragged/sequence_descriptor.py:     The location in the broader ID tensor where the KV-cache IDs for the sequence
./DeepSpeed/deepspeed/inference/v2/ragged/sequence_descriptor.py:     are stored. Used on flush.
./DeepSpeed/deepspeed/inference/v2/ragged/sequence_descriptor.py:    TODO: this was previously a property but causing issues with PR-4668 need to consult w. Connor
./DeepSpeed/deepspeed/inference/v2/ragged/sequence_descriptor.py:                 If we have multiple groups, it's possible to have an empty group.
./DeepSpeed/deepspeed/inference/v2/ragged/ragged_manager.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/ragged/ragged_manager.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/ragged/ragged_manager.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/ragged/ragged_manager.py:     Container for tracking all sequences in the system.
./DeepSpeed/deepspeed/inference/v2/ragged/ragged_manager.py:     Allocator for tracking sequences.
./DeepSpeed/deepspeed/inference/v2/ragged/ragged_manager.py:         Load our helpers for host allocation.
./DeepSpeed/deepspeed/inference/v2/ragged/ragged_manager.py:         Initialize the allocator for tracking sequences (so this doesn't need to be ad-hoc).
./DeepSpeed/deepspeed/inference/v2/ragged/ragged_manager.py:             Storage to back tracking the KV cache allocation.
./DeepSpeed/deepspeed/inference/v2/ragged/ragged_manager.py:         Initialize the sequence container.
./DeepSpeed/deepspeed/inference/v2/ragged/ragged_manager.py:         Finally initialize the KV cache.
./DeepSpeed/deepspeed/inference/v2/ragged/ragged_manager.py:         TODO(cmikeh2): Debug call here might be unnecessary and is potentially on critical path.
./DeepSpeed/deepspeed/inference/v2/ragged/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/ragged/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/ragged/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/ragged/kv_cache.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/ragged/kv_cache.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/ragged/kv_cache.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/ragged/kv_cache.py:             TODO(cmikeh2): Change the weighting based on the type of the KV-cache
./DeepSpeed/deepspeed/inference/v2/ragged/kv_cache.py:             Perform a dummy nccl call before calculating available memory, on some systems (H100) we've observed higher memory allocations from NCCL
./DeepSpeed/deepspeed/inference/v2/ragged/kv_cache.py:             In a multi-process setting, we need to ensure that all processes have the same
./DeepSpeed/deepspeed/inference/v2/ragged/kv_cache.py:             KV cache capacity to ensure scheduling guarantees are equivalent on all ranks.
./DeepSpeed/deepspeed/inference/v2/ragged/kv_cache.py:                 This is ugly but don't want the fragmentation of the 8 byte Tensor maybe
./DeepSpeed/deepspeed/inference/v2/ragged/kv_cache.py:                 hanging around.
./DeepSpeed/deepspeed/inference/v2/ragged/blocked_allocator.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/ragged/blocked_allocator.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/ragged/blocked_allocator.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/ragged/blocked_allocator.py:     Number of blocks in the KV-cache(s).
./DeepSpeed/deepspeed/inference/v2/ragged/blocked_allocator.py:     Array of blocks, where each element is the next block in the linked list.
./DeepSpeed/deepspeed/inference/v2/ragged/blocked_allocator.py:     Index of the head of the linked list.
./DeepSpeed/deepspeed/inference/v2/ragged/blocked_allocator.py:     Number of free blocks in the KV-cache.
./DeepSpeed/deepspeed/inference/v2/ragged/blocked_allocator.py:             Parse all blocks for validity before mutating the list.
./DeepSpeed/deepspeed/inference/v2/ragged/ragged_wrapper.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/ragged/ragged_wrapper.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/ragged/ragged_wrapper.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/ragged/ragged_wrapper.py:     TODO(cmikeh2): Tune this approach. This is mainly a placeholder right now.
./DeepSpeed/deepspeed/inference/v2/ragged/ragged_wrapper.py:     Tensors to populate the ragged batch into.
./DeepSpeed/deepspeed/inference/v2/ragged/ragged_wrapper.py:     Holds the block ids for each sequence in the ragged batch.
./DeepSpeed/deepspeed/inference/v2/ragged/ragged_wrapper.py:         Default behavior should be no padding
./DeepSpeed/deepspeed/inference/v2/ragged/ragged_wrapper.py:             This doesn't really fall under schedulability, so we'll unconditionally check for it.
./DeepSpeed/deepspeed/inference/v2/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/scheduling_utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/scheduling_utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/scheduling_utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/inference_parameter.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/inference_parameter.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/inference_parameter.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/engine_factory.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/engine_factory.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/engine_factory.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/engine_factory.py:     Load metadata, for grabbing the policy name we'll have all ranks just check for
./DeepSpeed/deepspeed/inference/v2/engine_factory.py:     rank 0.
./DeepSpeed/deepspeed/inference/v2/engine_factory.py:     Get the policy
./DeepSpeed/deepspeed/inference/v2/engine_factory.py:     Load the model config
./DeepSpeed/deepspeed/inference/v2/engine_factory.py:         Set up logging
./DeepSpeed/deepspeed/inference/v2/engine_factory.py:         get HF checkpoint engine
./DeepSpeed/deepspeed/inference/v2/engine_factory.py:         get model config from HF AutoConfig
./DeepSpeed/deepspeed/inference/v2/engine_factory.py:         get the policy
./DeepSpeed/deepspeed/inference/v2/engine_factory.py:         TODO: generalize this to other models
./DeepSpeed/deepspeed/inference/v2/engine_factory.py:             Ensure we're using the correct version of transformers for mistral
./DeepSpeed/deepspeed/inference/v2/config_v2.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/config_v2.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/config_v2.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/heuristics.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/heuristics.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/heuristics.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/heuristics.py:     Currently, we only have one implementation, so we just return it.
./DeepSpeed/deepspeed/inference/v2/modules/heuristics.py:     Currently, we only have one implementation, so we just return it.
./DeepSpeed/deepspeed/inference/v2/modules/heuristics.py:     Currently, we only have one implementation, so we just return it.
./DeepSpeed/deepspeed/inference/v2/modules/heuristics.py:         TODO: Get this off an engine config
./DeepSpeed/deepspeed/inference/v2/modules/heuristics.py:     Currently, we only have one implementation, so we just return it.
./DeepSpeed/deepspeed/inference/v2/modules/heuristics.py:     Currently, we only have one implementation, so we just return it.
./DeepSpeed/deepspeed/inference/v2/modules/heuristics.py:     Currently, we only have one implementation, so we just return it.
./DeepSpeed/deepspeed/inference/v2/modules/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/module_registry.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/module_registry.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/module_registry.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/configs/linear_config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/configs/linear_config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/configs/linear_config.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/configs/embedding_config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/configs/embedding_config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/configs/embedding_config.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/configs/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/configs/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/configs/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/configs/moe_config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/configs/moe_config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/configs/moe_config.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py:     No positional embeddings
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py:     Rotary positional embeddings - every half
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py:     Rotary positional embeddings - every other
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py:     Alibi
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py:     No masking
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py:     Causal masking
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py:     Local masking
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py:     Symmetric masking (this is a 1D tensor mask)
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py:     Arbitrary masking (this would correspond to a 2D tensor mask)
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py:     Number of query attention heads on this shard
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py:     Number of KV attention heads on this shard
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py:     Size of each attention head
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py:     Max number of sequences that may compose a ragged batch
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py:     Scale factor for attention scores
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py:     Input data type
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py:     Output data type
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py:     Masking type
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py:     Masking args
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py:     Positional embedding type
./DeepSpeed/deepspeed/inference/v2/modules/configs/attention_configs.py:     Positional embedding args
./DeepSpeed/deepspeed/inference/v2/modules/configs/norm_config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/configs/norm_config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/configs/norm_config.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/configs/norm_config.py:     Type of normalization
./DeepSpeed/deepspeed/inference/v2/modules/configs/norm_config.py:     Number of channels in the model embedding
./DeepSpeed/deepspeed/inference/v2/modules/configs/norm_config.py:     Data type of the residual input/outputs (we assume the residual must
./DeepSpeed/deepspeed/inference/v2/modules/configs/norm_config.py:     be the same data type for the entire model).
./DeepSpeed/deepspeed/inference/v2/modules/configs/norm_config.py:     Data type of the hidden states input
./DeepSpeed/deepspeed/inference/v2/modules/configs/norm_config.py:     Data type of the hidden states output
./DeepSpeed/deepspeed/inference/v2/modules/configs/norm_config.py:     Epsilon value for numerical stability
./DeepSpeed/deepspeed/inference/v2/modules/configs/unembed_config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/configs/unembed_config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/configs/unembed_config.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/implementations/attention/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/implementations/attention/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/implementations/attention/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/implementations/attention/dense_blocked_attention.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/implementations/attention/dense_blocked_attention.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/implementations/attention/dense_blocked_attention.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/implementations/attention/dense_blocked_attention.py:         TODO(cmikeh2): Attention kernel gets created here.
./DeepSpeed/deepspeed/inference/v2/modules/implementations/attention/dense_blocked_attention.py:         TODO(cmikeh2): Pre-allocate storage buffer for the attention atoms.
./DeepSpeed/deepspeed/inference/v2/modules/implementations/unembed/ragged_unembed.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/implementations/unembed/ragged_unembed.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/implementations/unembed/ragged_unembed.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/implementations/unembed/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/implementations/unembed/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/implementations/unembed/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/implementations/linear/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/implementations/linear/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/implementations/linear/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/implementations/linear/blas_fp_linear.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/implementations/linear/blas_fp_linear.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/implementations/linear/blas_fp_linear.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/implementations/embedding/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/implementations/embedding/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/implementations/embedding/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/implementations/embedding/ragged_embedding.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/implementations/embedding/ragged_embedding.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/implementations/embedding/ragged_embedding.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/implementations/embedding/ragged_embedding.py:         TODO(cmikeh2): How do we want to avoid the int32 vs int64 issue?
./DeepSpeed/deepspeed/inference/v2/modules/implementations/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/implementations/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/implementations/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/implementations/__init__.py: Imports for registering ops
./DeepSpeed/deepspeed/inference/v2/modules/implementations/post_norm/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/implementations/post_norm/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/implementations/post_norm/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/implementations/post_norm/cuda_post_ln.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/implementations/post_norm/cuda_post_ln.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/implementations/post_norm/cuda_post_ln.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/implementations/pre_norm/cuda_pre_ln.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/implementations/pre_norm/cuda_pre_ln.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/implementations/pre_norm/cuda_pre_ln.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/implementations/pre_norm/cuda_pre_ln.py:         Buffers for the hidden output (residual is updated in-place)
./DeepSpeed/deepspeed/inference/v2/modules/implementations/pre_norm/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/implementations/pre_norm/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/implementations/pre_norm/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/implementations/pre_norm/cuda_pre_rms.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/implementations/pre_norm/cuda_pre_rms.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/implementations/pre_norm/cuda_pre_rms.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/implementations/pre_norm/cuda_pre_rms.py:             Only need to check one since the support matrix for the two rms kernels is the same
./DeepSpeed/deepspeed/inference/v2/modules/implementations/pre_norm/cuda_pre_rms.py:         Buffers for both the hidden and residual outputs
./DeepSpeed/deepspeed/inference/v2/modules/implementations/moe/cutlass_multi_gemm.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/implementations/moe/cutlass_multi_gemm.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/implementations/moe/cutlass_multi_gemm.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/implementations/moe/cutlass_multi_gemm.py:             Currently not supporting gated activations in MoE
./DeepSpeed/deepspeed/inference/v2/modules/implementations/moe/cutlass_multi_gemm.py:         Convenience variables for frequently accessed items.
./DeepSpeed/deepspeed/inference/v2/modules/implementations/moe/cutlass_multi_gemm.py:         Gating buffers
./DeepSpeed/deepspeed/inference/v2/modules/implementations/moe/cutlass_multi_gemm.py:         Scatter buffers
./DeepSpeed/deepspeed/inference/v2/modules/implementations/moe/cutlass_multi_gemm.py:         GEMM Buffers
./DeepSpeed/deepspeed/inference/v2/modules/implementations/moe/cutlass_multi_gemm.py:         Gather buffer
./DeepSpeed/deepspeed/inference/v2/modules/implementations/moe/cutlass_multi_gemm.py:         Get views on the buffers for gating
./DeepSpeed/deepspeed/inference/v2/modules/implementations/moe/cutlass_multi_gemm.py:         Get views on the buffers for GEMM
./DeepSpeed/deepspeed/inference/v2/modules/implementations/moe/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/implementations/moe/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/implementations/moe/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/attention_base.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/attention_base.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/attention_base.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/post_norm_base.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/post_norm_base.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/post_norm_base.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/unembed_base.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/unembed_base.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/unembed_base.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/moe_base.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/moe_base.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/moe_base.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/linear_base.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/linear_base.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/linear_base.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/pre_norm_base.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/pre_norm_base.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/pre_norm_base.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/embedding_base.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/embedding_base.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/interfaces/embedding_base.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/modules/ds_module.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/modules/ds_module.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/modules/ds_module.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/engine_v2.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/engine_v2.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/engine_v2.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/engine_v2.py:         Build model from policy
./DeepSpeed/deepspeed/inference/v2/engine_v2.py:         Create state manager
./DeepSpeed/deepspeed/inference/v2/engine_v2.py:             We can disable checks since we already validated schedulability.
./DeepSpeed/deepspeed/inference/v2/engine_v2.py:         Send all metadata to the device
./DeepSpeed/deepspeed/inference/v2/engine_v2.py:         Prep all data structures for the actual forward (in anticipation of CG in the future)
./DeepSpeed/deepspeed/inference/v2/engine_v2.py:         and also to amortize some of the costs in a more straightforward way.
./DeepSpeed/deepspeed/inference/v2/engine_v2.py:         Model implementation will pick up in the forward.
./DeepSpeed/deepspeed/inference/v2/engine_v2.py:         We return one set of logits per sequence in the batch (saves cost on unembedding)
./DeepSpeed/deepspeed/inference/v2/engine_v2.py:             Can only compose a batch from a limited number of sequences
./DeepSpeed/deepspeed/inference/v2/engine_v2.py:                 We ran out of KV cache
./DeepSpeed/deepspeed/inference/v2/engine_v2.py:             Would run out of tracking metadata
./DeepSpeed/deepspeed/inference/v2/engine_v2.py:             Would exceed the maximum batch size
./DeepSpeed/deepspeed/inference/v2/engine_v2.py:         Save the flattened parameters
./DeepSpeed/deepspeed/inference/v2/allocator.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/allocator.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/allocator.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/inference_utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/v2/inference_utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/v2/inference_utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/v2/inference_utils.py:     The torch dtype must always be the first value (so we return torch.dtype)
./DeepSpeed/deepspeed/inference/v2/inference_utils.py:     Copied from https://stackoverflow.com/a/43210118
./DeepSpeed/deepspeed/inference/v2/inference_utils.py:     Allows us to use multiple values for each Enum index and returns first
./DeepSpeed/deepspeed/inference/v2/inference_utils.py:     listed value when Enum is called
./DeepSpeed/deepspeed/inference/v2/inference_utils.py:         first value is canonical value
./DeepSpeed/deepspeed/inference/engine.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/inference/engine.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/inference/engine.py: DeepSpeed Team
./DeepSpeed/deepspeed/inference/engine.py:         Have to import here because inference_module is a global, but python
./DeepSpeed/deepspeed/inference/engine.py:         globals only work at the module level and will not be updated unless
./DeepSpeed/deepspeed/inference/engine.py:         we import it each time we init a new inference engine.
./DeepSpeed/deepspeed/inference/engine.py:         patch model generate with ours if model uses it
./DeepSpeed/deepspeed/inference/engine.py:         todo: keep this self.injection_dict because we don't use to change config.injection_policy API
./DeepSpeed/deepspeed/inference/engine.py:         todo: this will get changed when Molly's PR on auto injection dict is merged
./DeepSpeed/deepspeed/inference/engine.py:         todo: refactor the mp_group and mp_size related in the next refactor
./DeepSpeed/deepspeed/inference/engine.py:        self._validate_args(self.mpu, config.replace_with_kernel_inject)
./DeepSpeed/deepspeed/inference/engine.py:         these are not needed in the config as we are creating them ourselves in the inference engine
./DeepSpeed/deepspeed/inference/engine.py:             This is a hack to remove the prepare_mask function on HF side for BLOOM architecture
./DeepSpeed/deepspeed/inference/engine.py:             This is a hack to redefine the alibi func due to TP
./DeepSpeed/deepspeed/inference/engine.py:         Check if model passed to engine is loaded w/ meta tensors, in which case
./DeepSpeed/deepspeed/inference/engine.py:         kernel injection must be enabled.
./DeepSpeed/deepspeed/inference/engine.py:         NOTE: This check assumes a Hugging Face hierarchy for the device type i.e. module.device.type
./DeepSpeed/deepspeed/inference/engine.py:         convert model to intended dtype
./DeepSpeed/deepspeed/inference/engine.py:         We only support three modes: 1) user specified policy for tensor-parallelism, 2) kernel injection (replace_with_kernel_inject), and 3) automatic tensor parallelism if tp_size > 1.
./DeepSpeed/deepspeed/inference/engine.py:             1. User specified Tensor Parallelism
./DeepSpeed/deepspeed/inference/engine.py:                 construct the tuple and pass that instead of a string or dict.
./DeepSpeed/deepspeed/inference/engine.py:                 2. DeepSpeed Kernel Injection
./DeepSpeed/deepspeed/inference/engine.py:                 3. Automatic Tensor Parallelism
./DeepSpeed/deepspeed/inference/engine.py:         Check if local CUDA graphs can be created in replacement modules
./DeepSpeed/deepspeed/inference/engine.py:         Have to import here because inference_module is a global, but python
./DeepSpeed/deepspeed/inference/engine.py:         globals only work at the module level and will not be updated unless
./DeepSpeed/deepspeed/inference/engine.py:         we import it each time we init a new inference engine.
./DeepSpeed/deepspeed/inference/engine.py:     todo: remove this once all the config dicts are centralized from top level pydantic config
./DeepSpeed/deepspeed/inference/engine.py:         this is being passed to replace_transformer_layer(config=self.user_model_config_dict)
./DeepSpeed/deepspeed/inference/engine.py:         Call the init process
./DeepSpeed/deepspeed/inference/engine.py:         Call the init process
./DeepSpeed/deepspeed/inference/engine.py:     TODO: remove this function and add this functionality to pydantic config checking
./DeepSpeed/deepspeed/inference/engine.py:         TODO: to support SD pipeline we need to avoid this check for now
./DeepSpeed/deepspeed/inference/engine.py:                     meta tensor cannot be casted or copied to, so we need to replace it with a normal tensor here
./DeepSpeed/deepspeed/inference/engine.py:                     meta tensor cannot be casted or copied to, so we need to replace it with a normal tensor here
./DeepSpeed/deepspeed/inference/engine.py:                         meta tensor cannot be casted or copied to, so we need to replace it with a normal tensor here
./DeepSpeed/deepspeed/inference/engine.py:                         meta tensor cannot be casted or copied to, so we need to replace it with a normal tensor here
./DeepSpeed/deepspeed/inference/engine.py:         client_module is only passed when using the injection_dict method.
./DeepSpeed/deepspeed/inference/engine.py:             config is our DeepSpeedInferenceConfig and self.config is the HF model config
./DeepSpeed/deepspeed/inference/engine.py:         warmup to create the workspace and cublas handle
./DeepSpeed/deepspeed/inference/engine.py:         create cuda_graph and assign static_inputs and static_outputs
./DeepSpeed/deepspeed/inference/engine.py:         Reset KV-cache at the beginning of generate
./DeepSpeed/deepspeed/ops/random_ltd/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/random_ltd/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/random_ltd/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/random_ltd/dropping_utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/random_ltd/dropping_utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/random_ltd/dropping_utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/random_ltd/dropping_utils.py:     Not certain the optimized kernel is actually better here, cause it kind of screws
./DeepSpeed/deepspeed/ops/random_ltd/dropping_utils.py:     with alignment right if the sequence length is not divisible by like 16
./DeepSpeed/deepspeed/ops/random_ltd/dropping_utils.py:     new_mask = random_ltd_module.mask_gather_gpt(attn_mask, reserved_length)
./DeepSpeed/deepspeed/ops/lamb/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/lamb/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/lamb/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/lamb/fused_lamb.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/lamb/fused_lamb.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/lamb/fused_lamb.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/lamb/fused_lamb.py:         backward compatibility
./DeepSpeed/deepspeed/ops/lamb/fused_lamb.py:         assuming a list/generator of parameter means single group
./DeepSpeed/deepspeed/ops/lamb/fused_lamb.py:        remove the previous coeffs
./DeepSpeed/deepspeed/ops/lamb/fused_lamb.py:                 compute combined scale factor for this group
./DeepSpeed/deepspeed/ops/lamb/fused_lamb.py:                     norm is in fact norm*scale
./DeepSpeed/deepspeed/ops/lamb/fused_lamb.py:                note: p.grad should not ever be set for correct operation of mixed precision optimizer that sometimes sends None gradients
./DeepSpeed/deepspeed/ops/lamb/fused_lamb.py:                 State initialization
./DeepSpeed/deepspeed/ops/lamb/fused_lamb.py:                     Exponential moving average of gradient values
./DeepSpeed/deepspeed/ops/lamb/fused_lamb.py:                     Exponential moving average of squared gradient values
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py: DeepSpeed note, code taken & adapted from commit 9aa94789f13ada713af36cfd8cca2fc9a7f6b79a
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py: https://github.com/ptillet/torch-blocksparse/blob/master/torch_blocksparse/matmul.py
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:    ------------#
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:    - Prologue -#
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:    ------------#
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:             output offset
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:             dense input offset
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:             sparse input offset
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:             output offset
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:             dense input offset
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:             sparse input offset
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:     initialize a, b pointers
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:    # ---------------- ##
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:    #    Inner Loop    ##
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:    # ---------------- ##
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         pre-fetch
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:     write-back directly
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:     accumulate partial results using spin-locks
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:#############
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:  MAIN API  #
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:#############
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:     Given an array sizes representing reduction size for each
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:     column of a block-mode matrix multiplication,
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:     performs load-balancing to achieve more smaller reductions
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:     between `seg_size` elements
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:        global triton
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:        if triton is None:
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:            triton = importlib.import_module('triton')
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         segment size
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         heuristics taken from OpenAI blocksparse code
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         https://github.com/openai/blocksparse/blob/master/blocksparse/matmul.py#L95
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:        if max_size > min_size * 2.0:
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:          seg_max = max(triton.cdiv(max_size, 4), min_size*2)
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:        else:
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:          seg_max = max_size
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         split reduction into segments
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:             column id
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:             lock id
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:             segment size
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:    #########################
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:     SPARSE = DENSE x DENSE #
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:    #########################
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:        _sparse_matmul._load_utils()
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:        start_width = 64 // block
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:        segmented = _sparse_matmul.sdd_segment(layout.type(torch.int32), start_width)
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         create locks
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         Shape check
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         create kernel
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:             create output
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:             maximum grid size is 65535
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:             so operation might be decomposed into multiple
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:             kernel calls
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         save for backward pass
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:    #########################
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:     DENSE = DENSE x SPARSE #
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:    #########################
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:     Given a binary layout of 0s and 1s,
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:     Construct look-up table for efficient execution on GPUs
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         load-balancing
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:             concatenate depth
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         pointer increments
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         divide block into multiple steps
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         first increment for each reduction is actually the offset
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         block-mode input increments
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         adjust offset and segment size
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         create header
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         create lut
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         create locks
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         shapes / dtypes
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         kernel
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         output
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         shapes / dtypes
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         kernel
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         output
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         save for backward
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         saved for backward
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         gradients w.r.t. a
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         gradients w.r.t. b
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         C look-up table
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         DA look-up table
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         DB look-up table
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         look-up table cache
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         attributes
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:             Dims to be reduced on the 'inside' of the matmul, either -1 or -2
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:             Inner dim of the dense input should be equal to the inner dim of the sparse input
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:             Expected shape for sparse inputs
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         Support using the same layout across attention heads etc.
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         timings
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:     pad shapes of a tensor to make it
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:     compatible with kernel calls
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         timings
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         pad shapes with ones
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         execute
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         This removes any leading singleton dimensions we may have added to the tensor that weren't in the input
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         When autocast is enabled, torch.matmul autocasts to float16, so we do the same here
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:             One input is sparse
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:             Add extra leading singleton dimensions if needed
./DeepSpeed/deepspeed/ops/sparse_attention/matmul.py:         Pad shapes with leading singleton dimensions
./DeepSpeed/deepspeed/ops/sparse_attention/bert_sparse_self_attention.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/sparse_attention/bert_sparse_self_attention.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/sparse_attention/bert_sparse_self_attention.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/sparse_attention/bert_sparse_self_attention.py:         SparsityConfig parameters needs to be set accordingly
./DeepSpeed/deepspeed/ops/sparse_attention/trsrc/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/sparse_attention/trsrc/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/sparse_attention/trsrc/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/sparse_attention/trsrc/__init__.py: Go over all local source files and parse them as strings
./DeepSpeed/deepspeed/ops/sparse_attention/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/sparse_attention/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/sparse_attention/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_self_attention.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_self_attention.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_self_attention.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_self_attention.py:             SparsityConfig parameters needs to be set accordingly
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_self_attention.py:         sparsity information
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_self_attention.py:         initialize sparse layout and register as buffer
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_self_attention.py:         mask modes
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_self_attention.py:         if layout is never synchronized across GPUs, broadcast the layout from global rank 0
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_self_attention.py:     add to cache
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_self_attention.py:     forward pass
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_self_attention.py:         transpose back key if it is already transposed
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_self_attention.py:         check that operation is supported
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_self_attention.py:         squeeze key_padding_mask if it is given
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_self_attention.py:         squeeze attn_mask if it is given
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_self_attention.py:         cache look-up table computations etc
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_self_attention.py:         attention scores
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_self_attention.py:         outputs
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_attention_utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_attention_utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_attention_utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_attention_utils.py:             RoBERTa has positions 0 & 1 reserved, so embedding size is max position + 2
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_attention_utils.py:         SparsityConfig parameters needs to be set accordingly
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_attention_utils.py:         SparsityConfig parameters needs to be set accordingly
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_attention_utils.py:             may not be needed as input_ids are not used if inputs_embeds are given
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_attention_utils.py:                 pad position_id with pad_token_id
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_attention_utils.py:             pad attention mask without attention on the padding tokens
./DeepSpeed/deepspeed/ops/sparse_attention/sparse_attention_utils.py:             pad token_type_ids with token_type_id = 0
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:         TODO Currently we allocate layout per head; needs to be updated if heads share a single layout.
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:         set all global blocks except the last one if (in last local window)
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:             vertical global attention
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:            (((i // self.num_local_blocks) + 1) * self.num_local_blocks)
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:            if (first_row < num_blocks):
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:             horizontal global attention; only in bidirectional attention
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:         set last global blocks; handle possible short last local window
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:             vertical global attention
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:            (((start // self.num_local_blocks) + 1) * self.num_local_blocks)
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:            if (first_row < num_blocks):
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:             horizontal global attention
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:         if there is any remaining not attended part, use the lats local window block size as local window for the remaining applicable local windows
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:                 if global block idx is in the range of the sequence blocks
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:                    global rows
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:                    global columns
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:                 if global block idx is in the range of the sequence blocks
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:                    global rows
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:                    global columns
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:        global rows
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:        global columns
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:             zero out anything attending to the future
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:                 if global block idx is in the range of the sequence blocks
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:                    global rows
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:                    global columns
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:                 if global block idx is in the range of the sequence blocks
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:                    global rows
./DeepSpeed/deepspeed/ops/sparse_attention/sparsity_config.py:                    global columns
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py: DeepSpeed note, code taken & adapted from commit 9aa94789f13ada713af36cfd8cca2fc9a7f6b79a
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py: https://github.com/ptillet/torch-blocksparse/blob/master/torch_blocksparse/matmul.py
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:     create index ranges
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:     extract information from LUT
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:     block id and column id
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:     pointers to X
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:     apply scale
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:     apply RPE
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:     apply key-padding mask
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:     apply attention mask
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:     computation
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:     create index ranges
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:     extract information from look-up table
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:     bounds checking on lut
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:     initialize pointers to block-sparse input
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:     compute fused softmax backward
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:         sizes along rows
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:         offsets in block format
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:         block indices
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:         construct look-up table
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:         handle None rpe
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:         handle None key_padding_mask
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:         handle None attention_mask
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:         run kernel
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:         save to context
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:         retrieve from context
./DeepSpeed/deepspeed/ops/sparse_attention/softmax.py:         run kernel
./DeepSpeed/deepspeed/ops/deepspeed4science/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/deepspeed4science/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/deepspeed4science/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/deepspeed4science/evoformer_attn.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/deepspeed4science/evoformer_attn.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/deepspeed4science/evoformer_attn.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/transformer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/transformer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/transformer.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/transformer.py: Cuda modules will be imported if needed
./DeepSpeed/deepspeed/ops/transformer/transformer.py:         For testing only.
./DeepSpeed/deepspeed/ops/transformer/transformer.py:         This appears to be an effective way to release context memory
./DeepSpeed/deepspeed/ops/transformer/transformer.py:             For testing only.
./DeepSpeed/deepspeed/ops/transformer/transformer.py:            self.attn_qkvw[i * self.config.hidden_size:(i + 1) * self.config.hidden_size] = \
./DeepSpeed/deepspeed/ops/transformer/transformer.py:                initial_weights[i].clone()
./DeepSpeed/deepspeed/ops/transformer/transformer.py:            torch.empty_like(initial_weights[i]).data.copy_(initial_weights[i].data)
./DeepSpeed/deepspeed/ops/transformer/transformer.py:         Load cuda modules if needed
./DeepSpeed/deepspeed/ops/transformer/transformer.py:         create the layer in cuda kernels.
./DeepSpeed/deepspeed/ops/transformer/inference/diffusers_attention.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/diffusers_attention.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/diffusers_attention.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/diffusers_attention.py: Cuda modules will be imported if needed
./DeepSpeed/deepspeed/ops/transformer/inference/diffusers_attention.py:             https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/gpt2/modeling_gpt2.py#L191
./DeepSpeed/deepspeed/ops/transformer/inference/diffusers_2d_transformer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/diffusers_2d_transformer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/diffusers_2d_transformer.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/config.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/ds_mlp.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/ds_mlp.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/ds_mlp.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/ds_mlp.py:         used for quantization
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py:         used for quantization
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py:             https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/gpt2/modeling_gpt2.py#L191
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py:    ########## This part is taken/modified form the HF modeling_bloom.py ################
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py:     Reference: https://github.com/huggingface/transformers/blob/main/src/transformers/models/bloom/modeling_bloom.py
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py:         Get the size and dimension.
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py:         Split.
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py:         Note: torch.split does not create contiguous tensors by default.
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py:         [batch_size, head_dim, q_length, k_length]
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py:         [batch_size, q_length, num_heads, head_dim] -> [q_length, batch_size * num_heads, head_dim]
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py:         [batch_size, k_length, num_heads, head_dim] -> [k_length, batch_size * num_heads, head_dim]
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py:             concatenate along seq_length dimension -> [batch_size, qk_length, num_heads, head_dim]
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py:         Raw attention scores. [batch_size * num_heads, q_length, k_length]
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py:         change view to [batch_size, num_heads, q_length, k_length]
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py:         When using the hybrid engine with BLOOM, input_mask needs to be converted from torch.bool -> torch.int64
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py:         change view [batch_size x num_heads, q_length, k_length]
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py:         matmul: [batch_size * num_heads, q_length, head_dim]
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py:         change view [batch_size, num_heads, q_length, head_dim]
./DeepSpeed/deepspeed/ops/transformer/inference/ds_attention.py:    ##################### End of HF modeling_bloom addition ########################
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py:             self-ouput weights
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py:         triton flash attention is enabled when the compute capability >= 8.0
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py:         used for quantization
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py:             https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/gpt2/modeling_gpt2.py#L191
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py:         triton autotune table update for score/context matmul
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py:     initialize offsets
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py:     mask
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py:     initialize pointer to m and l
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py:     scale sm_scale by log_2(e) and use
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py:     2^x instead of exp in the loop because CSE and LICM
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py:     don't work as expected with `exp` in the loop
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py:     load q: it will stay in SRAM throughout
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py:     loop over k, v and update accumulator
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py:         -- load k, v --
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py:         -- compute qk ---
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py:         -- compute scaling constant ---
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py:         -- scale and update acc --
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py:         -- update m_i and l_i --
./DeepSpeed/deepspeed/ops/transformer/inference/triton/attention.py:     write back l and m
./DeepSpeed/deepspeed/ops/transformer/inference/triton/gelu.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/triton/gelu.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/triton/gelu.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/triton/gelu.py:     Using approximation introduces greater parity errors.
./DeepSpeed/deepspeed/ops/transformer/inference/triton/gelu.py:     return tl.sigmoid(1.702 * x) * x
./DeepSpeed/deepspeed/ops/transformer/inference/triton/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/triton/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/triton/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/triton/mlp.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/triton/mlp.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/triton/mlp.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/triton/mlp.py:         used for quantization
./DeepSpeed/deepspeed/ops/transformer/inference/triton/ops.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/triton/ops.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/triton/ops.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/triton/ops.py:     activation
./DeepSpeed/deepspeed/ops/transformer/inference/triton/ops.py:     intermediate fc in FF
./DeepSpeed/deepspeed/ops/transformer/inference/triton/ops.py:     output fc in FF
./DeepSpeed/deepspeed/ops/transformer/inference/triton/ops.py:     residual add and layerNorm after attention
./DeepSpeed/deepspeed/ops/transformer/inference/triton/ops.py:     activation
./DeepSpeed/deepspeed/ops/transformer/inference/triton/ops.py:     intermediate fc in FF
./DeepSpeed/deepspeed/ops/transformer/inference/triton/ops.py:     output fc in FF
./DeepSpeed/deepspeed/ops/transformer/inference/triton/ops.py:     residual add and layerNorm after attention
./DeepSpeed/deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py:     BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, num_warps, num_stages
./DeepSpeed/deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py:     1. make sure we have enough smem
./DeepSpeed/deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py:         basic configs for compute-bound matmuls
./DeepSpeed/deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py:     matrix multiplication
./DeepSpeed/deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py:     re-order program ID for better L2 performance
./DeepSpeed/deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py:     do matrix multiplication
./DeepSpeed/deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py:     pointers
./DeepSpeed/deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py:     bias addition
./DeepSpeed/deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py:     activation
./DeepSpeed/deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py:        acc = tl.sigmoid(1.702 * acc) * acc
./DeepSpeed/deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py:     rematerialize rm and rn to save registers
./DeepSpeed/deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py:     handles write-back with reduction-splitting
./DeepSpeed/deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py:         BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, num_warps, num_stages
./DeepSpeed/deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py:         make sure we have enough smem
./DeepSpeed/deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py:     Pointers to matrices
./DeepSpeed/deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py:     Matrix dimensions
./DeepSpeed/deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py:     Meta-parameters
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py: -----------------------------------------------------------------------------
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py: util class/functions for triton
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py:     activation
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py:         if caching is enabled, get the lock and bin path
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py: -----------------------------------------------------------------------------
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py: triton matmul class
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py:         fp16 activation and fp16 weight matmul into fp16 output
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py:             handle non-contiguous inputs if necessary
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py:             checks constraints
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py:             allocates output
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py:             accumulator types
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py:             launch kernel
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py:         checks constraints
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py:         allocates output
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py:         checks constraints
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py:         allocates output
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py:             Here we also transpose the output when writing to memory.
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py: -----------------------------------------------------------------------------
./DeepSpeed/deepspeed/ops/transformer/inference/triton/matmul_ext.py: mapping
./DeepSpeed/deepspeed/ops/transformer/inference/triton/softmax.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/triton/softmax.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/triton/softmax.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/triton/softmax.py:     Allocate output
./DeepSpeed/deepspeed/ops/transformer/inference/triton/residual_add.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/triton/residual_add.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/triton/residual_add.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/triton/residual_add.py:     check that all tensors are on the same device
./DeepSpeed/deepspeed/ops/transformer/inference/triton/residual_add.py:     check that all tensors have the same dtype
./DeepSpeed/deepspeed/ops/transformer/inference/triton/residual_add.py:     check that all tensors have the right shape
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:     position of elements processed by this program
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:     compute mean
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:     compute variance
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:     multiply by weight and add bias
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:         # write-back
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:     position of elements processed by this program
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:     compute mean
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:     compute variance
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:     multiply by weight and add bias
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:         write-back
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:     position of elements processed by this program
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:     compute mean
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:     compute variance
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:     multiply by weight and add bias
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:         write-back
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:     allocate output
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:     reshape input data into 2D tensor
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:     Less than 64KB per feature: enqueue fused kernel
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:     heuristics for number of warps
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:     allocate output and scratch-pad for residual addition
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:     reshape input data into 2D tensor
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:     Less than 64KB per feature: enqueue fused kernel
./DeepSpeed/deepspeed/ops/transformer/inference/triton/layer_norm.py:     heuristics for number of warps
./DeepSpeed/deepspeed/ops/transformer/inference/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/bias_add.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/bias_add.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/bias_add.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:     initialize offsets
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:     initialize pointer to m and l
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:     scale sm_scale by log_2(e) and use
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:     2^x instead of exp in the loop because CSE and LICM
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:     don't work as expected with `exp` in the loop
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:     load q: it will stay in SRAM throughout
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:     loop over k, v and update accumulator
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:    hi = (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:    hi = (start_m + 1) * BLOCK_M
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:         -- load k, v --
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:         -- compute qk ---
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:        if IS_CAUSAL:
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float("-inf"))
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:         -- compute scaling constant ---
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:         -- scale and update acc --
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:         -- update m_i and l_i --
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:         update pointers
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:     write back l and m
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:    l_ptrs = L + off_hz * N_CTX + offs_m
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:    tl.store(l_ptrs, m_i + tl.math.log2(l_i))
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:     write back O
./DeepSpeed/deepspeed/ops/transformer/inference/triton_ops.py:         shape constraints
./DeepSpeed/deepspeed/ops/transformer/inference/diffusers_transformer_block.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/diffusers_transformer_block.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/diffusers_transformer_block.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/diffusers_transformer_block.py: Ops will be loaded on demand
./DeepSpeed/deepspeed/ops/transformer/inference/diffusers_transformer_block.py:         Ensure ops are built by the time we start running
./DeepSpeed/deepspeed/ops/transformer/inference/diffusers_transformer_block.py:         Pull the bias in if we can
./DeepSpeed/deepspeed/ops/transformer/inference/diffusers_transformer_block.py:         Pull the bias in if we can
./DeepSpeed/deepspeed/ops/transformer/inference/diffusers_transformer_block.py:         In v0.12.0 of diffuser, several new kwargs were added. Capturing
./DeepSpeed/deepspeed/ops/transformer/inference/diffusers_transformer_block.py:         those with kwargs to maintain backward compatibility
./DeepSpeed/deepspeed/ops/transformer/inference/diffusers_transformer_block.py:         In v0.11.0 of diffusers, the kwarg was changed from 'context' to 'encoder_hidden_states'
./DeepSpeed/deepspeed/ops/transformer/inference/diffusers_transformer_block.py:         This is so we can support older and newer versions of diffusers
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/softmax_context.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/softmax_context.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/softmax_context.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/vector_matmul.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/vector_matmul.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/vector_matmul.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/linear.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/linear.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/linear.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/gelu_gemm.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/gelu_gemm.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/gelu_gemm.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/qkv_gemm.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/qkv_gemm.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/qkv_gemm.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/mlp_gemm.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/mlp_gemm.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/mlp_gemm.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/softmax.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/softmax.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/softmax.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/softmax.py:                 expand atten_mask from two dim into 4 dim, insert two dims in the middle
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/base.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/base.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/base.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/residual_add.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/residual_add.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/residual_add.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/residual_add.py:                     only use residual add if its set and we are not pre layer norm
./DeepSpeed/deepspeed/ops/transformer/inference/op_binding/residual_add.py:             fallback
./DeepSpeed/deepspeed/ops/transformer/inference/moe_inference.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/transformer/inference/moe_inference.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/transformer/inference/moe_inference.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/transformer/inference/moe_inference.py: accelerator modules will be imported if needed
./DeepSpeed/deepspeed/ops/transformer/inference/moe_inference.py:         used for quantization
./DeepSpeed/deepspeed/ops/transformer/inference/moe_inference.py:             InferenceSpecializedBuilder is not among DeepSpeed provided builder yet, so we infer by builder name string
./DeepSpeed/deepspeed/ops/transformer/inference/moe_inference.py:            ############# MoE Gating + Experts ###############
./DeepSpeed/deepspeed/ops/transformer/inference/moe_inference.py:            ###############################################
./DeepSpeed/deepspeed/ops/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/__init__.py:from ..git_version_info_installed import installed_ops as __installed_ops__
./DeepSpeed/deepspeed/ops/__init__.py:if __installed_ops__['sparse_attn']:
./DeepSpeed/deepspeed/ops/aio/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/aio/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/aio/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/adagrad/cpu_adagrad.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/adagrad/cpu_adagrad.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/adagrad/cpu_adagrad.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/adagrad/cpu_adagrad.py:         need to destroy the C++ object explicitly to avoid a memory leak when deepspeed.initialize
./DeepSpeed/deepspeed/ops/adagrad/cpu_adagrad.py:         is used multiple times in the same process (notebook or pytest worker)
./DeepSpeed/deepspeed/ops/adagrad/cpu_adagrad.py:         intended device for step
./DeepSpeed/deepspeed/ops/adagrad/cpu_adagrad.py:                 State initialization
./DeepSpeed/deepspeed/ops/adagrad/cpu_adagrad.py:                    print(f'group {group_id} param {param_id} = {p.numel()}')
./DeepSpeed/deepspeed/ops/adagrad/cpu_adagrad.py:                    use full precision by default unless self.fp32_optimizer_states is off
./DeepSpeed/deepspeed/ops/adagrad/cpu_adagrad.py:                    memory_format=torch.preserve_format)
./DeepSpeed/deepspeed/ops/adagrad/cpu_adagrad.py:                     gradient variances
./DeepSpeed/deepspeed/ops/adagrad/cpu_adagrad.py:                    memory_format=torch.preserve_format)
./DeepSpeed/deepspeed/ops/adagrad/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/adagrad/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/adagrad/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/quantizer/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/quantizer/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/quantizer/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/quantizer/quantizer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/quantizer/quantizer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/quantizer/quantizer.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/quantizer/quantizer.py: Cuda modules will be imported if needed
./DeepSpeed/deepspeed/ops/quantizer/quantizer.py:     Load cuda modules if needed
./DeepSpeed/deepspeed/ops/adam/fused_adam.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/adam/fused_adam.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/adam/fused_adam.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/adam/fused_adam.py:         Skip buffer
./DeepSpeed/deepspeed/ops/adam/fused_adam.py:             assume same step across group now to simplify things
./DeepSpeed/deepspeed/ops/adam/fused_adam.py:             per parameter step can be easily support by making it tensor, or pass list into kernel
./DeepSpeed/deepspeed/ops/adam/fused_adam.py:             create lists for multi-tensor apply
./DeepSpeed/deepspeed/ops/adam/fused_adam.py:                 State initialization
./DeepSpeed/deepspeed/ops/adam/fused_adam.py:                     DeepSpeed ZeRO 3 processes each subgroup a time, so we need to keep tracking step count for each tensor separately.
./DeepSpeed/deepspeed/ops/adam/fused_adam.py:                     While this is not an issue for ZeRO 1 & 2, since they apply a single optimization step to the whole param group at the same time.
./DeepSpeed/deepspeed/ops/adam/fused_adam.py:                     In order to keep backward compatibility for the existing checkpoints, we use group['state'] to initialize state['step'] if it exists.
./DeepSpeed/deepspeed/ops/adam/fused_adam.py:                     Exponential moving average of gradient values
./DeepSpeed/deepspeed/ops/adam/fused_adam.py:                     Exponential moving average of squared gradient values
./DeepSpeed/deepspeed/ops/adam/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/adam/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/adam/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/adam/multi_tensor_apply.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/adam/multi_tensor_apply.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/adam/multi_tensor_apply.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/adam/cpu_adam.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/adam/cpu_adam.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/adam/cpu_adam.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/adam/cpu_adam.py:         need to destroy the C++ object explicitly to avoid a memory leak when deepspeed.initialize
./DeepSpeed/deepspeed/ops/adam/cpu_adam.py:         is used multiple times in the same process (notebook or pytest worker)
./DeepSpeed/deepspeed/ops/adam/cpu_adam.py:         intended device for step
./DeepSpeed/deepspeed/ops/adam/cpu_adam.py:         converting the fp16 params to a group of parameter
./DeepSpeed/deepspeed/ops/adam/cpu_adam.py:                 State initialization
./DeepSpeed/deepspeed/ops/adam/cpu_adam.py:                    print(f'group {group_id} param {param_id} = {p.numel()}')
./DeepSpeed/deepspeed/ops/adam/cpu_adam.py:                    use full precision by default unless self.fp32_optimizer_states is off
./DeepSpeed/deepspeed/ops/adam/cpu_adam.py:                     gradient momentums
./DeepSpeed/deepspeed/ops/adam/cpu_adam.py:                    memory_format=torch.preserve_format)
./DeepSpeed/deepspeed/ops/adam/cpu_adam.py:                     gradient variances
./DeepSpeed/deepspeed/ops/adam/cpu_adam.py:                    memory_format=torch.preserve_format)
./DeepSpeed/deepspeed/ops/lion/cpu_lion.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/lion/cpu_lion.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/lion/cpu_lion.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/lion/cpu_lion.py:         need to destroy the C++ object explicitly to avoid a memory leak when deepspeed.initialize
./DeepSpeed/deepspeed/ops/lion/cpu_lion.py:         is used multiple times in the same process (notebook or pytest worker)
./DeepSpeed/deepspeed/ops/lion/cpu_lion.py:         intended device for step
./DeepSpeed/deepspeed/ops/lion/cpu_lion.py:         converting the fp16 params to a group of parameter
./DeepSpeed/deepspeed/ops/lion/cpu_lion.py:                 State initialization
./DeepSpeed/deepspeed/ops/lion/cpu_lion.py:                    print(f'group {group_id} param {param_id} = {p.numel()}')
./DeepSpeed/deepspeed/ops/lion/cpu_lion.py:                    use full precision by default unless self.fp32_optimizer_states is off
./DeepSpeed/deepspeed/ops/lion/cpu_lion.py:                     gradient momentums
./DeepSpeed/deepspeed/ops/lion/cpu_lion.py:                    memory_format=torch.preserve_format)
./DeepSpeed/deepspeed/ops/lion/cpu_lion.py:                     gradient variances
./DeepSpeed/deepspeed/ops/lion/cpu_lion.py:                    memory_format=torch.preserve_format)
./DeepSpeed/deepspeed/ops/lion/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/lion/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/lion/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/lion/multi_tensor_apply.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/lion/multi_tensor_apply.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/lion/multi_tensor_apply.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/lion/fused_lion.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/ops/lion/fused_lion.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/ops/lion/fused_lion.py: DeepSpeed Team
./DeepSpeed/deepspeed/ops/lion/fused_lion.py:         Skip buffer
./DeepSpeed/deepspeed/ops/lion/fused_lion.py:             assume same step across group now to simplify things
./DeepSpeed/deepspeed/ops/lion/fused_lion.py:             per parameter step can be easily support by making it tensor, or pass list into kernel
./DeepSpeed/deepspeed/ops/lion/fused_lion.py:             create lists for multi-tensor apply
./DeepSpeed/deepspeed/ops/lion/fused_lion.py:                 State initialization
./DeepSpeed/deepspeed/ops/lion/fused_lion.py:                     DeepSpeed ZeRO 3 processes each subgroup a time, so we need to keep tracking step count for each tensor separately.
./DeepSpeed/deepspeed/ops/lion/fused_lion.py:                     While this is not an issue for ZeRO 1 & 2, since they apply a single optimization step to the whole param group at the same time.
./DeepSpeed/deepspeed/ops/lion/fused_lion.py:                     In order to keep backward compatibility for the existing checkpoints, we use group['state'] to initialize state['step'] if it exists.
./DeepSpeed/deepspeed/ops/lion/fused_lion.py:                     Exponential moving average of gradient values
./DeepSpeed/deepspeed/sequence/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/sequence/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/sequence/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/sequence/layer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/sequence/layer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/sequence/layer.py: DeepSpeed Team
./DeepSpeed/deepspeed/sequence/layer.py:         transpose groups of heads with the seq-len parallel dimension, so that we can scatter them!
./DeepSpeed/deepspeed/sequence/layer.py:     if scattering the seq-dim, transpose the heads back to the original dimension
./DeepSpeed/deepspeed/sequence/layer.py:         TODO Merge three alltoall calls into one
./DeepSpeed/deepspeed/sequence/layer.py:         TODO (Reza): change the api on the megatron-deepspeed side so that we only receive all data (q,k, and v) together!
./DeepSpeed/deepspeed/sequence/layer.py:        in shape : e.g.,  [s/p:h:]
./DeepSpeed/deepspeed/sequence/layer.py:        out shape : e.g., [s:h/p:]
./DeepSpeed/deepspeed/sequence/layer.py:        out e.g., [s/p::h]
./DeepSpeed/deepspeed/autotuning/tuner/base_tuner.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/autotuning/tuner/base_tuner.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/autotuning/tuner/base_tuner.py: DeepSpeed Team
./DeepSpeed/deepspeed/autotuning/tuner/base_tuner.py:                 Select the next batch of configuration for evaluation
./DeepSpeed/deepspeed/autotuning/tuner/base_tuner.py:                 Generate experiments for measurement of performance
./DeepSpeed/deepspeed/autotuning/tuner/base_tuner.py:                     logger.info(f"tuner finds better = {exp}")
./DeepSpeed/deepspeed/autotuning/tuner/base_tuner.py:                 Update the tuner with evaluated performance results
./DeepSpeed/deepspeed/autotuning/tuner/base_tuner.py:                 Early stop if no more promising configurations are likely to be found
./DeepSpeed/deepspeed/autotuning/tuner/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/autotuning/tuner/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/autotuning/tuner/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/autotuning/tuner/__init__.py: from .ga_tuner import GATuner
./DeepSpeed/deepspeed/autotuning/tuner/model_based_tuner.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/autotuning/tuner/model_based_tuner.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/autotuning/tuner/model_based_tuner.py: DeepSpeed Team
./DeepSpeed/deepspeed/autotuning/tuner/model_based_tuner.py:         print(configs)
./DeepSpeed/deepspeed/autotuning/tuner/model_based_tuner.py:         TODO the current implementation requires that all configs have the same shape.
./DeepSpeed/deepspeed/autotuning/tuner/model_based_tuner.py:         top_configs = [self.all_configs[i] for i in top_idx]
./DeepSpeed/deepspeed/autotuning/tuner/model_based_tuner.py:                 Select top promising trials
./DeepSpeed/deepspeed/autotuning/tuner/model_based_tuner.py:             To avoid over-exploitation, randomly select one that has not been explored.
./DeepSpeed/deepspeed/autotuning/tuner/model_based_tuner.py:                 Do normal selection
./DeepSpeed/deepspeed/autotuning/tuner/model_based_tuner.py:             Need to track both the sampled configs and indices
./DeepSpeed/deepspeed/autotuning/tuner/utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/autotuning/tuner/utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/autotuning/tuner/utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/autotuning/tuner/utils.py:     normalization, should not matter in tree models
./DeepSpeed/deepspeed/autotuning/tuner/cost_model.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/autotuning/tuner/cost_model.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/autotuning/tuner/cost_model.py: DeepSpeed Team
./DeepSpeed/deepspeed/autotuning/tuner/index_based_tuner.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/autotuning/tuner/index_based_tuner.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/autotuning/tuner/index_based_tuner.py: DeepSpeed Team
./DeepSpeed/deepspeed/autotuning/config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/autotuning/config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/autotuning/config.py: DeepSpeed Team
./DeepSpeed/deepspeed/autotuning/constants.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/autotuning/constants.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/autotuning/constants.py: DeepSpeed Team
./DeepSpeed/deepspeed/autotuning/constants.py:########################################
./DeepSpeed/deepspeed/autotuning/constants.py: autotuner implementation constants
./DeepSpeed/deepspeed/autotuning/constants.py:########################################
./DeepSpeed/deepspeed/autotuning/constants.py:########################################
./DeepSpeed/deepspeed/autotuning/constants.py: autotuner configuration constants
./DeepSpeed/deepspeed/autotuning/constants.py:########################################
./DeepSpeed/deepspeed/autotuning/constants.py: Autotuner. By default, this feature is not enabled.
./DeepSpeed/deepspeed/autotuning/constants.py: Users can configure in ds_config.json as below example:
./DeepSpeed/deepspeed/autotuning/constants.py:########################################
./DeepSpeed/deepspeed/autotuning/constants.py: MODEL INFO
./DeepSpeed/deepspeed/autotuning/constants.py:########################################
./DeepSpeed/deepspeed/autotuning/constants.py:########################################
./DeepSpeed/deepspeed/autotuning/constants.py: autotuner search space constants
./DeepSpeed/deepspeed/autotuning/constants.py:########################################
./DeepSpeed/deepspeed/autotuning/constants.py: TUNING_MICRO_BATCH_SIZE_PREFIX="tune_micro_batch_size_z"
./DeepSpeed/deepspeed/autotuning/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/autotuning/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/autotuning/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/autotuning/utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/autotuning/utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/autotuning/utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/autotuning/utils.py:     e.g., worker-0 slots=16
./DeepSpeed/deepspeed/autotuning/utils.py:                 skip empty lines
./DeepSpeed/deepspeed/autotuning/utils.py:     HF requires that "ZeRO Offload can only work with DeepSpeed optimizers"
./DeepSpeed/deepspeed/autotuning/utils.py:             skip the arg_mappings section when naming the exp file
./DeepSpeed/deepspeed/autotuning/utils.py:             recursively call the func to get name for the child dicts
./DeepSpeed/deepspeed/autotuning/utils.py:         write the expr config to a json file
./DeepSpeed/deepspeed/autotuning/scheduler.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/autotuning/scheduler.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/autotuning/scheduler.py: DeepSpeed Team
./DeepSpeed/deepspeed/autotuning/scheduler.py:                     skip existing experiments (except for the ones that were interrupted)
./DeepSpeed/deepspeed/autotuning/scheduler.py:         overwrite the user arg in the arg_mappings
./DeepSpeed/deepspeed/autotuning/scheduler.py:             request satisfied
./DeepSpeed/deepspeed/autotuning/scheduler.py:             request not satisfied
./DeepSpeed/deepspeed/autotuning/scheduler.py:         All pending experiments are scheduled, waiting for them to complete
./DeepSpeed/deepspeed/autotuning/scheduler.py:         clean up the running experiments
./DeepSpeed/deepspeed/autotuning/scheduler.py:     Infrastructure-specific job-id
./DeepSpeed/deepspeed/autotuning/scheduler.py:     remove "--deepspeed_config ds_config.json" from user_args
./DeepSpeed/deepspeed/autotuning/scheduler.py:         "--deepspeed_config" is omitted in HF
./DeepSpeed/deepspeed/autotuning/scheduler.py:         user_args[idx + 1] = exp["ds_config_path"]
./DeepSpeed/deepspeed/autotuning/scheduler.py:         pass base64 serialized ds_config to launcher
./DeepSpeed/deepspeed/autotuning/scheduler.py:     PDSH flags for max node fan out and specific hosts to launch on
./DeepSpeed/deepspeed/autotuning/scheduler.py:     See https://linux.die.net/man/1/pdsh for flag details
./DeepSpeed/deepspeed/autotuning/scheduler.py:     In case of failure must propagate the error-condition back to the caller (usually shell). The
./DeepSpeed/deepspeed/autotuning/scheduler.py:     actual error and traceback should have been printed in the subprocess, so in order to avoid
./DeepSpeed/deepspeed/autotuning/scheduler.py:     unnecessary noise we just quietly exit here with the same code as the subprocess
./DeepSpeed/deepspeed/autotuning/autotuner.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/autotuning/autotuner.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/autotuning/autotuner.py: DeepSpeed Team
./DeepSpeed/deepspeed/autotuning/autotuner.py:         set the active resource for the autotuner resource manager
./DeepSpeed/deepspeed/autotuning/autotuner.py:         get resource requirement for each autotuning experiment
./DeepSpeed/deepspeed/autotuning/autotuner.py:         assume the model uses Adam optimizer
./DeepSpeed/deepspeed/autotuning/autotuner.py:         ZeroStageEnum.disabled:
./DeepSpeed/deepspeed/autotuning/autotuner.py:         each zero stage uses a different template configuration file
./DeepSpeed/deepspeed/autotuning/autotuner.py:         replace the corresponding parameter values if the user specifies them in the DeepSpeed configuration file
./DeepSpeed/deepspeed/autotuning/autotuner.py:             fill the template with the expr config
./DeepSpeed/deepspeed/autotuning/autotuner.py:             if the config does not use offloading, remove the offloading section
./DeepSpeed/deepspeed/autotuning/autotuner.py:             set gradient accumulation steps according to max_train_batch_size_per_gpu
./DeepSpeed/deepspeed/autotuning/autotuner.py:             generate the expr name
./DeepSpeed/deepspeed/autotuning/autotuner.py:         model info profile run with DEFAULT_MIN_MEM_CONFIG
./DeepSpeed/deepspeed/autotuning/autotuner.py:         calculate max micro batch size using gpu memory, model instantiation memory and activation memory
./DeepSpeed/deepspeed/autotuning/autotuner.py:         calculated_max_micro_batch_size = (memory_per_gpu - instantiation_memory) // activation_memory_micro_batch_size_1
./DeepSpeed/deepspeed/autotuning/autotuner.py:             user-specified micro batch size per gpu is a list which overwrites the default tuning behavior
./DeepSpeed/deepspeed/autotuning/autotuner.py:             auto-detects the list of micro batch sizes to tune
./DeepSpeed/deepspeed/autotuning/autotuner.py:         return if the tuning_micro_batch_sizes list is empty
./DeepSpeed/deepspeed/autotuning/autotuner.py:         tune micro batch sizes and gradient accumulation steps given max_train_batch_size_per_gpu
./DeepSpeed/deepspeed/autotuning/autotuner.py:         if the best metric or the micro batch size for that best metric in the current Zero stage after tuning micro batch size is less than the corresponding value in the previous Zero stage, return, do not tune other Zero configuration parameters
./DeepSpeed/deepspeed/autotuning/autotuner.py:         in a auto-detected tuning_micro_batch_sizes list, max_micro_batch_size might not be performant as the memory consumption is close to max
./DeepSpeed/deepspeed/autotuning/autotuner.py:         try smaller values while gas stays the same
./DeepSpeed/deepspeed/autotuning/autotuner.py:         if finding a more performant mbs value, use it to replace max_micro_batch_size in the list
./DeepSpeed/deepspeed/autotuning/autotuner.py:         get min and max micro batch size with gradient accumulation steps = 1
./DeepSpeed/deepspeed/autotuning/autotuner.py:         search for the min micro batch size
./DeepSpeed/deepspeed/autotuning/autotuner.py:                 user specifies train_micro_batch_size_per_gpu as an int
./DeepSpeed/deepspeed/autotuning/autotuner.py:                 user does not specify train_micro_batch_size_per_gpu or sets it to "auto" when using Hugging Face
./DeepSpeed/deepspeed/autotuning/autotuner.py:         search for the max micro batch size
./DeepSpeed/deepspeed/autotuning/autotuner.py:         binary search until low is the smallest micro batch size that OOMs.
./DeepSpeed/deepspeed/autotuning/autotuner.py:         NUM_GPUS=$(( ${NUM_WORKERS} * ${NUM_GPUS_PER_WORKER} ))
./DeepSpeed/deepspeed/autotuning/autotuner.py:         DP_SIZE=$(( ${NUM_GPUS} / (${PP_SIZE} * ${MP_SIZE}) ))
./DeepSpeed/deepspeed/autotuning/autotuner.py:         GRAD_ACC_STEPS=$(( ${TARGET_GLOBAL_BATCH_SIZE} / (${BATCH_SIZE} * ${DP_SIZE}) ))
./DeepSpeed/deepspeed/autotuning/autotuner.py:         constant stride
./DeepSpeed/deepspeed/autotuning/autotuner.py:         if gas is the same as min_gas, do not add mbs to the tuning list
./DeepSpeed/deepspeed/moe/sharded_moe.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/moe/sharded_moe.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/moe/sharded_moe.py: DeepSpeed Team
./DeepSpeed/deepspeed/moe/sharded_moe.py: Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
./DeepSpeed/deepspeed/moe/sharded_moe.py:
./DeepSpeed/deepspeed/moe/sharded_moe.py: This source code is licensed under the BSD license found in the
./DeepSpeed/deepspeed/moe/sharded_moe.py: LICENSE file in the root directory of this source tree.
./DeepSpeed/deepspeed/moe/sharded_moe.py:     To enable Tutel MoE optimizations:
./DeepSpeed/deepspeed/moe/sharded_moe.py:       python3 -m pip install --user --upgrade git+https://github.com/microsoft/tutel@v0.1.x
./DeepSpeed/deepspeed/moe/sharded_moe.py:     Fail silently so we don't spam logs unnecessarily if user isn't using tutel
./DeepSpeed/deepspeed/moe/sharded_moe.py: einsum dimensions: (g)roup, (s)equence, (e)xpert, (m)odel, (c)apacity
./DeepSpeed/deepspeed/moe/sharded_moe.py: See https://arxiv.org/pdf/2006.16668.pdf for details.
./DeepSpeed/deepspeed/moe/sharded_moe.py: Based on https://github.com/pytorch/pytorch/pull/40762
./DeepSpeed/deepspeed/moe/sharded_moe.py:             TODO: replace with DS process group
./DeepSpeed/deepspeed/moe/sharded_moe.py: einsum rewrites are on par or more performant
./DeepSpeed/deepspeed/moe/sharded_moe.py: switch can be bubbled up in future
./DeepSpeed/deepspeed/moe/sharded_moe.py: einsum dimensions: (g)roup, (s)equence, (e)xpert, (m)odel, (c)apacity
./DeepSpeed/deepspeed/moe/sharded_moe.py: See https://arxiv.org/pdf/2006.16668.pdf for details.
./DeepSpeed/deepspeed/moe/sharded_moe.py:         [k, s] -> [s, k] -> [s, 1, k]
./DeepSpeed/deepspeed/moe/sharded_moe.py:         [k,s,m] -> [k, sm] -> [sm, k] -> [s, m, k]
./DeepSpeed/deepspeed/moe/sharded_moe.py:         bmm([s, 1, k], [s, m, k]^t) -> [s, m, 1]
./DeepSpeed/deepspeed/moe/sharded_moe.py: The following functions are extracted and scripted
./DeepSpeed/deepspeed/moe/sharded_moe.py: because otherwise during a torch.jit.trace, the non-Tensor
./DeepSpeed/deepspeed/moe/sharded_moe.py: values used in the calculations get recorded as constants.
./DeepSpeed/deepspeed/moe/sharded_moe.py: torch.jit.script coerces them into Tensors and preserves
./DeepSpeed/deepspeed/moe/sharded_moe.py: their dynamic shapes. This enables ONNX export.
./DeepSpeed/deepspeed/moe/sharded_moe.py: We can't script the entire top1gating function because it
./DeepSpeed/deepspeed/moe/sharded_moe.py: includes stateful caching logic which is incompatible with ONNX.
./DeepSpeed/deepspeed/moe/sharded_moe.py:     gates has shape of SE
./DeepSpeed/deepspeed/moe/sharded_moe.py:     to(torch.int64) works around a bug in torch.onnx.export:
./DeepSpeed/deepspeed/moe/sharded_moe.py:     it should cast k to int64 when converting torch.topk but it doesn't.
./DeepSpeed/deepspeed/moe/sharded_moe.py:     everything is in fp32 in this function
./DeepSpeed/deepspeed/moe/sharded_moe.py:     Create a mask for 1st's expert per token
./DeepSpeed/deepspeed/moe/sharded_moe.py:     noisy gating
./DeepSpeed/deepspeed/moe/sharded_moe.py:     mask only used tokens
./DeepSpeed/deepspeed/moe/sharded_moe.py:     gating decisions
./DeepSpeed/deepspeed/moe/sharded_moe.py:     if we don't want to drop any tokens
./DeepSpeed/deepspeed/moe/sharded_moe.py:     Compute l_aux
./DeepSpeed/deepspeed/moe/sharded_moe.py:     Random Token Selection
./DeepSpeed/deepspeed/moe/sharded_moe.py:         Tutel doesn't support index values masked with zero
./DeepSpeed/deepspeed/moe/sharded_moe.py:         so we need to replace masked indices with -1
./DeepSpeed/deepspeed/moe/sharded_moe.py:     Compute locations in capacity buffer
./DeepSpeed/deepspeed/moe/sharded_moe.py:     Store the capacity location for each token
./DeepSpeed/deepspeed/moe/sharded_moe.py:     Normalize gate probabilities
./DeepSpeed/deepspeed/moe/sharded_moe.py:     everything is in fp32 in this function
./DeepSpeed/deepspeed/moe/sharded_moe.py:     Create a mask for 1st's expert per token
./DeepSpeed/deepspeed/moe/sharded_moe.py:     Create a mask for 2nd's expert per token using Gumbel-max trick
./DeepSpeed/deepspeed/moe/sharded_moe.py:     https://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/
./DeepSpeed/deepspeed/moe/sharded_moe.py:     Replace top-expert with min value
./DeepSpeed/deepspeed/moe/sharded_moe.py:     Compute locations in capacity buffer
./DeepSpeed/deepspeed/moe/sharded_moe.py:     Update 2nd's location by accounting for locations of 1st
./DeepSpeed/deepspeed/moe/sharded_moe.py:     gating decisions
./DeepSpeed/deepspeed/moe/sharded_moe.py:     Compute l_aux
./DeepSpeed/deepspeed/moe/sharded_moe.py:     Remove locations outside capacity from mask
./DeepSpeed/deepspeed/moe/sharded_moe.py:     Store the capacity location for each token
./DeepSpeed/deepspeed/moe/sharded_moe.py:     Normalize gate probabilities
./DeepSpeed/deepspeed/moe/sharded_moe.py:     Avoid divide-by-zero
./DeepSpeed/deepspeed/moe/sharded_moe.py:     Calculate combine_weights and dispatch_mask
./DeepSpeed/deepspeed/moe/sharded_moe.py:         Only top-1 and top-2 are supported at the moment.
./DeepSpeed/deepspeed/moe/sharded_moe.py:         input jittering
./DeepSpeed/deepspeed/moe/sharded_moe.py:         Implement Algorithm 2 from GShard paper.
./DeepSpeed/deepspeed/moe/sharded_moe.py:         Initial implementation -> Reshape into S tokens by dropping sequence dimension.
./DeepSpeed/deepspeed/moe/sharded_moe.py:         Reshape into G groups so that each group can distribute tokens equally
./DeepSpeed/deepspeed/moe/sharded_moe.py:         group_size = kwargs['group_size'] if 'group_size' in kwargs.keys() else 1
./DeepSpeed/deepspeed/moe/sharded_moe.py:             If the non-expert is tensor-parallel, it will create
./DeepSpeed/deepspeed/moe/sharded_moe.py:             duplicate tokens on the tensor-parallel ranks.
./DeepSpeed/deepspeed/moe/sharded_moe.py:             Since our experts are not tensor-parallel, these duplicates
./DeepSpeed/deepspeed/moe/sharded_moe.py:             need to be dropped to ensure correctness.
./DeepSpeed/deepspeed/moe/sharded_moe.py:             this also doubles up as a communication optimization as we are
./DeepSpeed/deepspeed/moe/sharded_moe.py:             reducing the all-to-all communication volume.
./DeepSpeed/deepspeed/moe/sharded_moe.py:         Re-shape after all-to-all: ecm -> gecm
./DeepSpeed/deepspeed/moe/sharded_moe.py:         Re-shape back: gecm -> ecm
./DeepSpeed/deepspeed/moe/sharded_moe.py:             the dropped duplicate tokens need to be gathered on each
./DeepSpeed/deepspeed/moe/sharded_moe.py:             tensor parallel rank again for the tensor-parallel
./DeepSpeed/deepspeed/moe/sharded_moe.py:             non-expert of the next layer.
./DeepSpeed/deepspeed/moe/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/moe/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/moe/__init__.py: DeepSpeed Team
./DeepSpeed/deepspeed/moe/utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/moe/utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/moe/utils.py: DeepSpeed Team
./DeepSpeed/deepspeed/moe/utils.py:     gather all data parallel group names
./DeepSpeed/deepspeed/moe/utils.py:     Create the param MoE groups, leave param assign to next step
./DeepSpeed/deepspeed/moe/utils.py:     Assign param
./DeepSpeed/deepspeed/moe/utils.py:                 param_group['params'].remove(param)
./DeepSpeed/deepspeed/moe/utils.py:     Flatten the moe groups
./DeepSpeed/deepspeed/moe/layer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/moe/layer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/moe/layer.py: DeepSpeed Team
./DeepSpeed/deepspeed/moe/layer.py:             coefficient is used for weighted sum of the output of expert and mlp
./DeepSpeed/deepspeed/moe/layer.py:         Create process group for a layer if needed
./DeepSpeed/deepspeed/moe/layer.py:                 Condition 1 - no groups.mpu means no tensor parallelism
./DeepSpeed/deepspeed/moe/layer.py:                 Condition 2 - disabling expert tensor parallelism on purpose
./DeepSpeed/deepspeed/moe/layer.py:                 expert tensor parallelism is enabled
./DeepSpeed/deepspeed/moe/layer.py:         Set the group handle for the MOELayer (deepspeed_moe) object
./DeepSpeed/deepspeed/moe/layer.py:             Residual MoE
./DeepSpeed/deepspeed/moe/experts.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/moe/experts.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/moe/experts.py: DeepSpeed Team
./DeepSpeed/deepspeed/moe/experts.py:         TODO: revisit allreduce for moe.gate...
./DeepSpeed/deepspeed/moe/experts.py:             TODO: Create param groups to handle expert + data case (e.g. param.group = moe_group)
./DeepSpeed/deepspeed/moe/mappings.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/deepspeed/moe/mappings.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/deepspeed/moe/mappings.py: DeepSpeed Team
./DeepSpeed/deepspeed/moe/mappings.py: The file has been adapted from the following Megatron-LM file:
./DeepSpeed/deepspeed/moe/mappings.py: https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/mpu/mappings.py
./DeepSpeed/deepspeed/moe/mappings.py: Git commit hash: 9dc3c42a84aa656f583703cf8b6b4f79f712b796
./DeepSpeed/deepspeed/moe/mappings.py: We retain the following copyright from the original files:
./DeepSpeed/deepspeed/moe/mappings.py: Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
./DeepSpeed/deepspeed/moe/mappings.py: Licensed under the Apache License, Version 2.0 (the "License");
./DeepSpeed/deepspeed/moe/mappings.py: you may not use this file except in compliance with the License.
./DeepSpeed/deepspeed/moe/mappings.py: You may obtain a copy of the License at
./DeepSpeed/deepspeed/moe/mappings.py:
./DeepSpeed/deepspeed/moe/mappings.py:     http://www.apache.org/licenses/LICENSE-2.0
./DeepSpeed/deepspeed/moe/mappings.py:
./DeepSpeed/deepspeed/moe/mappings.py: Unless required by applicable law or agreed to in writing, software
./DeepSpeed/deepspeed/moe/mappings.py: distributed under the License is distributed on an "AS IS" BASIS,
./DeepSpeed/deepspeed/moe/mappings.py: WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
./DeepSpeed/deepspeed/moe/mappings.py: See the License for the specific language governing permissions and
./DeepSpeed/deepspeed/moe/mappings.py: limitations under the License.
./DeepSpeed/deepspeed/moe/mappings.py:     Size and dimension.
./DeepSpeed/deepspeed/moe/mappings.py:     Note: torch.cat already creates a contiguous tensor.
./DeepSpeed/deepspeed/moe/mappings.py:         no tensor parallelism for non-experts
./DeepSpeed/deepspeed/moe/mappings.py:         no tensor parallelism for non-experts
./DeepSpeed/op_builder/inference_cutlass_builder.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/inference_cutlass_builder.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/inference_cutlass_builder.py: DeepSpeed Team
./DeepSpeed/op_builder/inference_cutlass_builder.py:                 Only support Ampere and newer
./DeepSpeed/op_builder/cpu_lion.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/cpu_lion.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/cpu_lion.py: DeepSpeed Team
./DeepSpeed/op_builder/ragged_utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/ragged_utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/ragged_utils.py: DeepSpeed Team
./DeepSpeed/op_builder/fused_adam.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/fused_adam.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/fused_adam.py: DeepSpeed Team
./DeepSpeed/op_builder/cpu_adagrad.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/cpu_adagrad.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/cpu_adagrad.py: DeepSpeed Team
./DeepSpeed/op_builder/cpu/no_impl.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/cpu/no_impl.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/cpu/no_impl.py: DeepSpeed Team
./DeepSpeed/op_builder/cpu/fused_adam.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/cpu/fused_adam.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/cpu/fused_adam.py: DeepSpeed Team
./DeepSpeed/op_builder/cpu/comm.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/cpu/comm.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/cpu/comm.py: DeepSpeed Team
./DeepSpeed/op_builder/cpu/comm.py:         TODO: add soft compatibility check for private binary release.
./DeepSpeed/op_builder/cpu/comm.py:          a soft check, as in we know it can be trivially changed.
./DeepSpeed/op_builder/cpu/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/cpu/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/cpu/__init__.py: DeepSpeed Team
./DeepSpeed/op_builder/cpu/builder.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/cpu/builder.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/cpu/builder.py: DeepSpeed Team
./DeepSpeed/op_builder/cpu/builder.py:     is op_builder from deepspeed or a 3p version? this should only succeed if it's deepspeed
./DeepSpeed/op_builder/cpu/builder.py:     if successful this also means we're doing a local install and not JIT compile path
./DeepSpeed/op_builder/cpu/cpu_adam.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/cpu/cpu_adam.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/cpu/cpu_adam.py: DeepSpeed Team
./DeepSpeed/op_builder/transformer_inference.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/transformer_inference.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/transformer_inference.py: DeepSpeed Team
./DeepSpeed/op_builder/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/__init__.py: DeepSpeed Team
./DeepSpeed/op_builder/__init__.py: Do not remove, required for abstract accelerator to detect if we have a deepspeed or 3p op_builder
./DeepSpeed/op_builder/__init__.py: List of all available op builders from deepspeed op_builder
./DeepSpeed/op_builder/__init__.py:         during installation time cannot get builder due to torch not installed,
./DeepSpeed/op_builder/__init__.py:         return closure instead
./DeepSpeed/op_builder/__init__.py:         during runtime, return op builder class directly
./DeepSpeed/op_builder/__init__.py: reflect builder names and add builder closure, such as 'TransformerBuilder()' creates op builder wrt current accelerator
./DeepSpeed/op_builder/__init__.py:                 assign builder name to variable with same name
./DeepSpeed/op_builder/__init__.py:                 the following is equivalent to i.e. TransformerBuilder = "TransformerBuilder"
./DeepSpeed/op_builder/ragged_ops.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/ragged_ops.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/ragged_ops.py: DeepSpeed Team
./DeepSpeed/op_builder/ragged_ops.py:                 Blocked flash has a dependency on Ampere + newer
./DeepSpeed/op_builder/npu/no_impl.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/npu/no_impl.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/npu/no_impl.py: DeepSpeed Team
./DeepSpeed/op_builder/npu/cpu_lion.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/npu/cpu_lion.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/npu/cpu_lion.py: DeepSpeed Team
./DeepSpeed/op_builder/npu/fused_adam.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/npu/fused_adam.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/npu/fused_adam.py: DeepSpeed Team
./DeepSpeed/op_builder/npu/fused_adam.py:         iteration group['params']
./DeepSpeed/op_builder/npu/cpu_adagrad.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/npu/cpu_adagrad.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/npu/cpu_adagrad.py: DeepSpeed Team
./DeepSpeed/op_builder/npu/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/npu/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/npu/__init__.py: DeepSpeed Team
./DeepSpeed/op_builder/npu/builder.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/npu/builder.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/npu/builder.py: DeepSpeed Team
./DeepSpeed/op_builder/npu/builder.py:     is op_builder from deepspeed or a 3p version? this should only succeed if it's deepspeed
./DeepSpeed/op_builder/npu/builder.py:     if successful this also means we're doing a local install and not JIT compile path
./DeepSpeed/op_builder/npu/cpu_adam.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/npu/cpu_adam.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/npu/cpu_adam.py: DeepSpeed Team
./DeepSpeed/op_builder/quantizer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/quantizer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/quantizer.py: DeepSpeed Team
./DeepSpeed/op_builder/sparse_attn.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/sparse_attn.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/sparse_attn.py: DeepSpeed Team
./DeepSpeed/op_builder/sparse_attn.py:         Check to see if llvm and cmake are installed since they are dependencies
./DeepSpeed/op_builder/sparse_attn.py:        required_commands = ['llvm-config|llvm-config-9', 'cmake']
./DeepSpeed/op_builder/sparse_attn.py:        command_status = list(map(self.command_exists, required_commands))
./DeepSpeed/op_builder/sparse_attn.py:        deps_compatible = all(command_status)
./DeepSpeed/op_builder/sparse_attn.py:         torch-cpu will not have a cuda version
./DeepSpeed/op_builder/sparse_attn.py:             auto-install of triton is broken on some systems, reverting to manual install for now
./DeepSpeed/op_builder/sparse_attn.py:             see this issue: https://github.com/microsoft/DeepSpeed/issues/1710
./DeepSpeed/op_builder/random_ltd.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/random_ltd.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/random_ltd.py: DeepSpeed Team
./DeepSpeed/op_builder/builder.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/builder.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/builder.py: DeepSpeed Team
./DeepSpeed/op_builder/builder.py:     Ensure there is not a cuda version mismatch between torch and nvcc compiler
./DeepSpeed/op_builder/builder.py:     Ignore patch versions, only look at major + minor
./DeepSpeed/op_builder/builder.py:             Special treatment of CUDA 11.0 because compute_86 is not supported.
./DeepSpeed/op_builder/builder.py: list compatible minor CUDA versions - so that for example pytorch built with cuda-11.0 can be used
./DeepSpeed/op_builder/builder.py: to build deepspeed and system-wide installed cuda 11.2
./DeepSpeed/op_builder/builder.py:     This is a show-stopping error, should probably not proceed past this
./DeepSpeed/op_builder/builder.py:             Echo compile and link commands that are used.
./DeepSpeed/op_builder/builder.py:             Create a compiler object.
./DeepSpeed/op_builder/builder.py:             Configure compiler and linker to build according to Python install.
./DeepSpeed/op_builder/builder.py:             Create a temporary directory to hold test files.
./DeepSpeed/op_builder/builder.py:             Define a simple C program that calls the function in question
./DeepSpeed/op_builder/builder.py:             Write the test program to a file.
./DeepSpeed/op_builder/builder.py:             Redirect stderr file descriptor to a file to silence compile/link warnings.
./DeepSpeed/op_builder/builder.py:             Workaround for behavior in distutils.ccompiler.CCompiler.object_filenames()
./DeepSpeed/op_builder/builder.py:             Otherwise, a local directory will be used instead of tempdir
./DeepSpeed/op_builder/builder.py:             Attempt to compile the C program into an object file.
./DeepSpeed/op_builder/builder.py:             Attempt to link the object file into an executable.
./DeepSpeed/op_builder/builder.py:             Be sure to tack on any libraries that have been specified.
./DeepSpeed/op_builder/builder.py:             Compile and link succeeded
./DeepSpeed/op_builder/builder.py:             Restore stderr file descriptor and close the stderr redirect file.
./DeepSpeed/op_builder/builder.py:             Delete the temporary directory holding the test program and stderr files.
./DeepSpeed/op_builder/builder.py:             gcc does not provide -march on PowerPC, use -mcpu instead
./DeepSpeed/op_builder/builder.py:         Construct cpu_info dict from lscpu that is similar to what py-cpuinfo provides
./DeepSpeed/op_builder/builder.py:             Ensure the op we're about to load was compiled with the same
./DeepSpeed/op_builder/builder.py:             torch/cuda versions we are currently using at runtime.
./DeepSpeed/op_builder/builder.py:         Torch will try and apply whatever CCs are in the arch list at compile time,
./DeepSpeed/op_builder/builder.py:         we have already set the intended targets ourselves we know that will be
./DeepSpeed/op_builder/builder.py:         needed at runtime. This prevents CC collisions such as multiple __half
./DeepSpeed/op_builder/builder.py:         implementations. Stash arch list to reset after build.
./DeepSpeed/op_builder/builder.py:         Reset arch list so we are not silently removing it for other possible use cases
./DeepSpeed/op_builder/builder.py:             Compile for underlying architectures since we know those at runtime
./DeepSpeed/op_builder/builder.py:             Cross-compile mode, compile for various architectures
./DeepSpeed/op_builder/builder.py:             env override takes priority
./DeepSpeed/op_builder/builder.py:         Fix from apex that might be relevant for us as well, related to https://github.com/NVIDIA/apex/issues/456
./DeepSpeed/op_builder/builder.py:             hip converts paths to absolute, this converts back to relative
./DeepSpeed/op_builder/spatial_inference.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/spatial_inference.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/spatial_inference.py: DeepSpeed Team
./DeepSpeed/op_builder/cpu_adam.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/cpu_adam.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/cpu_adam.py: DeepSpeed Team
./DeepSpeed/op_builder/transformer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/transformer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/transformer.py: DeepSpeed Team
./DeepSpeed/op_builder/all_ops.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/all_ops.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/all_ops.py: DeepSpeed Team
./DeepSpeed/op_builder/all_ops.py:     during installation time accelerator is visible, otherwise return deepspeed.accelerator
./DeepSpeed/op_builder/all_ops.py: List of all available ops
./DeepSpeed/op_builder/all_ops.py: reflect all builder names into __op_builders__
./DeepSpeed/op_builder/all_ops.py:     avoid self references
./DeepSpeed/op_builder/all_ops.py:                 append builder to __op_builders__ list
./DeepSpeed/op_builder/fused_lion.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/fused_lion.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/fused_lion.py: DeepSpeed Team
./DeepSpeed/op_builder/stochastic_transformer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/stochastic_transformer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/stochastic_transformer.py: DeepSpeed Team
./DeepSpeed/op_builder/fused_lamb.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/fused_lamb.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/fused_lamb.py: DeepSpeed Team
./DeepSpeed/op_builder/evoformer_attn.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/evoformer_attn.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/evoformer_attn.py: DeepSpeed Team
./DeepSpeed/op_builder/inference_core_ops.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/inference_core_ops.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/inference_core_ops.py: DeepSpeed Team
./DeepSpeed/op_builder/async_io.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/op_builder/async_io.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/op_builder/async_io.py: DeepSpeed Team
./DeepSpeed/op_builder/async_io.py:         -O0 for improved debugging, since performance is bound by I/O
./DeepSpeed/op_builder/async_io.py:         Check for the existence of libaio by using distutils
./DeepSpeed/op_builder/async_io.py:         to compile and link a test program that calls io_submit,
./DeepSpeed/op_builder/async_io.py:         which is a function provided by libaio that is used in the async_io op.
./DeepSpeed/op_builder/async_io.py:         If needed, one can define -I and -L entries in CFLAGS and LDFLAGS
./DeepSpeed/op_builder/async_io.py:         respectively to specify the directories for libaio.h and libaio.so.
./DeepSpeed/op_builder/async_io.py:             Check for the libaio package via known package managers
./DeepSpeed/op_builder/async_io.py:             to print suggestions on which package to install.
./DeepSpeed/tests/lightning/test_simple.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/lightning/test_simple.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/lightning/test_simple.py: DeepSpeed Team
./DeepSpeed/tests/unit/elasticity/test_elastic.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/elasticity/test_elastic.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/elasticity/test_elastic.py: DeepSpeed Team
./DeepSpeed/tests/unit/megatron_model.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/megatron_model.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/megatron_model.py: DeepSpeed Team
./DeepSpeed/tests/unit/megatron_model.py:     setting "make-vocab-size-divisible-by" to avoid word-embedding size change in resizing testing.
./DeepSpeed/tests/unit/megatron_model.py:         setting "make-vocab-size-divisible-by" to avoid word-embedding size change in resizing testing.
./DeepSpeed/tests/unit/megatron_model.py:                 hardcode attn mask for testing, PP requires the attn_mask to be stashed
./DeepSpeed/tests/unit/launcher/test_run.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/launcher/test_run.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/launcher/test_run.py: DeepSpeed Team
./DeepSpeed/tests/unit/launcher/test_run.py:     First try no include/exclude
./DeepSpeed/tests/unit/launcher/test_run.py:     exclude slots
./DeepSpeed/tests/unit/launcher/test_run.py:     only use one slot
./DeepSpeed/tests/unit/launcher/test_run.py:     including slots multiple times shouldn't break things
./DeepSpeed/tests/unit/launcher/test_run.py:     including just 'worker-0' without : should still use all GPUs
./DeepSpeed/tests/unit/launcher/test_run.py:     excluding just 'worker-0' without : should eliminate everything
./DeepSpeed/tests/unit/launcher/test_run.py:     exclude all slots manually
./DeepSpeed/tests/unit/launcher/test_run.py:     First try no include/exclude
./DeepSpeed/tests/unit/launcher/test_run.py:     include a node
./DeepSpeed/tests/unit/launcher/test_run.py:     exclude a node
./DeepSpeed/tests/unit/launcher/test_run.py:     exclude part of each node
./DeepSpeed/tests/unit/launcher/test_run.py:     host does not exist
./DeepSpeed/tests/unit/launcher/test_run.py:     slot does not exist
./DeepSpeed/tests/unit/launcher/test_run.py:     formatting
./DeepSpeed/tests/unit/launcher/test_run.py:     inclusion
./DeepSpeed/tests/unit/launcher/test_run.py:     exclusion
./DeepSpeed/tests/unit/launcher/test_run.py:     good hostfile w. empty lines and comment
./DeepSpeed/tests/unit/launcher/test_run.py:    worker-1 slots=3
./DeepSpeed/tests/unit/launcher/test_run.py:     this is a comment
./DeepSpeed/tests/unit/launcher/test_run.py:     duplicate host
./DeepSpeed/tests/unit/launcher/test_run.py:     incorrect whitespace
./DeepSpeed/tests/unit/launcher/test_run.py:     no whitespace
./DeepSpeed/tests/unit/launcher/test_run.py:     empty
./DeepSpeed/tests/unit/launcher/test_run.py:     mix of good/bad
./DeepSpeed/tests/unit/launcher/test_ds_arguments.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/launcher/test_ds_arguments.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/launcher/test_ds_arguments.py: DeepSpeed Team
./DeepSpeed/tests/unit/launcher/test_ds_arguments.py:         negative case for range overlapping
./DeepSpeed/tests/unit/launcher/test_ds_arguments.py:         invalid core list must fail
./DeepSpeed/tests/unit/launcher/test_ds_arguments.py:         negative case for reverse order -- case 1
./DeepSpeed/tests/unit/launcher/test_ds_arguments.py:         invalid core list must fail
./DeepSpeed/tests/unit/launcher/test_ds_arguments.py:         negative case for reverse order -- case 2
./DeepSpeed/tests/unit/launcher/test_ds_arguments.py:         invalid core list must fail
./DeepSpeed/tests/unit/launcher/test_multinode_runner.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/launcher/test_multinode_runner.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/launcher/test_multinode_runner.py: DeepSpeed Team
./DeepSpeed/tests/unit/pipe/test_pipe_module.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/pipe/test_pipe_module.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/pipe/test_pipe_module.py: DeepSpeed Team
./DeepSpeed/tests/unit/pipe/test_pipe_module.py:         Ensure all parameters are accounted for.
./DeepSpeed/tests/unit/pipe/test_pipe_module.py:             label 0 is meaningless
./DeepSpeed/tests/unit/monitor/test_monitor.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/monitor/test_monitor.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/monitor/test_monitor.py: DeepSpeed Team
./DeepSpeed/tests/unit/checkpoint/test_reshape_checkpoint.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/checkpoint/test_reshape_checkpoint.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/checkpoint/test_reshape_checkpoint.py: DeepSpeed Team
./DeepSpeed/tests/unit/checkpoint/test_reshape_checkpoint.py: Specify 3d shape as pp/tp/dp
./DeepSpeed/tests/unit/checkpoint/test_moe_checkpoint.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/checkpoint/test_moe_checkpoint.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/checkpoint/test_moe_checkpoint.py: DeepSpeed Team
./DeepSpeed/tests/unit/checkpoint/test_moe_checkpoint.py:         param group must have a random unique name (for now)
./DeepSpeed/tests/unit/checkpoint/test_moe_checkpoint.py:         TODO: clean-up this requirement, the unique name should not be required here
./DeepSpeed/tests/unit/checkpoint/test_other_optimizer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/checkpoint/test_other_optimizer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/checkpoint/test_other_optimizer.py: DeepSpeed Team
./DeepSpeed/tests/unit/checkpoint/test_other_optimizer.py:         Load & verify optimizer states
./DeepSpeed/tests/unit/checkpoint/test_other_optimizer.py:         Ignore optimizer states
./DeepSpeed/tests/unit/checkpoint/test_other_optimizer.py:         Load & verify optimizer states
./DeepSpeed/tests/unit/checkpoint/test_other_optimizer.py:         Ignore optimizer states
./DeepSpeed/tests/unit/checkpoint/test_shared_weights.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/checkpoint/test_shared_weights.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/checkpoint/test_shared_weights.py: DeepSpeed Team
./DeepSpeed/tests/unit/checkpoint/test_shared_weights.py:         tie layer 1 and layer 2
./DeepSpeed/tests/unit/checkpoint/test_sparse.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/checkpoint/test_sparse.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/checkpoint/test_sparse.py: DeepSpeed Team
./DeepSpeed/tests/unit/checkpoint/test_tag_validation.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/checkpoint/test_tag_validation.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/checkpoint/test_tag_validation.py: DeepSpeed Team
./DeepSpeed/tests/unit/checkpoint/test_zero_optimizer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/checkpoint/test_zero_optimizer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/checkpoint/test_zero_optimizer.py: DeepSpeed Team
./DeepSpeed/tests/unit/checkpoint/test_zero_optimizer.py:         torch 1.2.* stores raw tensor id numbers in checkpoint state which leads to
./DeepSpeed/tests/unit/checkpoint/test_zero_optimizer.py:         false positive mismatches in checkpoint state comparisons.
./DeepSpeed/tests/unit/checkpoint/test_zero_optimizer.py:         Newer torch versions store tensor ids as 0, 1, 2, ...
./DeepSpeed/tests/unit/checkpoint/test_zero_optimizer.py:         Load checkpoint with dp world size = 2
./DeepSpeed/tests/unit/checkpoint/test_zero_optimizer.py:         1. pretrain a model and save it
./DeepSpeed/tests/unit/checkpoint/test_zero_optimizer.py:         2. load and immediately save a model with a fresh ds engine
./DeepSpeed/tests/unit/checkpoint/test_zero_optimizer.py:         This test reproduces a bug where one tries to retrieve a 16bit model before grad_accum
./DeepSpeed/tests/unit/checkpoint/test_zero_optimizer.py:         cycle was completed.
./DeepSpeed/tests/unit/checkpoint/test_zero_optimizer.py:         So we config grad_accum=2 and step only once and save_16bit_model
./DeepSpeed/tests/unit/checkpoint/test_zero_optimizer.py:         we stepped only once, and now save 16bit model before gradient_accumulation_steps=2 is complete
./DeepSpeed/tests/unit/checkpoint/test_zero_optimizer.py:         let's test just as well that we can save the checkpoint too
./DeepSpeed/tests/unit/checkpoint/test_zero_optimizer.py:         Validate backwards-compatibility of including frozen parameters in checkpoint
./DeepSpeed/tests/unit/checkpoint/test_zero_optimizer.py:         Validate exclusion of frozen parameters
./DeepSpeed/tests/unit/checkpoint/test_zero_optimizer.py:         Excluding frozen parameters should reduce checkpoint size
./DeepSpeed/tests/unit/checkpoint/test_zero_optimizer.py:         Validate custom state_dict model
./DeepSpeed/tests/unit/checkpoint/common.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/checkpoint/common.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/checkpoint/common.py: DeepSpeed Team
./DeepSpeed/tests/unit/checkpoint/common.py:     These are compared in more depth in other places
./DeepSpeed/tests/unit/checkpoint/common.py:                 these params are converted to float at runtime, cast to half for comparison
./DeepSpeed/tests/unit/checkpoint/common.py: following mixture-of-experts.md
./DeepSpeed/tests/unit/checkpoint/common.py:     Flush zero stage 3 cache
./DeepSpeed/tests/unit/checkpoint/common.py:                     some storage can be shared within an expert's checkpoint
./DeepSpeed/tests/unit/checkpoint/common.py:             should not attempt to get the file name to load it
./DeepSpeed/tests/unit/checkpoint/test_mics_optimizer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/checkpoint/test_mics_optimizer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/checkpoint/test_mics_optimizer.py: DeepSpeed Team
./DeepSpeed/tests/unit/checkpoint/test_mics_optimizer.py: Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
./DeepSpeed/tests/unit/checkpoint/test_mics_optimizer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/checkpoint/test_lr_scheduler.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/checkpoint/test_lr_scheduler.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/checkpoint/test_lr_scheduler.py: DeepSpeed Team
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py: DeepSpeed Team
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py:            (PipeTopo(num_pp=1,
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py:                      num_dp=4),
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py:             PipeTopo(num_pp=4,
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py:                      num_dp=1)),
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py:            (PipeTopo(num_pp=2,
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py:                      num_dp=2),
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py:             PipeTopo(num_pp=2,
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py:                      num_dp=2)),
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py:            (PipeTopo(num_pp=4,
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py:                      num_dp=1),
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py:             PipeTopo(num_pp=2,
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py:                      num_dp=2)),
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py:         Base and test can have different lengths, so make sure we map from the
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py:         smaller to larger model
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py:         Compare layers individually since partitions are different
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py:                 Skip functionals, etc.
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py:             Find the corresponding layer in B
./DeepSpeed/tests/unit/checkpoint/test_pipeline.py:             Compare layer parameters
./DeepSpeed/tests/unit/checkpoint/test_latest_checkpoint.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/checkpoint/test_latest_checkpoint.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/checkpoint/test_latest_checkpoint.py: DeepSpeed Team
./DeepSpeed/tests/unit/checkpoint/test_latest_checkpoint.py:         should be no-op, since latest doesn't exist
./DeepSpeed/tests/unit/simple_model.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/simple_model.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/simple_model.py: DeepSpeed Team
./DeepSpeed/tests/unit/simple_model.py:         Freeze first layer
./DeepSpeed/tests/unit/simple_model.py:         using two MoE layers to check implications of sharing a single storage
./DeepSpeed/tests/unit/simple_model.py:         interleaving MoE modules with dense to create an opportunity
./DeepSpeed/tests/unit/simple_model.py:         for gradients to be merged in ZeRO stage 2 average_tensor reduce bucket
./DeepSpeed/tests/unit/simple_model.py:         We assume up to one full node executing unit tests
./DeepSpeed/tests/unit/util.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/util.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/util.py: DeepSpeed Team
./DeepSpeed/tests/unit/util.py:     Sometimes bf16 tests are runnable even if not natively supported by accelerator
./DeepSpeed/tests/unit/alexnet_model.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/alexnet_model.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/alexnet_model.py: DeepSpeed Team
./DeepSpeed/tests/unit/alexnet_model.py: Define this here because we cannot pickle local lambda functions
./DeepSpeed/tests/unit/alexnet_model.py:     Only one rank per machine downloads.
./DeepSpeed/tests/unit/alexnet_model.py:         disable dropout
./DeepSpeed/tests/unit/alexnet_model.py:         deepspeed_io defaults to creating a dataloader that uses a
./DeepSpeed/tests/unit/alexnet_model.py:         multiprocessing pool. Our tests use pools and we cannot nest pools in
./DeepSpeed/tests/unit/alexnet_model.py:         python. Therefore we're injecting this kwarg to ensure that no pools
./DeepSpeed/tests/unit/alexnet_model.py:         are used in the dataloader.
./DeepSpeed/tests/unit/runtime/test_ds_config_model.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/test_ds_config_model.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/test_ds_config_model.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:             get lr before training starts
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify initial lr
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify warmup completion
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify post-warmup completion
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify initial lr
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify lr at warmup completion
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify decay phase
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify starting lr
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:             Verify staircase increasing lr
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:             Verify continuous increasing lr
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify starting lr
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify peak lr
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify increasing phase
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify decreasing phase
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify decay phase
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify starting lr
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify peak lr
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify decreasing phase
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify increasing phase
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify decay phase
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify starting lr
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify peak lr
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify end lr
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify increasing phase
./DeepSpeed/tests/unit/runtime/test_lr_schedulers.py:         Verify decreasing phase
./DeepSpeed/tests/unit/runtime/sparse_tensor/test_averaging_sparse_gradients.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/sparse_tensor/test_averaging_sparse_gradients.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/sparse_tensor/test_averaging_sparse_gradients.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/sparse_tensor/test_sparse_grads.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/sparse_tensor/test_sparse_grads.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/sparse_tensor/test_sparse_grads.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/sparse_tensor/test_csr.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/sparse_tensor/test_csr.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/sparse_tensor/test_csr.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/test_multi_output_model.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/test_multi_output_model.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/test_multi_output_model.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/test_autocast.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/test_autocast.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/test_autocast.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/pipe/test_pipe_schedule.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/pipe/test_pipe_schedule.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/pipe/test_pipe_schedule.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/pipe/test_pipe_schedule.py:         Ensure we don't send an activation the first step
./DeepSpeed/tests/unit/runtime/pipe/test_pipe_schedule.py:         the last active step is only a send
./DeepSpeed/tests/unit/runtime/pipe/test_pipe_schedule.py:         no work later on
./DeepSpeed/tests/unit/runtime/pipe/test_pipe_schedule.py:         Normally we need to load/forward/send
./DeepSpeed/tests/unit/runtime/pipe/test_pipe.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/pipe/test_pipe.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/pipe/test_pipe.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/pipe/test_pipe.py:         Allocate model for consistent initial weights.
./DeepSpeed/tests/unit/runtime/pipe/test_pipe.py:         Train with just data parallelism
./DeepSpeed/tests/unit/runtime/pipe/test_pipe.py:     def _check_model_params_equal(self, model1, model2):
./DeepSpeed/tests/unit/runtime/pipe/test_pipe.py:         for p1, p2 in zip(model1.parameters(), model2.parameters()):
./DeepSpeed/tests/unit/runtime/pipe/test_pipe.py:             if p1.data.ne(p2.data).sum() > 0:
./DeepSpeed/tests/unit/runtime/pipe/test_pipe.py:                 assert False, f"model params not equal"
./DeepSpeed/tests/unit/runtime/pipe/test_pipe.py:         Allocate model for consistent initial weights.
./DeepSpeed/tests/unit/runtime/pipe/test_pipe.py:         Train with not set use_reentrant, default: True
./DeepSpeed/tests/unit/runtime/pipe/test_pipe.py:         Train with set use_reentrant=False, this will use ``non_reentrant_checkpoint``
./DeepSpeed/tests/unit/runtime/pipe/test_pipe.py:         the following check could passed on higher version docker: nvcr.io/nvidia/pytorch:23.07-py3(torch2.1.0 cuda12.1)
./DeepSpeed/tests/unit/runtime/pipe/test_pipe.py:         Check if models have same weights after training
./DeepSpeed/tests/unit/runtime/pipe/test_pipe.py:         self._check_model_params_equal(base_model, test_model)
./DeepSpeed/tests/unit/runtime/pipe/test_topology.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/pipe/test_topology.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/pipe/test_topology.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/pipe/test_topology.py:     Easy access method
./DeepSpeed/tests/unit/runtime/pipe/test_topology.py:     Handle nonsense. We don't want to RuntimeError because it allows us to write more
./DeepSpeed/tests/unit/runtime/pipe/test_topology.py:     generalized code for data/model/pipe parallelism
./DeepSpeed/tests/unit/runtime/pipe/test_topology.py:         Test collectives along the pipeline parallel process groups
./DeepSpeed/tests/unit/runtime/pipe/test_topology.py:         Test collectives along the data parallel process groups
./DeepSpeed/tests/unit/runtime/test_runtime_utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/test_runtime_utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/test_runtime_utils.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/test_runtime_utils.py:         param2 is now MoE parameter
./DeepSpeed/tests/unit/runtime/test_runtime_utils.py:         param2 is now MoE parameter
./DeepSpeed/tests/unit/runtime/test_ds_initialize.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/test_ds_initialize.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/test_ds_initialize.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/test_ds_initialize.py:         20B test
./DeepSpeed/tests/unit/runtime/test_ds_initialize.py:        hidden_dim = 16 * 1024
./DeepSpeed/tests/unit/runtime/test_ds_initialize.py:         Skip checks
./DeepSpeed/tests/unit/runtime/test_ds_initialize.py:         Config declaration
./DeepSpeed/tests/unit/runtime/test_ds_initialize.py:         Enumerate supported configurations
./DeepSpeed/tests/unit/runtime/test_ds_initialize.py:         ZeRO 1 Wrapper
./DeepSpeed/tests/unit/runtime/test_ds_initialize.py:         ZeRO 2 Wrapper
./DeepSpeed/tests/unit/runtime/test_ds_initialize.py:         ZeRO 3 Wrapper
./DeepSpeed/tests/unit/runtime/test_ds_initialize.py:         Amp Wrapper
./DeepSpeed/tests/unit/runtime/test_ds_initialize.py:         FP16 Wrapper
./DeepSpeed/tests/unit/runtime/test_ds_initialize.py:         BF16 Wrapper
./DeepSpeed/tests/unit/runtime/test_ds_initialize.py:         No Wrapper
./DeepSpeed/tests/unit/runtime/test_ds_initialize.py:                 Verify invalid combination is correctly handled
./DeepSpeed/tests/unit/runtime/test_pld.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/test_pld.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/test_pld.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/test_mup_optimizers.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/test_mup_optimizers.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/test_mup_optimizers.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/comm/test_coalesced_collectives.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/comm/test_coalesced_collectives.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/comm/test_coalesced_collectives.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/utils/test_partition.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/utils/test_partition.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/utils/test_partition.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/utils/test_partition.py:     Parameters per layer for a transformer model with 24 transformers and hidden dim 1024
./DeepSpeed/tests/unit/runtime/zero/test_zero_config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/zero/test_zero_config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/zero/test_zero_config.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/zero/test_zero_tiled.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/zero/test_zero_tiled.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/zero/test_zero_tiled.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/zero/test_zero_tiled.py:     compare grads
./DeepSpeed/tests/unit/runtime/zero/test_zero_tiled.py:     compare grads
./DeepSpeed/tests/unit/runtime/zero/test_zero.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/zero/test_zero.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/zero/test_zero.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/zero/test_zero.py: testing the fix https://github.com/microsoft/DeepSpeed/pull/1227
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         force all params to be partitioned by forcing threshold=0
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:                 run the same layer multiple times in a loop - to test a stack of forwards, followed by a stack of backwards
./DeepSpeed/tests/unit/runtime/zero/test_zero.py: testing the fix https://github.com/microsoft/DeepSpeed/pull/1227
./DeepSpeed/tests/unit/runtime/zero/test_zero.py: also reproduces the https://github.com/microsoft/DeepSpeed/pull/1372
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         XXX: ideally refactor with the 2_param_group test as 75% is the same
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         force all params to be partitioned by forcing threshold=0
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:                 to reproduce https://github.com/microsoft/DeepSpeed/pull/1372 it is important that
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:                 the number of total elements is uneven:
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:                 (1) 4 layers of 3*(3+1)=12 elements each, 48 in total
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:                 (2) the following adds 4+1=5 elements
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:                 total 48+5=53 (uneven as desired) elements
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         we want at least 2x layers as there are gpus to trigger round_robin_fp16_groups reshuffle in zero2
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         Flush zero stage 3 cache
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         make sure all sides saved it
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         dump_state_dict(fp32_model)
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:                 float() workaround for torch<1.6
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         TODO:
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         - need to test with multiple param groups
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         force all params to be partitioned by forcing threshold=0
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         make sure all sides saved it
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         dump_state_dict(model)
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         dump_state_dict(fp32_model)
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:                 float() workaround for torch<1.6
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         get nccl all-gather send buffers alignment factor
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:                 verify that data partition start locations are 4-byte aligned
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:             for ease in testing convert outputs to dict.
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:             check the gradients
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:             layer1 = [..., 1, 2, ...]
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:             layer2 = [..., 2, 4, ...]
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:             layer3 = [..., 3, 6, ...]
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:             dloss_wrt_layer3 = hidden2
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:             dloss_wrt_layer2 = layer3 * hidden1
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:             dloss_wrt_layer1 = layer3 * layer2 * x
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:                 parameters dont split evenly across ranks so rank 1 has a zero-padded
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:                 partition
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         TODO. add testing for this - for now we just call it to make sure it
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         doesn't throw
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         taking an optimizer step invalidates all parameters, make sure everything
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         has been partitioned afterwards
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:                 only do weight initialization on root rank to
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:                 make sure we are broadcasting correctly from rank 0
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:             TODO. finish writing this test
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:             for ease in testing convert outputs to dict.
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:             check the gradients
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:             layer1 = [..., 1, 2, ...]
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:             layer2 = [..., 2, 4, ...]
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:             layer3 = [..., 3, 6, ...]
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:             dloss_wrt_layer3 = hidden2
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:             dloss_wrt_layer2 = layer3 * hidden1
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:             dloss_wrt_layer1 = layer3 * layer2 * x
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:                 parameters dont split evenly across ranks so rank 1 has a zero-padded
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:                 partition
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         TODO. add testing for this - for now we just call it to make sure it
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         doesn't throw
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:         force all params to be partitioned by forcing threshold=0
./DeepSpeed/tests/unit/runtime/zero/test_zero.py:                 freeze one fc
./DeepSpeed/tests/unit/runtime/zero/test_ignore_unused_parameters.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/zero/test_ignore_unused_parameters.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/zero/test_ignore_unused_parameters.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/zero/test_zero_tensor_fragment.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/zero/test_zero_tensor_fragment.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/zero/test_zero_tensor_fragment.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/zero/test_zero_tensor_fragment.py:     Needed in ZeRO 3. Not doing so can give memory leak
./DeepSpeed/tests/unit/runtime/zero/test_zero_tensor_fragment.py:     Need multiple gpus to test possible hanging
./DeepSpeed/tests/unit/runtime/zero/test_zero_tensor_fragment.py:             dist.broadcast(rand_value, src=0, group=group)
./DeepSpeed/tests/unit/runtime/zero/test_zero_tensor_fragment.py:     Need multiple gpus to test possible hanging
./DeepSpeed/tests/unit/runtime/zero/test_zero_tensor_fragment.py:         Needed in ZeRO 3. Not doing so can leak memory.
./DeepSpeed/tests/unit/runtime/zero/test_zeropp.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/zero/test_zeropp.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/zero/test_zeropp.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/zero/test_zeropp.py:Large sweep along hidden dim, num_layers, and zpg of different sizes
./DeepSpeed/tests/unit/runtime/zero/test_zeropp.py:Assert when zpg=1 that secondary group and tensors are invalid
./DeepSpeed/tests/unit/runtime/zero/test_zero_context.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/zero/test_zero_context.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/zero/test_zero_context.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/zero/test_zero_context.py: Test that no sub-class or super-class is missed
./DeepSpeed/tests/unit/runtime/zero/test_zero_context.py:         This would not be partitioned before bugfix 5ca8167
./DeepSpeed/tests/unit/runtime/zero/test_zero_context.py:         on exit from `GatheredParameters` the gathered params should be freed and not leak memory
./DeepSpeed/tests/unit/runtime/zero/test_zero_context.py:         calling stop() while uninitialized - has no effect
./DeepSpeed/tests/unit/runtime/zero/test_zero_context.py:         any call to start() (from dataloader or not) initializes the timer
./DeepSpeed/tests/unit/runtime/zero/test_zero_context.py:         calling stop() after initialized - increments the local micro step counter
./DeepSpeed/tests/unit/runtime/zero/test_zero_context.py:         calling start()/stop() to increment the step counter until start_step
./DeepSpeed/tests/unit/runtime/zero/test_zero_context.py:         calling start()/stop() accumulates duration during gradient accumulation
./DeepSpeed/tests/unit/runtime/zero/test_zero_context.py:             step elapsed time is reset after gradient accumulation steps
./DeepSpeed/tests/unit/runtime/zero/test_zero_context.py:                 external use of self.linear1.weight
./DeepSpeed/tests/unit/runtime/zero/test_zero_context.py:         Ensure there is no impact outside the context
./DeepSpeed/tests/unit/runtime/zero/test_zero_context.py:         Gather and make a change
./DeepSpeed/tests/unit/runtime/zero/test_zero_context.py:         should now be scattered again
./DeepSpeed/tests/unit/runtime/zero/test_zero_context.py:         Now gather again and ensure the change is global
./DeepSpeed/tests/unit/runtime/zero/test_zero_context.py:             all ranks compare
./DeepSpeed/tests/unit/runtime/zero/utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/zero/utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/zero/utils.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/zero/utils.py:     Setup for a serial run
./DeepSpeed/tests/unit/runtime/zero/test_zero_offloadpp.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/zero/test_zero_offloadpp.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/zero/test_zero_offloadpp.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/zero/test_zero_offloadpp.py:Large sweep along hidden dim, num_layers of different sizes
./DeepSpeed/tests/unit/runtime/zero/test_zero_dynamic_class.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/zero/test_zero_dynamic_class.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/zero/test_zero_dynamic_class.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/zero/test_zero_dynamic_class.py:         ensure that zero3 processed the parameter
./DeepSpeed/tests/unit/runtime/zero/test_zero_dynamic_class.py:         ensure that zero3 processed the parameter
./DeepSpeed/tests/unit/runtime/zero/test_zero_nesting_init.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/zero/test_zero_nesting_init.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/zero/test_zero_nesting_init.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/zero/test_zero_nesting_init.py:         ensure that zero3 processed the parameter
./DeepSpeed/tests/unit/runtime/zero/test_zero_nesting_init.py:         ensure that zero3 processed the parameter
./DeepSpeed/tests/unit/runtime/zero/test_zero_nesting_init.py:     Testing a model with composed and nested zero.Inits, with 3 zero.Init contexts, 1 parent and 2 children.
./DeepSpeed/tests/unit/runtime/zero/test_zero_nesting_init.py:     The skeleton of the model is like so
./DeepSpeed/tests/unit/runtime/zero/test_zero_nesting_init.py:    
./DeepSpeed/tests/unit/runtime/zero/test_zero_nesting_init.py:     class VisionEncoderDecoderModel(...)::
./DeepSpeed/tests/unit/runtime/zero/test_zero_nesting_init.py:         def __init__(self):
./DeepSpeed/tests/unit/runtime/zero/test_zero_nesting_init.py:                 encoder = AutoModel.from_config(config.encoder)
./DeepSpeed/tests/unit/runtime/zero/test_zero_nesting_init.py:                 decoder = AutoModelForCausalLM.from_config(config.decoder)
./DeepSpeed/tests/unit/runtime/zero/test_zero_nesting_init.py:    
./DeepSpeed/tests/unit/runtime/zero/test_zero_nesting_init.py:     And the user calls like below:
./DeepSpeed/tests/unit/runtime/zero/test_zero_nesting_init.py:     VisionEncoderDecoderModel.from_pretrained(...)
./DeepSpeed/tests/unit/runtime/zero/test_zero_nesting_init.py:     which calls this constructor inside zero.Init
./DeepSpeed/tests/unit/runtime/zero/test_zero_context_return.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/zero/test_zero_context_return.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/zero/test_zero_context_return.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/zero/test_zero_context_return.py:         return the bias to trigger a dangling external param
./DeepSpeed/tests/unit/runtime/zero/test_zero_context_return.py:             forward the external param
./DeepSpeed/tests/unit/runtime/zero/test_zero_context_return.py:         bias is actually dangler.d_linear1.bias
./DeepSpeed/tests/unit/runtime/zero/test_zero_context_return.py:         Make sure it's at the right level of the stack
./DeepSpeed/tests/unit/runtime/zero/test_zero_context_ancestry.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/zero/test_zero_context_ancestry.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/zero/test_zero_context_ancestry.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/zero/test_zero_context_ancestry.py: test that sub-classes get params that aren't prematurely partitioned and thus requiring gathering
./DeepSpeed/tests/unit/runtime/zero/test_zero_context_ancestry.py: fixed by https://github.com/microsoft/DeepSpeed/pull/1202
./DeepSpeed/tests/unit/runtime/zero/test_zero_context_ancestry.py:         test that all params have been partitioned
./DeepSpeed/tests/unit/runtime/zero/test_zero_context_ancestry.py:         test that the weights manipulation during each __init__ worked in all w/o needing gathering
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing_non_reentrant.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing_non_reentrant.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing_non_reentrant.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing_non_reentrant.py: TODO: add tests with model parallelism for activation partitioning and other features.
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing_non_reentrant.py: the hack to clone the module `test_activation_checkpointing` and inject
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing_non_reentrant.py: `non_reentrant_checkpoint` as the `ckpt` of the origin test module
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing_non_reentrant.py: below classes are used to test the graph with inputs have no grad and parameters has grad, namely partial graph?
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing_non_reentrant.py:         First return is a tensor
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing.py: TODO: add tests with model parallelism for activation partitioning and other features.
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing.py:     Move to device
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing.py:     Get rid of dropouts until we fork the RNG between tests.
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing.py:     Move to device
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing.py:     Get rid of dropouts until we fork the RNG between tests.
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing.py:
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing.py: Helpers
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing.py:
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing.py:             must cast BoolTensor in older torch versions
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing.py:
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing.py: Tests
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing.py:
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing.py: both bool and float are important, as bool is not differentiable
./DeepSpeed/tests/unit/runtime/activation_checkpointing/test_activation_checkpointing.py:         First return is a tensor
./DeepSpeed/tests/unit/runtime/test_data.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/test_data.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/test_data.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/test_data.py:         TODO: no way to set DeepSpeedEngine.deepspeed_io params, need to use
./DeepSpeed/tests/unit/runtime/test_data.py:         pin_memory=False for cuda device
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Test whether the momentum mask works
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Test whether momentum mask still exist after saving checkpoint
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Test whether momentum mask stays the same after loading checkpoint
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Test whether worker&server error is reset
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Test whether momentum mask stays the same after loading checkpoint
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Test whether worker&server error is reset
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         TODO: Add correctness tests/asserts comparing with baseline?
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Test whether the momentum mask works
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Test whether momentum mask still exist after saving checkpoint
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Test whether momentum mask stays the same after loading checkpoint
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Test whether worker&server error is reset
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Test whether momentum mask stays the same after loading checkpoint
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Test whether worker&server error is reset
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         TODO: Add correctness tests/asserts comparing with baseline?
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Test whether the momentum mask works
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Test whether momentum mask still exist after saving checkpoint
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Test whether momentum mask stays the same after loading checkpoint
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Test whether worker&server error is reset
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Test whether scaling_coeffs is loaded correctly
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Test whether momentum mask stays the same after loading checkpoint
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Test whether worker&server error is reset
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Test whether scaling_coeffs, lamb_coeff_freeze, last_factor are reset
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         TODO: Add correctness tests/asserts comparing with baseline?
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         A simulated compression function using deepspeed.comm
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         Adding bias to the initialization of the gradient we are communicating
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         In order to get rid of the case where some elements in the gradient are too small
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         If the number in the compensated_server_m is too small (e.g 1e-8), then calling sign() might be problematic
./DeepSpeed/tests/unit/runtime/half_precision/onebit/test_onebit.py:         The test would skip those numbers that are too small in compensated_server_m
./DeepSpeed/tests/unit/runtime/half_precision/test_fp8.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/half_precision/test_fp8.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/half_precision/test_fp8.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/half_precision/test_fp8.py:             Have to set seed before model
./DeepSpeed/tests/unit/runtime/half_precision/test_fp8.py:             TransformerEngine Model
./DeepSpeed/tests/unit/runtime/half_precision/test_fp8.py:             Create FP8 recipe. Note: All input args are optional.
./DeepSpeed/tests/unit/runtime/half_precision/test_fp8.py:             Init DeepSpeed
./DeepSpeed/tests/unit/runtime/half_precision/test_fp8.py:                 Enables autocasting for the forward pass
./DeepSpeed/tests/unit/runtime/half_precision/test_fp8.py:         config
./DeepSpeed/tests/unit/runtime/half_precision/test_fp16.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/half_precision/test_fp16.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/half_precision/test_fp16.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/half_precision/test_fp16.py:         initialize MoE
./DeepSpeed/tests/unit/runtime/half_precision/test_fp16.py:         initialize MoE
./DeepSpeed/tests/unit/runtime/half_precision/test_fp16.py:         optimizer = torch.optim.AdamW(params=model.parameters())
./DeepSpeed/tests/unit/runtime/half_precision/test_fp16.py:         initialize MoE
./DeepSpeed/tests/unit/runtime/half_precision/test_fp16.py:         Ensure the static scaler is configured.
./DeepSpeed/tests/unit/runtime/half_precision/test_fp16.py:         Now make sure things work..
./DeepSpeed/tests/unit/runtime/half_precision/test_fp16.py:         Ensure model has 2 parameters, to cause empty partition with DP=3
./DeepSpeed/tests/unit/runtime/half_precision/test_fp16.py:         Now make sure things work..
./DeepSpeed/tests/unit/runtime/half_precision/test_bf16.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/half_precision/test_bf16.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/half_precision/test_bf16.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/half_precision/test_bf16.py:         Ensure model has 2 parameters, to cause empty partition with DP=3
./DeepSpeed/tests/unit/runtime/half_precision/test_bf16.py:         Now make sure things work..
./DeepSpeed/tests/unit/runtime/half_precision/test_dynamic_loss_scale.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/half_precision/test_dynamic_loss_scale.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/half_precision/test_dynamic_loss_scale.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/half_precision/test_dynamic_loss_scale.py:         Ensure the dynamic loss scaler is correctly configured.
./DeepSpeed/tests/unit/runtime/half_precision/test_dynamic_loss_scale.py:         Ensure the dynamic loss scaler is correctly configured.
./DeepSpeed/tests/unit/runtime/half_precision/test_dynamic_loss_scale.py:         Ensure the dynamic loss scaler is correctly configured.
./DeepSpeed/tests/unit/runtime/half_precision/test_dynamic_loss_scale.py:         Run model with overflows to decrease scale
./DeepSpeed/tests/unit/runtime/half_precision/test_dynamic_loss_scale.py:         Run model scale_window + 1 times to increase scale once
./DeepSpeed/tests/unit/runtime/half_precision/test_dynamic_loss_scale.py:         Run model with overflows to decrease scale
./DeepSpeed/tests/unit/runtime/half_precision/test_dynamic_loss_scale.py:         Ensure the dynamic loss scaler is correctly configured.
./DeepSpeed/tests/unit/runtime/half_precision/test_dynamic_loss_scale.py:         Ensure the dynamic loss scaler is correctly configured.
./DeepSpeed/tests/unit/runtime/half_precision/test_dynamic_loss_scale.py:         Ensure the dynamic loss scaler is correctly configured.
./DeepSpeed/tests/unit/runtime/half_precision/test_dynamic_loss_scale.py:         Run model with overflows to decrease scale
./DeepSpeed/tests/unit/runtime/half_precision/test_dynamic_loss_scale.py:         Run model scale_window + 1 times to increase scale once
./DeepSpeed/tests/unit/runtime/half_precision/test_dynamic_loss_scale.py:         Run model with overflows to decrease scale
./DeepSpeed/tests/unit/runtime/test_data_efficiency.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/test_data_efficiency.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/test_data_efficiency.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/test_ds_config_dict.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/runtime/test_ds_config_dict.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/runtime/test_ds_config_dict.py: DeepSpeed Team
./DeepSpeed/tests/unit/runtime/test_ds_config_dict.py: A test on its own
./DeepSpeed/tests/unit/runtime/test_ds_config_dict.py: A test on its own
./DeepSpeed/tests/unit/runtime/test_ds_config_dict.py:Tests different batch config provided in deepspeed json file
./DeepSpeed/tests/unit/runtime/test_ds_config_dict.py:        test cases when all parameters are provided
./DeepSpeed/tests/unit/runtime/test_ds_config_dict.py:        test cases when two out of three parameters are provided
./DeepSpeed/tests/unit/runtime/test_ds_config_dict.py:            when gas is provided with one more parameter
./DeepSpeed/tests/unit/runtime/test_ds_config_dict.py:            test the case when only micro_batch or train_batch is provided
./DeepSpeed/tests/unit/runtime/test_ds_config_dict.py:            when only gas is provided
./DeepSpeed/tests/unit/runtime/test_ds_config_dict.py:            when gas is provided with something else and gas does not divide batch
./DeepSpeed/tests/unit/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/__init__.py: DeepSpeed Team
./DeepSpeed/tests/unit/comm/test_dist.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/comm/test_dist.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/comm/test_dist.py: DeepSpeed Team
./DeepSpeed/tests/unit/comm/test_dist.py: Demonstration of pytest's parameterization and fixtures
./DeepSpeed/tests/unit/comm/test_dist.py: Demonstration of distributed tests grouped in single class
./DeepSpeed/tests/unit/comm/test_dist.py: Demonstration of world_size override
./DeepSpeed/tests/unit/comm/test_dist.py: Demonstration of the DistributedFixture class
./DeepSpeed/tests/unit/comm/test_dist.py:             torch.dist is not done and for some reason the user says they don't want it done
./DeepSpeed/tests/unit/comm/test_dist.py:             torch.dist is not done and for some reason the user says they don't want it done
./DeepSpeed/tests/unit/utils/test_get_optim_files.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/utils/test_get_optim_files.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/utils/test_get_optim_files.py: DeepSpeed Team
./DeepSpeed/tests/unit/utils/test_init_on_device.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/utils/test_init_on_device.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/utils/test_init_on_device.py: DeepSpeed Team
./DeepSpeed/tests/unit/utils/test_groups.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/utils/test_groups.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/utils/test_groups.py: DeepSpeed Team
./DeepSpeed/tests/unit/common.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/common.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/common.py: DeepSpeed Team
./DeepSpeed/tests/unit/common.py: Worker timeout for tests that hang
./DeepSpeed/tests/unit/common.py:         Make xdist workers use different port ranges to avoid race conditions
./DeepSpeed/tests/unit/common.py:     Select first open port in range
./DeepSpeed/tests/unit/common.py:         CUDA_VISIBLE_DEVICES is not set, discover it using accelerator specific command instead
./DeepSpeed/tests/unit/common.py:     rotate list based on xdist worker id, example below
./DeepSpeed/tests/unit/common.py:     wid=0 -> ['0', '1', '2', '3']
./DeepSpeed/tests/unit/common.py:     wid=1 -> ['1', '2', '3', '0']
./DeepSpeed/tests/unit/common.py:     wid=2 -> ['2', '3', '0', '1']
./DeepSpeed/tests/unit/common.py:     wid=3 -> ['3', '0', '1', '2']
./DeepSpeed/tests/unit/common.py:         Grab fixture / parametrize kwargs from pytest request object
./DeepSpeed/tests/unit/common.py:         Verify we have enough accelerator devices to run this test
./DeepSpeed/tests/unit/common.py:         Set start method to `forkserver` (or `fork`)
./DeepSpeed/tests/unit/common.py:         Create process pool or use cached one
./DeepSpeed/tests/unit/common.py:         Run the test
./DeepSpeed/tests/unit/common.py:             Shortcut to exit pytest in the case of a hanged test. This
./DeepSpeed/tests/unit/common.py:             usually means an environment error and the rest of tests will
./DeepSpeed/tests/unit/common.py:             hang (causing super long unit test runtimes)
./DeepSpeed/tests/unit/common.py:         Tear down distributed environment and close process pools
./DeepSpeed/tests/unit/common.py:         If we skipped a test, propagate that to this process
./DeepSpeed/tests/unit/common.py:                 NOTE: unit tests don't support multi-node so local_rank == global rank
./DeepSpeed/tests/unit/common.py:                 In case of multiprocess launching LOCAL_SIZE should be same as WORLD_SIZE
./DeepSpeed/tests/unit/common.py:                 DeepSpeed single node launcher would also set LOCAL_SIZE accordingly
./DeepSpeed/tests/unit/common.py:             turn off NCCL logging if set
./DeepSpeed/tests/unit/common.py:     These values are just placeholders so that pytest recognizes this as a fixture
./DeepSpeed/tests/unit/common.py:     Temporary directory that is shared among test methods in a class
./DeepSpeed/tests/unit/common.py:         Catch world_size override pytest mark
./DeepSpeed/tests/unit/common.py:         DistributedTest subclasses may have multiple test methods
./DeepSpeed/tests/unit/profiling/flops_profiler/test_flops_profiler.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/profiling/flops_profiler/test_flops_profiler.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/profiling/flops_profiler/test_flops_profiler.py: DeepSpeed Team
./DeepSpeed/tests/unit/compression/test_compression.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/compression/test_compression.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/compression/test_compression.py: DeepSpeed Team
./DeepSpeed/tests/unit/compression/test_dequantization.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/compression/test_dequantization.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/compression/test_dequantization.py: DeepSpeed Team
./DeepSpeed/tests/unit/compression/test_dequantization.py: Copyright (c) 2023, 2023, Oracle and/or its affiliates.
./DeepSpeed/tests/unit/model_parallelism/test_configurable_parallel_mp.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/model_parallelism/test_configurable_parallel_mp.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/model_parallelism/test_configurable_parallel_mp.py: DeepSpeed Team
./DeepSpeed/tests/unit/model_parallelism/test_configurable_parallel_mp.py: TODO: integrated testing of TP and ZeRO 1/2/3
./DeepSpeed/tests/unit/model_parallelism/test_configurable_parallel_mp.py: This fixture provides the baseline model with mp=2 to TestConfigurableMPResize
./DeepSpeed/tests/unit/model_parallelism/test_configurable_parallel_pp.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/model_parallelism/test_configurable_parallel_pp.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/model_parallelism/test_configurable_parallel_pp.py: DeepSpeed Team
./DeepSpeed/tests/unit/model_parallelism/test_configurable_parallel_pp.py:         basic test case, mp_size=2, pp_size=2, verify ckpt saving/loading.
./DeepSpeed/tests/unit/model_parallelism/test_configurable_parallel_pp.py:             Compare outputs of each microbatch
./DeepSpeed/tests/unit/model_parallelism/test_configurable_parallel_pp.py: Fixture for defining the checkpoint path since all tests in
./DeepSpeed/tests/unit/model_parallelism/test_configurable_parallel_pp.py: TestConfigurableResizePP will use the same tmpdir
./DeepSpeed/tests/unit/model_parallelism/test_configurable_parallel_pp.py: Base class for creating / saving model output for baseline models. This is
./DeepSpeed/tests/unit/model_parallelism/test_configurable_parallel_pp.py: not meant to be used directly as a fixture to any classes
./DeepSpeed/tests/unit/model_parallelism/test_configurable_parallel_pp.py:                 baseline should be [[hidden, True]]]
./DeepSpeed/tests/unit/model_parallelism/test_configurable_parallel_pp.py: This may look odd, but there is a limitation with DistributedFixture that
./DeepSpeed/tests/unit/model_parallelism/test_configurable_parallel_pp.py: doesn't allow us to reuse a fixture with different worldsizes. This could be
./DeepSpeed/tests/unit/model_parallelism/test_configurable_parallel_pp.py: implemented in conftest.py::pytest_fixture_setup and common.py::DistributedFixture
./DeepSpeed/tests/unit/model_parallelism/test_configurable_parallel_pp.py:                 test should be [[hidden, True]]]
./DeepSpeed/tests/unit/model_parallelism/test_configurable_parallel_pp.py:     These tests are divided by baseline model worldsize and test model worldsize
./DeepSpeed/tests/unit/inference/test_inference.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/test_inference.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/test_inference.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/test_inference.py: Get a list of all models and mapping from task to supported models
./DeepSpeed/tests/unit/inference/test_inference.py: Get all combinations of task:model to test
./DeepSpeed/tests/unit/inference/test_inference.py: Assign to pytest variables for testing
./DeepSpeed/tests/unit/inference/test_inference.py:     Verify all test models are registered in HF
./DeepSpeed/tests/unit/inference/test_inference.py:     Verify all models are assigned to at least one task
./DeepSpeed/tests/unit/inference/test_inference.py:             This model on V100 is hitting memory problems that limit the number of output tokens
./DeepSpeed/tests/unit/inference/test_inference.py: Used to verify DeepSpeed kernel injection worked with a model
./DeepSpeed/tests/unit/inference/test_inference.py: Verify that test is valid
./DeepSpeed/tests/unit/inference/test_inference.py:     These should be removed once we fix several inference tests failing
./DeepSpeed/tests/unit/inference/test_inference.py:         Load the model on CPU first to avoid OOM for large models @fp32
./DeepSpeed/tests/unit/inference/test_inference.py:         Switch device to GPU after converting to half
./DeepSpeed/tests/unit/inference/test_inference.py:         Warm-up queries for perf measurement
./DeepSpeed/tests/unit/inference/test_inference.py:        for i in range(10):
./DeepSpeed/tests/unit/inference/test_inference.py:            _ = pipe(query, **inf_kwargs)
./DeepSpeed/tests/unit/inference/test_inference.py:         Warm-up queries for perf measurement
./DeepSpeed/tests/unit/inference/test_inference.py:        for i in range(10):
./DeepSpeed/tests/unit/inference/test_inference.py:            _ = pipe(query, **inf_kwargs)
./DeepSpeed/tests/unit/inference/test_inference.py:         facebook/opt* and some bigscient/bloom* models are not matching
./DeepSpeed/tests/unit/inference/test_inference.py:         baseline exactly, adding an exception to them for now
./DeepSpeed/tests/unit/inference/test_inference.py:         These performance tests are only measuring the time for a single
./DeepSpeed/tests/unit/inference/test_inference.py:         inference request, we just want to check that performance isn't terrible
./DeepSpeed/tests/unit/inference/test_inference.py:        assert ds_time <= (bs_time * 1.1)
./DeepSpeed/tests/unit/inference/test_inference.py:         We have to load these large models on CPU with pipeline because not
./DeepSpeed/tests/unit/inference/test_inference.py:         enough GPU memory
./DeepSpeed/tests/unit/inference/test_inference.py:         Switch device to GPU so that input tensors are not on CPU
./DeepSpeed/tests/unit/inference/test_inference.py:         TODO: enable this test for H100 tests
./DeepSpeed/tests/unit/inference/test_inference.py:         We have to load these large models on CPU with pipeline because not
./DeepSpeed/tests/unit/inference/test_inference.py:         enough GPU memory
./DeepSpeed/tests/unit/inference/test_inference.py:        bs_output = pipe(query, **inf_kwargs)
./DeepSpeed/tests/unit/inference/test_inference.py:         Switch device to GPU so that input tensors are not on CPU
./DeepSpeed/tests/unit/inference/test_inference.py:        print(local_rank, "baseline", bs_output)
./DeepSpeed/tests/unit/inference/test_inference.py:        assert assert_fn(bs_output, ds_output)
./DeepSpeed/tests/unit/inference/test_inference.py:         We have to load these large models on CPU with pipeline because not
./DeepSpeed/tests/unit/inference/test_inference.py:         enough GPU memory
./DeepSpeed/tests/unit/inference/test_inference.py:         Switch device to GPU so that input tensors are not on CPU
./DeepSpeed/tests/unit/inference/test_inference.py:         TODO: enable this test after torch 2.1 stable release
./DeepSpeed/tests/unit/inference/test_inference.py:         We have to load these large models on CPU with pipeline because not
./DeepSpeed/tests/unit/inference/test_inference.py:         enough GPU memory
./DeepSpeed/tests/unit/inference/test_inference.py:         Switch device to GPU so that input tensors are not on CPU
./DeepSpeed/tests/unit/inference/test_inference.py:        ["gpt2", "EleutherAI/gpt-j-6b"], # Causing OOM for this test
./DeepSpeed/tests/unit/inference/test_inference.py:         imports here to avoid import errors when pytest collects tests
./DeepSpeed/tests/unit/inference/test_inference.py:         The bootstrap_stderr function in lm_eval.metrics uses a
./DeepSpeed/tests/unit/inference/test_inference.py:         multiprocessing Pool to increase performance. Since we use a Pool for
./DeepSpeed/tests/unit/inference/test_inference.py:         our distributed tests and cannot nest Pools, we must redefine and
./DeepSpeed/tests/unit/inference/test_inference.py:         patch this function with a version that does not use Pool.
./DeepSpeed/tests/unit/inference/test_inference.py:        assert ds_time <= bs_time
./DeepSpeed/tests/unit/inference/test_inference_config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/test_inference_config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/test_inference_config.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/test_stable_diffusion.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/test_stable_diffusion.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/test_stable_diffusion.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/test_stable_diffusion.py: Setup for these models is different from other pipelines, so we add a separate test
./DeepSpeed/tests/unit/inference/test_stable_diffusion.py:         RMSE threshold value is arbitrary, may need to adjust as needed
./DeepSpeed/tests/unit/inference/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/__init__.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/quantization/test_intX_quantization.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/quantization/test_intX_quantization.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/quantization/test_intX_quantization.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/quantization/test_intX_quantization.py:     This threshold value is emperically selected.
./DeepSpeed/tests/unit/inference/quantization/test_intX_quantization.py:     This threshold value is emperically selected.
./DeepSpeed/tests/unit/inference/quantization/test_intX_quantization.py:     This threshold value is emperically selected.
./DeepSpeed/tests/unit/inference/quantization/test_intX_quantization.py:             This threshold value is emperically selected.
./DeepSpeed/tests/unit/inference/v2/kernels/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/__init__.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_moe_scatter.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_moe_scatter.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_moe_scatter.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_moe_scatter.py:     Sequence composition shouldn't matter here
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_moe_scatter.py:     This will make each token's value equal to its index. NOTE: This will break for
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_moe_scatter.py:     tokens with index > 2048.
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_moe_scatter.py:     Gating outputs
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_moe_scatter.py:     Scatter outputs
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_blocked_flash.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_blocked_flash.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_blocked_flash.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/__init__.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_ragged_embed.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_ragged_embed.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_ragged_embed.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_ragged_embed.py:         Positional embeddings aren't padded because it's simulated
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_ragged_embed.py:     Heads/Block size are irrelevant here but need something.
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_top_1_gating.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_top_1_gating.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_top_1_gating.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_blocked_kv_copy.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_blocked_kv_copy.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_blocked_kv_copy.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_blocked_rotary_emb.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_blocked_rotary_emb.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_blocked_rotary_emb.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/ragged_testing_utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/ragged_testing_utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/ragged_testing_utils.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/ragged_testing_utils.py:     At the beginning of operation, the design of the allocator is such that it will return
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/ragged_testing_utils.py:     linear blocks of memory. The following will "warm up" the allocator so that we can be
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/ragged_testing_utils.py:     more certain that code is not dependent on this behavior.
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/ragged_testing_utils.py:             Create empty descriptor
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/ragged_testing_utils.py:             Update `seen_tokens` in the descriptor
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/ragged_testing_utils.py:             Ensure there's enough KV-cache for the sequence
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/ragged_testing_utils.py:             Insert sequence into batch
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/ragged_testing_utils.py:             Create empty descriptor
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/ragged_testing_utils.py:             Update `seen_tokens` in the descriptor
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/ragged_testing_utils.py:             Ensure there's enough KV-cache for the sequence
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/ragged_testing_utils.py:             We will skip KV cache allocation here because we did a lump allocation above
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/ragged_testing_utils.py:             for both the fill and the sequence itself.
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_moe_gather.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_moe_gather.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_moe_gather.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_moe_gather.py:     Sequence composition shouldn't matter here
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_moe_gather.py:     This will make each token's value equal to its index. NOTE: This will break for
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_moe_gather.py:     tokens with index > 2048.
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_moe_gather.py:     Gating outputs
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_moe_gather.py:     Scatter outputs
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_atom_builder.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_atom_builder.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_atom_builder.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_atom_builder.py:         Since the ptr was 0, first 2 elements should be 0
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_atom_builder.py:         Since we have a single sequence, the q_start_idx should always be
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_atom_builder.py:         whichever atom we're on multiplied by the block size
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_logits_gather.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_logits_gather.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/ragged_ops/test_logits_gather.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/cutlass_ops/test_moe_gemm.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/cutlass_ops/test_moe_gemm.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/cutlass_ops/test_moe_gemm.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/cutlass_ops/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/cutlass_ops/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/cutlass_ops/__init__.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_pre_ln.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_pre_ln.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_pre_ln.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_pre_ln.py:     Input vals
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_pre_ln.py:     Reference output
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_pre_ln.py:     New output
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_pre_ln.py:     Check
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_bias_activation.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_bias_activation.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_bias_activation.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_bias_activation.py:     Input vals
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_bias_activation.py:     Reference output
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_bias_activation.py:     New output
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_bias_activation.py:     Check
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/__init__.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_rms_norm.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_rms_norm.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_rms_norm.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_gated_activation.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_gated_activation.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_gated_activation.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_gated_activation.py:     Reference output
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_gated_activation.py:     Build kernel
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_gated_activation.py:     New output
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_gated_activation.py:     Check
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_gated_activation.py:     Reference output
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_gated_activation.py:     New output
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_gated_activation.py:     Reference output
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_gated_activation.py:     New output
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_post_ln.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_post_ln.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_post_ln.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_post_ln.py:     Input vals
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_post_ln.py:     Reference output
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_post_ln.py:     New output
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_post_ln.py:     Check
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_blas_linear.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_blas_linear.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_blas_linear.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_blas_linear.py: Note: only testing with FP16 and BF16 because we use TF32 on Ampere and we don't have a good
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_blas_linear.py: set of tolerances. Since this is just on top of BLAS though, the test is more about
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_blas_linear.py: making sure the stride/contiguity is correct and that's data type agnostic.
./DeepSpeed/tests/unit/inference/v2/kernels/core_ops/test_blas_linear.py:     Transpose the weights then revert to the format we expect.
./DeepSpeed/tests/unit/inference/v2/model_implementations/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/model_implementations/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/model_implementations/__init__.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/model_implementations/sharding/test_qkv_sharding.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/model_implementations/sharding/test_qkv_sharding.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/model_implementations/sharding/test_qkv_sharding.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/model_implementations/sharding/test_qkv_sharding.py:     Correct shape is 1536 (=16 * 64 + 2 * 4 * 64), 1024
./DeepSpeed/tests/unit/inference/v2/model_implementations/sharding/test_mlp_sharding.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/model_implementations/sharding/test_mlp_sharding.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/model_implementations/sharding/test_mlp_sharding.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/model_implementations/sharding/test_attn_out_sharding.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/model_implementations/sharding/test_attn_out_sharding.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/model_implementations/sharding/test_attn_out_sharding.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/model_implementations/sharding/test_attn_out_sharding.py: None of the logic should be dependent on head size.
./DeepSpeed/tests/unit/inference/v2/model_implementations/sharding/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/model_implementations/sharding/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/model_implementations/sharding/__init__.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/__init__.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_multi_parameter_layer.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_multi_parameter_layer.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_multi_parameter_layer.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/utils.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_layer_inheritance.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_layer_inheritance.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_layer_inheritance.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_contiguify.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_contiguify.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_contiguify.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_contiguify.py:     Create parameters and populate them into the containers
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_contiguify.py:     Validate containers before contiguify
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_contiguify.py:     Validate restore pass
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_parameter_list.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_parameter_list.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_parameter_list.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_parameter_list.py:     Set the first expert
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_parameter_list.py:     Set the second expert
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_parameter_list.py:     We have all the experts, so the layer should be initialized
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_mapping.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_mapping.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_mapping.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_mapping.py:     We want to check into double digits to make sure that this isn't specific
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_mapping.py:     to single difit indexing.
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_mapping.py:     The single parameter setting should immediately make the parameter finalized
./DeepSpeed/tests/unit/inference/v2/model_implementations/parameters/test_mapping.py:     and the whole layer initialized.
./DeepSpeed/tests/unit/inference/v2/ragged/test_ragged_wrapper.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/ragged/test_ragged_wrapper.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/ragged/test_ragged_wrapper.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/ragged/test_blocked_allocator.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/ragged/test_blocked_allocator.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/ragged/test_blocked_allocator.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/ragged/test_blocked_allocator.py:     Test that we can't allocate more blocks than we have.
./DeepSpeed/tests/unit/inference/v2/ragged/test_blocked_allocator.py:     Allocate
./DeepSpeed/tests/unit/inference/v2/ragged/test_blocked_allocator.py:     Deallocate all blocks
./DeepSpeed/tests/unit/inference/v2/ragged/test_blocked_allocator.py:     Get all the blocks again
./DeepSpeed/tests/unit/inference/v2/ragged/test_blocked_allocator.py:     Deallocate in chunks
./DeepSpeed/tests/unit/inference/v2/ragged/test_blocked_allocator.py:     Block 0 should not be freed if passed with an invalid index.
./DeepSpeed/tests/unit/inference/v2/ragged/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/ragged/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/ragged/__init__.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/ragged/test_manager_configs.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/ragged/test_manager_configs.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/ragged/test_manager_configs.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/inference_test_utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/inference_test_utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/inference_test_utils.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/inference_test_utils.py:             Note: BF16 tolerance is higher than FP16 because of the lower precision (7 (+1) bits vs
./DeepSpeed/tests/unit/inference/v2/inference_test_utils.py:             10 (+1) bits)
./DeepSpeed/tests/unit/inference/v2/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/__init__.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/modules/test_custom_module.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/modules/test_custom_module.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/modules/test_custom_module.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/modules/test_custom_module.py:     Input vals
./DeepSpeed/tests/unit/inference/v2/modules/test_custom_module.py:     Reference output
./DeepSpeed/tests/unit/inference/v2/modules/test_custom_module.py:     New output
./DeepSpeed/tests/unit/inference/v2/modules/test_custom_module.py:     Check
./DeepSpeed/tests/unit/inference/v2/modules/test_blocked_attn.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/modules/test_blocked_attn.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/modules/test_blocked_attn.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/modules/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/modules/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/modules/__init__.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/modules/test_blas_linear_module.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/modules/test_blas_linear_module.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/modules/test_blas_linear_module.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/modules/test_blas_linear_module.py:     Input vals
./DeepSpeed/tests/unit/inference/v2/modules/test_blas_linear_module.py:     Reference output
./DeepSpeed/tests/unit/inference/v2/modules/test_blas_linear_module.py:     New output
./DeepSpeed/tests/unit/inference/v2/modules/test_blas_linear_module.py:     Check
./DeepSpeed/tests/unit/inference/v2/modules/test_post_ln_module.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/modules/test_post_ln_module.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/modules/test_post_ln_module.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/modules/test_post_ln_module.py:     Input vals
./DeepSpeed/tests/unit/inference/v2/modules/test_post_ln_module.py:     Reference output
./DeepSpeed/tests/unit/inference/v2/modules/test_post_ln_module.py:     New output
./DeepSpeed/tests/unit/inference/v2/modules/test_post_ln_module.py:     Check
./DeepSpeed/tests/unit/inference/v2/modules/test_pre_rms_module.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/modules/test_pre_rms_module.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/modules/test_pre_rms_module.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/modules/test_pre_rms_module.py:     Input vals
./DeepSpeed/tests/unit/inference/v2/modules/test_pre_rms_module.py:     Reference output
./DeepSpeed/tests/unit/inference/v2/modules/test_pre_rms_module.py:     New output
./DeepSpeed/tests/unit/inference/v2/modules/test_pre_rms_module.py:     Check
./DeepSpeed/tests/unit/inference/v2/modules/test_cuda_pre_ln_module.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/modules/test_cuda_pre_ln_module.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/modules/test_cuda_pre_ln_module.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/modules/test_cuda_pre_ln_module.py:     Input vals
./DeepSpeed/tests/unit/inference/v2/modules/test_cuda_pre_ln_module.py:     Reference output
./DeepSpeed/tests/unit/inference/v2/modules/test_cuda_pre_ln_module.py:     New output
./DeepSpeed/tests/unit/inference/v2/modules/test_cuda_pre_ln_module.py:     Check
./DeepSpeed/tests/unit/inference/v2/modules/test_cutlass_moe.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/v2/modules/test_cutlass_moe.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/v2/modules/test_cutlass_moe.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/v2/modules/test_cutlass_moe.py:     Parameters
./DeepSpeed/tests/unit/inference/v2/modules/test_cutlass_moe.py:         Input vals
./DeepSpeed/tests/unit/inference/v2/modules/test_cutlass_moe.py:         Reference implementation
./DeepSpeed/tests/unit/inference/v2/modules/test_cutlass_moe.py:         Increase the tolerance for larger meta ops since the error is additive
./DeepSpeed/tests/unit/inference/test_checkpoint_sharding.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/test_checkpoint_sharding.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/test_checkpoint_sharding.py: DeepSpeed Team
./DeepSpeed/tests/unit/inference/test_checkpoint_sharding.py:         Only write a checkpoint if one does not exist
./DeepSpeed/tests/unit/inference/test_checkpoint_sharding.py:             Load model and save sharded checkpoint
./DeepSpeed/tests/unit/inference/test_checkpoint_sharding.py:         Load model on meta tensors
./DeepSpeed/tests/unit/inference/test_checkpoint_sharding.py:         Note that we use half precision to load initially, even for int8
./DeepSpeed/tests/unit/inference/test_checkpoint_sharding.py:                 download only on first process
./DeepSpeed/tests/unit/inference/test_checkpoint_sharding.py:         Load model on meta tensors
./DeepSpeed/tests/unit/inference/test_checkpoint_sharding.py:         Note that we use half precision to load initially, even for int8
./DeepSpeed/tests/unit/inference/test_model_profiling.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/inference/test_model_profiling.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/inference/test_model_profiling.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:            l = 0
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:            num_layers = len(self.layer)
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:            chunk_length = math.ceil(math.sqrt(num_layers))
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:            while l < num_layers:
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:                hidden_states = checkpoint.checkpoint(
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:                    custom(
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:                        l,  # noqa: F821
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:                        l + chunk_length),
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:                    hidden_states,
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:                    attention_mask * 1)
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:                l += chunk_length
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:             decoder layers
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:     prepare test data
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:     run baseline
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:     run ds
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:     check forward evaluation
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py: FP16 test cases can only run on the devices support FP16.
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:                             (8,2048,2048,32,1,True,True),
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:         Only run fp16 test cases on devices with FP16 capability.
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:         Only run fp16 test cases on devices with FP16 capability.
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:                             (64,1024,128,16,3,True,False),
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:                             (64,1024,128,16,3,True,True),
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:                             (64,1024,128,16,3,False,False),
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:                             (64,1024,128,16,3,False,True),
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_forward.py:         Only run fp16 test cases on devices with FP16 capability.
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:if not deepspeed.ops.__installed_ops__['transformer']:
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:pytest.skip(
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:    "transformer kernels are temporarily disabled because of unexplained failures",
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:    allow_module_level=True)
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:        toler = np.linalg.norm(x.astype('float64')) * 0.0005
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:            l = 0
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:            num_layers = len(self.layer)
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:            chunk_length = math.ceil(math.sqrt(num_layers))
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:            while l < num_layers:
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:                hidden_states = checkpoint.checkpoint(
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:                    custom(
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:                        l,  # noqa: F821
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:                        l + chunk_length),
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:                    hidden_states,
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:                    attention_mask * 1)
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:                l += chunk_length
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:             decoder layers
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:     prepare test data
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:     run baseline
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:     run ds
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:     check grads
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py: NOTE: Keep these different params as they have helped find divergence in behavior between AMD and NVIDIA.
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:        This is to flush denorms in forward pass. Please refer to https://github.com/pytorch/pytorch/blob/main/docs/source/notes/numerical_accuracy.rst#reduced-precision-fp16-and-bf16-gemms-and-convolutions-on-amd-instinct-mi200-devices
./DeepSpeed/tests/unit/ops/accelerators/test_accelerator_backward.py:         Only run fp16 test cases on devices with FP16 capability.
./DeepSpeed/tests/unit/ops/sparse_attention/test_sparse_attention.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/sparse_attention/test_sparse_attention.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/sparse_attention/test_sparse_attention.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/sparse_attention/test_sparse_attention.py: DeepSpeed note, some parts of code taken & adapted from commit c368a9fd1b2c9dee4cc94de9a6bb0be3d447be41
./DeepSpeed/tests/unit/ops/sparse_attention/test_sparse_attention.py: https://github.com/ptillet/torch-blocksparse/blob/master/tests/test_softmax.py
./DeepSpeed/tests/unit/ops/sparse_attention/test_sparse_attention.py: https://github.com/ptillet/torch-blocksparse/blob/master/tests/test_matmul.py
./DeepSpeed/tests/unit/ops/sparse_attention/test_sparse_attention.py: https://github.com/ptillet/torch-blocksparse/blob/master/tests/utils
./DeepSpeed/tests/unit/ops/sparse_attention/test_sparse_attention.py:                    maskedw[wz, wh, wi : wi+block, wj : wj+block] *= mask[bh, bi, bj]
./DeepSpeed/tests/unit/ops/deepspeed4science/test_DS4Sci_EvoformerAttention.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/deepspeed4science/test_DS4Sci_EvoformerAttention.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/deepspeed4science/test_DS4Sci_EvoformerAttention.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/transformer/inference/test_layer_norm.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/transformer/inference/test_layer_norm.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/transformer/inference/test_layer_norm.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/transformer/inference/test_layer_norm.py:        print(new_output - ref_output)
./DeepSpeed/tests/unit/ops/transformer/inference/test_layer_norm.py:     Need to run the reference first since there's an in-place component to ours
./DeepSpeed/tests/unit/ops/transformer/inference/test_layer_norm.py:     create data
./DeepSpeed/tests/unit/ops/transformer/inference/test_layer_norm.py:     forward pass
./DeepSpeed/tests/unit/ops/transformer/inference/test_layer_norm.py:     compare
./DeepSpeed/tests/unit/ops/transformer/inference/test_residual_add.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/transformer/inference/test_residual_add.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/transformer/inference/test_residual_add.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/transformer/inference/test_residual_add.py:         Residual add, as a sequence of casted additions, currently requires a higher tolerance
./DeepSpeed/tests/unit/ops/transformer/inference/test_residual_add.py:         than the other operators for FP16. We should instead better align the behaviors
./DeepSpeed/tests/unit/ops/transformer/inference/test_residual_add.py:         of the reference to match our kernel implementation (TODO(cmikeh2))
./DeepSpeed/tests/unit/ops/transformer/inference/test_residual_add.py:             Note: BF16 tolerance is higher than FP16 because of the lower precision (7 (+1) bits vs
./DeepSpeed/tests/unit/ops/transformer/inference/test_residual_add.py:             10 (+1) bits)
./DeepSpeed/tests/unit/ops/transformer/inference/inference_test_utils.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/transformer/inference/inference_test_utils.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/transformer/inference/inference_test_utils.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/transformer/inference/inference_test_utils.py:             Note: BF16 tolerance is higher than FP16 because of the lower precision (7 (+1) bits vs
./DeepSpeed/tests/unit/ops/transformer/inference/inference_test_utils.py:             10 (+1) bits)
./DeepSpeed/tests/unit/ops/transformer/inference/test_moe_res_matmult.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/transformer/inference/test_moe_res_matmult.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/transformer/inference/test_moe_res_matmult.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/transformer/inference/test_bias_gelu.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/transformer/inference/test_bias_gelu.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/transformer/inference/test_bias_gelu.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/transformer/inference/test_bias_gelu.py:     Expected behavior is that of casting to float32 internally and using the tanh approximation
./DeepSpeed/tests/unit/ops/transformer/inference/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/transformer/inference/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/transformer/inference/__init__.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/transformer/inference/test_gelu.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/transformer/inference/test_gelu.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/transformer/inference/test_gelu.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/transformer/inference/test_gelu.py:     If torch version = 1.12
./DeepSpeed/tests/unit/ops/transformer/inference/test_gelu.py:     Expected behavior is that of casting to float32 internally and using the tanh approximation
./DeepSpeed/tests/unit/ops/transformer/inference/test_bias_relu.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/transformer/inference/test_bias_relu.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/transformer/inference/test_bias_relu.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/transformer/inference/test_bias_relu.py:     Expected behavior is that of casting to float32 internally
./DeepSpeed/tests/unit/ops/transformer/inference/test_matmul.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/transformer/inference/test_matmul.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/transformer/inference/test_matmul.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/transformer/inference/test_matmul.py:     skip autotune in testing
./DeepSpeed/tests/unit/ops/transformer/inference/test_rms_norm.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/transformer/inference/test_rms_norm.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/transformer/inference/test_rms_norm.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/transformer/inference/test_rms_norm.py:    assert allclose(new_output[1], ref_output[1])
./DeepSpeed/tests/unit/ops/transformer/inference/test_attention.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/transformer/inference/test_attention.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/transformer/inference/test_attention.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/transformer/inference/test_attention.py: reference timplementation
./DeepSpeed/tests/unit/ops/transformer/inference/test_attention.py: test attention operator
./DeepSpeed/tests/unit/ops/transformer/inference/test_attention.py:     skip autotune in testing
./DeepSpeed/tests/unit/ops/transformer/inference/test_attention.py:     reference implementation
./DeepSpeed/tests/unit/ops/transformer/inference/test_attention.py:     adjust it to expected tensor format and run test
./DeepSpeed/tests/unit/ops/transformer/inference/test_softmax.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/transformer/inference/test_softmax.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/transformer/inference/test_softmax.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/transformer/inference/test_softmax.py:         return torch.empty_like(input)
./DeepSpeed/tests/unit/ops/transformer/inference/test_bias_add.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/transformer/inference/test_bias_add.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/transformer/inference/test_bias_add.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/transformer/inference/test_bias_geglu.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/transformer/inference/test_bias_geglu.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/transformer/inference/test_bias_geglu.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/transformer/inference/test_bias_geglu.py:     Expected behavior is that of casting to float32 internally
./DeepSpeed/tests/unit/ops/transformer/inference/test_bias_geglu.py:     Explicitly using the default GeLU
./DeepSpeed/tests/unit/ops/transformer/inference/test_bias_geglu.py:     Expected behavior is that of casting to float32 internally
./DeepSpeed/tests/unit/ops/transformer/inference/test_bias_geglu.py:     Explicitly using the default GeLU
./DeepSpeed/tests/unit/ops/aio/test_aio.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/aio/test_aio.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/aio/test_aio.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/adagrad/test_cpu_adagrad.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/adagrad/test_cpu_adagrad.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/adagrad/test_cpu_adagrad.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/spatial/test_nhwc_bias_add.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/spatial/test_nhwc_bias_add.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/spatial/test_nhwc_bias_add.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/quantizer/test_fake_quantization.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/quantizer/test_fake_quantization.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/quantizer/test_fake_quantization.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/quantizer/test_fake_quantization.py:     quantize
./DeepSpeed/tests/unit/ops/quantizer/test_fake_quantization.py:     dequantize
./DeepSpeed/tests/unit/ops/quantizer/test_fake_quantization.py: Test with two tensor shapes as (16, 4096) and (128, 256).
./DeepSpeed/tests/unit/ops/quantizer/test_fake_quantization.py: Test with number of quant groups as 1 and 16.
./DeepSpeed/tests/unit/ops/quantizer/test_fake_quantization.py: Note that we have an explicit boundary for groups as ((size / groups) - 1) / 4096 + 1) <= MAX_REG.
./DeepSpeed/tests/unit/ops/quantizer/test_fake_quantization.py:     8-bit quantization.
./DeepSpeed/tests/unit/ops/quantizer/test_fake_quantization.py:     run_quant_dequant will do quantize then dequantize, and return the dequantized value.
./DeepSpeed/tests/unit/ops/quantizer/test_fake_quantization.py:     4-bit quantization.
./DeepSpeed/tests/unit/ops/quantizer/test_quantize.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/quantizer/test_quantize.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/quantizer/test_quantize.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/quantizer/test_quantize.py:     Reference implementation
./DeepSpeed/tests/unit/ops/quantizer/test_quantize.py:     https://pytorch.org/docs/stable/quantization-support.html
./DeepSpeed/tests/unit/ops/quantizer/test_quantize.py:     fix seed
./DeepSpeed/tests/unit/ops/quantizer/test_quantize.py:     we need to convert the tensor to float64 to avoid overflow
./DeepSpeed/tests/unit/ops/adam/test_adamw.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/adam/test_adamw.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/adam/test_adamw.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/adam/test_adamw.py: yapf: disable
./DeepSpeed/tests/unit/ops/adam/test_adamw.py:'optimizer, zero_offload, torch_adam, adam_w_mode, resulting_optimizer
./DeepSpeed/tests/unit/ops/adam/test_adamw.py:         get base optimizer under zero
./DeepSpeed/tests/unit/ops/adam/test_hybrid_adam.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/adam/test_hybrid_adam.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/adam/test_hybrid_adam.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/adam/test_cpu_adam.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/adam/test_cpu_adam.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/adam/test_cpu_adam.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/adam/test_cpu_adam.py:                             (55),
./DeepSpeed/tests/unit/ops/adam/test_cpu_adam.py:         tolerance = cpu_param.float().norm().detach().numpy() * 1e-2
./DeepSpeed/tests/unit/ops/adam/test_cpu_adam.py:         check_equal(cpu_param.float().norm(),
./DeepSpeed/tests/unit/ops/adam/test_cpu_adam.py:                     cuda_param.float().cpu().norm(),
./DeepSpeed/tests/unit/ops/adam/test_cpu_adam.py:                     atol=tolerance,
./DeepSpeed/tests/unit/ops/adam/test_cpu_adam.py:                     verbose=True)
./DeepSpeed/tests/unit/ops/lion/test_lion.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/lion/test_lion.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/lion/test_lion.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/lion/test_lion.py: yapf: disable
./DeepSpeed/tests/unit/ops/lion/test_lion.py:'optimizer, zero_offload, resulting_optimizer
./DeepSpeed/tests/unit/ops/lion/test_lion.py:         get base optimizer under zero
./DeepSpeed/tests/unit/ops/lion/test_cpu_lion.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/ops/lion/test_cpu_lion.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/ops/lion/test_cpu_lion.py: DeepSpeed Team
./DeepSpeed/tests/unit/ops/lion/test_cpu_lion.py:                             (55),
./DeepSpeed/tests/unit/multi_output_model.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/multi_output_model.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/multi_output_model.py: DeepSpeed Team
./DeepSpeed/tests/unit/hybrid_engine/test_he_lora.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/hybrid_engine/test_he_lora.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/hybrid_engine/test_he_lora.py: DeepSpeed Team
./DeepSpeed/tests/unit/hybrid_engine/test_he_lora.py:     an simple implementation of LoRA
./DeepSpeed/tests/unit/hybrid_engine/test_he_lora.py:     for now only support Linear Layer
./DeepSpeed/tests/unit/hybrid_engine/test_he_lora.py:             for zero stage 3
./DeepSpeed/tests/unit/hybrid_engine/test_he_lora.py:         disable the original weight gradient
./DeepSpeed/tests/unit/hybrid_engine/test_he_lora.py:         fuse LoRA to the original weight
./DeepSpeed/tests/unit/hybrid_engine/test_he_lora.py:     turn off the gradient of all the parameters except the LoRA parameters
./DeepSpeed/tests/unit/hybrid_engine/test_he_lora.py:         Inject LoRA
./DeepSpeed/tests/unit/hybrid_engine/test_he_lora.py:         Verify gradient norm is larger than 0
./DeepSpeed/tests/unit/hybrid_engine/test_he_lora.py:         Verify parameter remains the same
./DeepSpeed/tests/unit/hybrid_engine/test_he_lora.py:         Verify fuse will mutate layer_params
./DeepSpeed/tests/unit/hybrid_engine/test_he_all.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/hybrid_engine/test_he_all.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/hybrid_engine/test_he_all.py: DeepSpeed Team
./DeepSpeed/tests/unit/hybrid_engine/test_he_llama.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/hybrid_engine/test_he_llama.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/hybrid_engine/test_he_llama.py: DeepSpeed Team
./DeepSpeed/tests/unit/hybrid_engine/test_he_llama.py:        output = model.generate(**tokens, do_sample=False, max_length=100)
./DeepSpeed/tests/unit/hybrid_engine/test_he_llama.py:         Make the model smaller so we can run it on a single GPU in CI
./DeepSpeed/tests/unit/modeling.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/modeling.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/modeling.py: DeepSpeed Team
./DeepSpeed/tests/unit/modeling.py: Copyright The Microsoft DeepSpeed Team
./DeepSpeed/tests/unit/modeling.py: DeepSpeed note, code taken from commit 3d59216cec89a363649b4fe3d15295ba936ced0f
./DeepSpeed/tests/unit/modeling.py: https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/modeling.py
./DeepSpeed/tests/unit/modeling.py: coding=utf-8
./DeepSpeed/tests/unit/modeling.py: Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.
./DeepSpeed/tests/unit/modeling.py: Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
./DeepSpeed/tests/unit/modeling.py:
./DeepSpeed/tests/unit/modeling.py: Licensed under the Apache License, Version 2.0 (the "License");
./DeepSpeed/tests/unit/modeling.py: you may not use this file except in compliance with the License.
./DeepSpeed/tests/unit/modeling.py: You may obtain a copy of the License at
./DeepSpeed/tests/unit/modeling.py:
./DeepSpeed/tests/unit/modeling.py:     http://www.apache.org/licenses/LICENSE-2.0
./DeepSpeed/tests/unit/modeling.py:
./DeepSpeed/tests/unit/modeling.py: Unless required by applicable law or agreed to in writing, software
./DeepSpeed/tests/unit/modeling.py: distributed under the License is distributed on an "AS IS" BASIS,
./DeepSpeed/tests/unit/modeling.py: WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
./DeepSpeed/tests/unit/modeling.py: See the License for the specific language governing permissions and
./DeepSpeed/tests/unit/modeling.py: limitations under the License.
./DeepSpeed/tests/unit/modeling.py:from numba import cuda
./DeepSpeed/tests/unit/modeling.py:from deepspeed_cuda import DeepSpeedSoftmaxConfig, DeepSpeedSoftmax
./DeepSpeed/tests/unit/modeling.py:        self.weight = Parameter(torch.Tensor(out_features, in_features))
./DeepSpeed/tests/unit/modeling.py:        if bias:
./DeepSpeed/tests/unit/modeling.py:            self.bias = Parameter(torch.Tensor(out_features))
./DeepSpeed/tests/unit/modeling.py:        else:
./DeepSpeed/tests/unit/modeling.py:            self.register_parameter('bias', None)
./DeepSpeed/tests/unit/modeling.py:        self.reset_parameters()
./DeepSpeed/tests/unit/modeling.py:            timing = []
./DeepSpeed/tests/unit/modeling.py:            t1 = GPUTimer()
./DeepSpeed/tests/unit/modeling.py:            t1.record()
./DeepSpeed/tests/unit/modeling.py:            timing.append(t1.elapsed())
./DeepSpeed/tests/unit/modeling.py:            t1.record()
./DeepSpeed/tests/unit/modeling.py:            timing.append(t1.elapsed())
./DeepSpeed/tests/unit/modeling.py:    apex.amp.register_half_function(apex.normalization.fused_layer_norm, 'FusedLayerNorm')
./DeepSpeed/tests/unit/modeling.py:    apex.amp.register_float_function(apex.normalization.FusedLayerNorm, 'forward')
./DeepSpeed/tests/unit/modeling.py:        self.softmax_config = DeepSpeedSoftmaxConfig()
./DeepSpeed/tests/unit/modeling.py:        self.softmax_config.batch_size = config.batch_size
./DeepSpeed/tests/unit/modeling.py:        self.softmax_config.max_seq_length = config.max_position_embeddings
./DeepSpeed/tests/unit/modeling.py:        self.softmax_config.hidden_size = config.hidden_size
./DeepSpeed/tests/unit/modeling.py:        self.softmax_config.heads = config.num_attention_heads
./DeepSpeed/tests/unit/modeling.py:        self.softmax_config.softmax_id = i
./DeepSpeed/tests/unit/modeling.py:        self.softmax_config.fp16 = config.fp16
./DeepSpeed/tests/unit/modeling.py:        self.softmax_config.prob_drop_out = 0.0
./DeepSpeed/tests/unit/modeling.py:        self.softmax = DeepSpeedSoftmax(i, self.softmax_config)
./DeepSpeed/tests/unit/modeling.py:         This is actually dropping out entire tokens to attend to, which might
./DeepSpeed/tests/unit/modeling.py:         seem a bit unusual, but is taken from the original Transformer paper.
./DeepSpeed/tests/unit/modeling.py:             self.weight[0].register_hook(lambda x, self=self: grads.append([x,"Q_W"]))
./DeepSpeed/tests/unit/modeling.py:             self.biases[0].register_hook(lambda x, self=self: grads.append([x,"Q_B"]))
./DeepSpeed/tests/unit/modeling.py:             self.weight[1].register_hook(lambda x, self=self: grads.append([x,"K_W"]))
./DeepSpeed/tests/unit/modeling.py:             self.biases[1].register_hook(lambda x, self=self: grads.append([x,"K_B"]))
./DeepSpeed/tests/unit/modeling.py:        layer = BertLayer(config, weights, biases)
./DeepSpeed/tests/unit/modeling.py:             decoder layers
./DeepSpeed/tests/unit/modeling.py:                print("pytorch weight is: ", layer_module.get_w())
./DeepSpeed/tests/unit/autotuning/test_autotuning.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/autotuning/test_autotuning.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/autotuning/test_autotuning.py: DeepSpeed Team
./DeepSpeed/tests/unit/moe/test_moe.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/moe/test_moe.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/moe/test_moe.py: DeepSpeed Team
./DeepSpeed/tests/unit/moe/test_moe.py:         E+D -- ep_size = 2
./DeepSpeed/tests/unit/moe/test_moe.py:         E only -- ep_size = 4
./DeepSpeed/tests/unit/moe/test_moe.py:        dist_init_required=False -- parameterize to True/False?
./DeepSpeed/tests/unit/moe/test_moe.py:                 Get all partition ids + their offsets
./DeepSpeed/tests/unit/moe/test_moe.py:                 Calculate rank and offsets for grad slices
./DeepSpeed/tests/unit/moe/test_moe.py:                     Calculate numel for grad slice depending on partition location
./DeepSpeed/tests/unit/moe/test_moe.py:                         Last partition_id uses its own offset
./DeepSpeed/tests/unit/moe/test_moe.py:                         Set numel to next partition's offset
./DeepSpeed/tests/unit/moe/test_moe.py:         E+D -- ep_size = 2
./DeepSpeed/tests/unit/moe/test_moe.py:         E only -- ep_size = 4
./DeepSpeed/tests/unit/moe/test_moe_tp.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/moe/test_moe_tp.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/moe/test_moe_tp.py: DeepSpeed Team
./DeepSpeed/tests/unit/moe/test_moe_tp.py:         TODO: replace this with a true parallel mlp in the future
./DeepSpeed/tests/unit/moe/test_moe_tp.py:         and run convergence tests
./DeepSpeed/tests/unit/moe/test_moe_tp.py:         set num experts to world size
./DeepSpeed/tests/unit/modelingpreln.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/unit/modelingpreln.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/unit/modelingpreln.py: DeepSpeed Team
./DeepSpeed/tests/unit/modelingpreln.py: Copyright The Microsoft DeepSpeed Team
./DeepSpeed/tests/unit/modelingpreln.py: DeepSpeed note, code taken from commit 3d59216cec89a363649b4fe3d15295ba936ced0f
./DeepSpeed/tests/unit/modelingpreln.py: https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/modeling.py
./DeepSpeed/tests/unit/modelingpreln.py: coding=utf-8
./DeepSpeed/tests/unit/modelingpreln.py: Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.
./DeepSpeed/tests/unit/modelingpreln.py: Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
./DeepSpeed/tests/unit/modelingpreln.py:
./DeepSpeed/tests/unit/modelingpreln.py: Licensed under the Apache License, Version 2.0 (the "License");
./DeepSpeed/tests/unit/modelingpreln.py: you may not use this file except in compliance with the License.
./DeepSpeed/tests/unit/modelingpreln.py: You may obtain a copy of the License at
./DeepSpeed/tests/unit/modelingpreln.py:
./DeepSpeed/tests/unit/modelingpreln.py:     http://www.apache.org/licenses/LICENSE-2.0
./DeepSpeed/tests/unit/modelingpreln.py:
./DeepSpeed/tests/unit/modelingpreln.py: Unless required by applicable law or agreed to in writing, software
./DeepSpeed/tests/unit/modelingpreln.py: distributed under the License is distributed on an "AS IS" BASIS,
./DeepSpeed/tests/unit/modelingpreln.py: WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
./DeepSpeed/tests/unit/modelingpreln.py: See the License for the specific language governing permissions and
./DeepSpeed/tests/unit/modelingpreln.py: limitations under the License.
./DeepSpeed/tests/unit/modelingpreln.py:        self.weight = Parameter(torch.Tensor(out_features, in_features))
./DeepSpeed/tests/unit/modelingpreln.py:        if bias:
./DeepSpeed/tests/unit/modelingpreln.py:            self.bias = Parameter(torch.Tensor(out_features))
./DeepSpeed/tests/unit/modelingpreln.py:        else:
./DeepSpeed/tests/unit/modelingpreln.py:            self.register_parameter('bias', None)
./DeepSpeed/tests/unit/modelingpreln.py:        self.reset_parameters()
./DeepSpeed/tests/unit/modelingpreln.py:            timing = []
./DeepSpeed/tests/unit/modelingpreln.py:            t1 = GPUTimer()
./DeepSpeed/tests/unit/modelingpreln.py:            t1.record()
./DeepSpeed/tests/unit/modelingpreln.py:            timing.append(t1.elapsed())
./DeepSpeed/tests/unit/modelingpreln.py:            t1.record()
./DeepSpeed/tests/unit/modelingpreln.py:            timing.append(t1.elapsed())
./DeepSpeed/tests/unit/modelingpreln.py:    apex.amp.register_half_function(apex.normalization.fused_layer_norm, 'FusedLayerNorm')
./DeepSpeed/tests/unit/modelingpreln.py:    apex.amp.register_float_function(apex.normalization.FusedLayerNorm, 'forward')
./DeepSpeed/tests/unit/modelingpreln.py:        def forward(self, x):
./DeepSpeed/tests/unit/modelingpreln.py:            u = x.mean(-1, keepdim=True)
./DeepSpeed/tests/unit/modelingpreln.py:            s = (x - u).pow(2).mean(-1, keepdim=True)
./DeepSpeed/tests/unit/modelingpreln.py:            x = (x - u) / torch.sqrt(s + self.variance_epsilon)
./DeepSpeed/tests/unit/modelingpreln.py:            return self.weight * x + self.bias
./DeepSpeed/tests/unit/modelingpreln.py:         self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
./DeepSpeed/tests/unit/modelingpreln.py:         any TensorFlow checkpoint file
./DeepSpeed/tests/unit/modelingpreln.py:        self.softmax_config = DeepSpeedSoftmaxConfig()
./DeepSpeed/tests/unit/modelingpreln.py:        self.softmax_config.batch_size = config.batch_size
./DeepSpeed/tests/unit/modelingpreln.py:        self.softmax_config.max_seq_length = config.max_position_embeddings
./DeepSpeed/tests/unit/modelingpreln.py:        self.softmax_config.hidden_size = config.hidden_size
./DeepSpeed/tests/unit/modelingpreln.py:        self.softmax_config.heads = config.num_attention_heads
./DeepSpeed/tests/unit/modelingpreln.py:        self.softmax_config.softmax_id = i
./DeepSpeed/tests/unit/modelingpreln.py:        self.softmax_config.fp16 = config.fp16
./DeepSpeed/tests/unit/modelingpreln.py:        self.softmax_config.prob_drop_out = 0.0
./DeepSpeed/tests/unit/modelingpreln.py:        self.softmax = DeepSpeedSoftmax(i, self.softmax_config)
./DeepSpeed/tests/unit/modelingpreln.py:        timing = []
./DeepSpeed/tests/unit/modelingpreln.py:        t1 = GPUTimer()
./DeepSpeed/tests/unit/modelingpreln.py:        t1.record()
./DeepSpeed/tests/unit/modelingpreln.py:        timing.append(t1.elapsed())
./DeepSpeed/tests/unit/modelingpreln.py:        print("Query elapsed: %s" % (time.clock() - start))
./DeepSpeed/tests/unit/modelingpreln.py:        t1.record()
./DeepSpeed/tests/unit/modelingpreln.py:        timing.append(t1.elapsed())
./DeepSpeed/tests/unit/modelingpreln.py:        print("Key elapsed: %s" % (time.clock() - start))
./DeepSpeed/tests/unit/modelingpreln.py:        t1.record()
./DeepSpeed/tests/unit/modelingpreln.py:        timing.append(t1.elapsed())
./DeepSpeed/tests/unit/modelingpreln.py:        print("Value elapsed: %s" % (time.clock() - start))
./DeepSpeed/tests/unit/modelingpreln.py:        t1.record()
./DeepSpeed/tests/unit/modelingpreln.py:         print(query_layer)
./DeepSpeed/tests/unit/modelingpreln.py:        timing.append(t1.elapsed())
./DeepSpeed/tests/unit/modelingpreln.py:        print("Query-Transform elapsed: %s" % (time.clock() - start))
./DeepSpeed/tests/unit/modelingpreln.py:        t1.record()
./DeepSpeed/tests/unit/modelingpreln.py:         print(key_layer)
./DeepSpeed/tests/unit/modelingpreln.py:        timing.append(t1.elapsed())
./DeepSpeed/tests/unit/modelingpreln.py:        print("Key-Transform elapsed: %s" % (time.clock() - start))
./DeepSpeed/tests/unit/modelingpreln.py:        t1.record()
./DeepSpeed/tests/unit/modelingpreln.py:        print(value_layer)
./DeepSpeed/tests/unit/modelingpreln.py:        timing.append(t1.elapsed())
./DeepSpeed/tests/unit/modelingpreln.py:        print("Value-Transform elapsed: %s" % (time.clock() - start))
./DeepSpeed/tests/unit/modelingpreln.py:         Take the dot product between "query" and "key" to get the raw attention scores.
./DeepSpeed/tests/unit/modelingpreln.py:        t1.record()
./DeepSpeed/tests/unit/modelingpreln.py:        print(query_layer.shape)
./DeepSpeed/tests/unit/modelingpreln.py:        print(key_layer.shape)
./DeepSpeed/tests/unit/modelingpreln.py:        print(attention_scores.shape)
./DeepSpeed/tests/unit/modelingpreln.py:        print("Pytorch: ", attention_scores)
./DeepSpeed/tests/unit/modelingpreln.py:        timing.append(t1.elapsed())
./DeepSpeed/tests/unit/modelingpreln.py:        print("Attention-Score elapsed: %s" % (time.clock() - start))
./DeepSpeed/tests/unit/modelingpreln.py:         Apply the attention mask is (precomputed for all layers in BertModel forward() function)
./DeepSpeed/tests/unit/modelingpreln.py:        t1.record()
./DeepSpeed/tests/unit/modelingpreln.py:         context_layer = self.softmax(query_layer, key_layer, value_layer, attention_mask)
./DeepSpeed/tests/unit/modelingpreln.py:        print("context shape is :", context_layer.shape)
./DeepSpeed/tests/unit/modelingpreln.py:        print("Cuda-ext:, ", attention_scores1)
./DeepSpeed/tests/unit/modelingpreln.py:         Normalize the attention scores to probabilities.
./DeepSpeed/tests/unit/modelingpreln.py:        ###attention_probs = self.softmax(attention_scores)
./DeepSpeed/tests/unit/modelingpreln.py:        timing.append(t1.elapsed())
./DeepSpeed/tests/unit/modelingpreln.py:        print("Softmax elapsed: %s" % (time.clock() - start))
./DeepSpeed/tests/unit/modelingpreln.py:        t1 = GPUTimer()
./DeepSpeed/tests/unit/modelingpreln.py:        t1.record()
./DeepSpeed/tests/unit/modelingpreln.py:        attention_scores = self.softmax(attention_scores, attention_mask)
./DeepSpeed/tests/unit/modelingpreln.py:        print("Softmax elapse {0:8.2f} ms", t1.elapsed() * 1000)
./DeepSpeed/tests/unit/modelingpreln.py:         This is actually dropping out entire tokens to attend to, which might
./DeepSpeed/tests/unit/modelingpreln.py:         seem a bit unusual, but is taken from the original Transformer paper.
./DeepSpeed/tests/unit/modelingpreln.py:        t1.record()
./DeepSpeed/tests/unit/modelingpreln.py:        timing.append(t1.elapsed())
./DeepSpeed/tests/unit/modelingpreln.py:        print("Context elapsed: %s" % (time.clock() - start))
./DeepSpeed/tests/unit/modelingpreln.py:        t1.record()
./DeepSpeed/tests/unit/modelingpreln.py:        context_layer1 = context_layer.permute(
./DeepSpeed/tests/unit/modelingpreln.py:                        0, 1, 3, 2, 4).contiguous()
./DeepSpeed/tests/unit/modelingpreln.py:        if grads is not None:
./DeepSpeed/tests/unit/modelingpreln.py:         context_layer.register_hook(lambda x, self = self : grads.append([x, "Context"]))
./DeepSpeed/tests/unit/modelingpreln.py:        timing.append(t1.elapsed())
./DeepSpeed/tests/unit/modelingpreln.py:        print("Context-Transform elapsed: %s" % (time.clock() - start))
./DeepSpeed/tests/unit/modelingpreln.py:        timing = []
./DeepSpeed/tests/unit/modelingpreln.py:        t1 = GPUTimer()
./DeepSpeed/tests/unit/modelingpreln.py:        t1.record()
./DeepSpeed/tests/unit/modelingpreln.py:        timing.append(t1.elapsed())
./DeepSpeed/tests/unit/modelingpreln.py:        print("Attention Output elapsed: %s" % (time.clock() - start))
./DeepSpeed/tests/unit/modelingpreln.py:        t1.record()
./DeepSpeed/tests/unit/modelingpreln.py:        hidden_states = self.LayerNorm(hidden_states + input_tensor)
./DeepSpeed/tests/unit/modelingpreln.py:        timing.append(t1.elapsed())
./DeepSpeed/tests/unit/modelingpreln.py:        print("LayerNorm elapsed: %s" % (time.clock() - start))
./DeepSpeed/tests/unit/modelingpreln.py:        timing = []
./DeepSpeed/tests/unit/modelingpreln.py:        t1 = GPUTimer()
./DeepSpeed/tests/unit/modelingpreln.py:        t1.record()
./DeepSpeed/tests/unit/modelingpreln.py:        print (hidden_states)
./DeepSpeed/tests/unit/modelingpreln.py:        print (self.dense.weight)
./DeepSpeed/tests/unit/modelingpreln.py:        timing.append(t1.elapsed())
./DeepSpeed/tests/unit/modelingpreln.py:        print("FF2 elapsed: %s" % (time.clock() - start))
./DeepSpeed/tests/unit/modelingpreln.py:        t1.record()
./DeepSpeed/tests/unit/modelingpreln.py:        hidden_states = self.LayerNorm(hidden_states + input_tensor)
./DeepSpeed/tests/unit/modelingpreln.py:        timing.append(t1.elapsed())
./DeepSpeed/tests/unit/modelingpreln.py:        print("LayerNorm elapsed: %s" % (time.clock() - start))
./DeepSpeed/tests/unit/modelingpreln.py:        print ("hidden shape is :", hidden_states.shape)
./DeepSpeed/tests/unit/modelingpreln.py:        attention_output = self.attention(hidden_states, attention_mask)
./DeepSpeed/tests/unit/modelingpreln.py:        intermediate_output = self.intermediate(attention_output)
./DeepSpeed/tests/unit/modelingpreln.py:        layer_output = self.output(intermediate_output, attention_output)
./DeepSpeed/tests/unit/modelingpreln.py:             self.weight[0].register_hook(lambda x, self=self: grads.append([x,"Q_W"]))
./DeepSpeed/tests/unit/modelingpreln.py:             self.biases[0].register_hook(lambda x, self=self: grads.append([x,"Q_B"]))
./DeepSpeed/tests/unit/modelingpreln.py:             self.weight[1].register_hook(lambda x, self=self: grads.append([x,"K_W"]))
./DeepSpeed/tests/unit/modelingpreln.py:             self.biases[1].register_hook(lambda x, self=self: grads.append([x,"K_B"]))
./DeepSpeed/tests/unit/modelingpreln.py:        layer = BertLayer(config, weights, biases)
./DeepSpeed/tests/unit/modelingpreln.py:             decoder layers
./DeepSpeed/tests/unit/modelingpreln.py:                print("pytorch weight is: ", layer_module.get_w())
./DeepSpeed/tests/conftest.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/conftest.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/conftest.py: DeepSpeed Team
./DeepSpeed/tests/conftest.py: tests directory-specific settings - this file is run automatically by pytest before any tests are run
./DeepSpeed/tests/conftest.py: Set this environment variable for the T5 inference unittest(s) (e.g. google/t5-v1_1-small)
./DeepSpeed/tests/conftest.py: allow having multiple repository checkouts and not needing to remember to rerun
./DeepSpeed/tests/conftest.py: 'pip install -e .[dev]' when switching between checkouts and running tests.
./DeepSpeed/tests/conftest.py: Override of pytest "runtest" for DistributedTest class
./DeepSpeed/tests/conftest.py: This hook is run before the default pytest_runtest_call
./DeepSpeed/tests/conftest.py:     We want to use our own launching function for distributed tests
./DeepSpeed/tests/conftest.py: We allow DistributedTest to reuse distributed environments. When the last
./DeepSpeed/tests/conftest.py: test for a class is run, we want to make sure those distributed environments
./DeepSpeed/tests/conftest.py: are destroyed.
./DeepSpeed/tests/onebit/test_nccl_perf.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/onebit/test_nccl_perf.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/onebit/test_nccl_perf.py: DeepSpeed Team
./DeepSpeed/tests/onebit/test_nccl_perf.py: Setting tensor_size (BERT-Large)
./DeepSpeed/tests/onebit/test_nccl_perf.py: Adding bias to the initialization of the gradient we are communicating
./DeepSpeed/tests/onebit/test_nccl_perf.py: In order to get rid of the case where some elements in the gradient are too small
./DeepSpeed/tests/onebit/test_nccl_perf.py: Warmup
./DeepSpeed/tests/onebit/test_nccl_perf.py:    deepspeed.comm.all_reduce(a_compressed)
./DeepSpeed/tests/onebit/test_nccl_perf.py:timer_names = ['compressed_allreduce']
./DeepSpeed/tests/onebit/test_nccl_perf.py:timers.log(names=timer_names, normalizer=1, memory_breakdown=None)
./DeepSpeed/tests/onebit/test_nccl_perf.py:print("tensor shape", a.shape)
./DeepSpeed/tests/onebit/test_mpi_backend.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/onebit/test_mpi_backend.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/onebit/test_mpi_backend.py: DeepSpeed Team
./DeepSpeed/tests/onebit/test_mpi_backend.py: Change cuda_aware to True to test out CUDA-Aware MPI communication
./DeepSpeed/tests/onebit/test_mpi_backend.py: A simulated compression function using deepspeed.comm
./DeepSpeed/tests/onebit/test_mpi_backend.py: Adding bias to the initialization of the gradient we are communicating
./DeepSpeed/tests/onebit/test_mpi_backend.py: In order to get rid of the case where some elements in the gradient are too small
./DeepSpeed/tests/onebit/test_mpi_backend.py: If the number in the compensated_server_m is too small (e.g 1e-8), then calling sign() might be problematic
./DeepSpeed/tests/onebit/test_mpi_backend.py: The test would skip those numbers that are too small in compensated_server_m
./DeepSpeed/tests/onebit/test_mpi_perf.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/onebit/test_mpi_perf.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/onebit/test_mpi_perf.py: DeepSpeed Team
./DeepSpeed/tests/onebit/test_mpi_perf.py: Configure wall clock timer
./DeepSpeed/tests/onebit/test_mpi_perf.py: Change cuda_aware to True to test out CUDA-Aware MPI communication
./DeepSpeed/tests/onebit/test_mpi_perf.py: Adding bias to the initialization of the gradient we are communicating
./DeepSpeed/tests/onebit/test_mpi_perf.py: In order to get rid of the case where some elements in the gradient are too small
./DeepSpeed/tests/onebit/test_mpi_perf.py: Warmup
./DeepSpeed/tests/onebit/test_nccl_backend.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/onebit/test_nccl_backend.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/onebit/test_nccl_backend.py: DeepSpeed Team
./DeepSpeed/tests/onebit/test_nccl_backend.py: A simulated compression function using deepspeed.comm
./DeepSpeed/tests/onebit/test_nccl_backend.py: Adding bias to the initialization of the gradient we are communicating
./DeepSpeed/tests/onebit/test_nccl_backend.py: In order to get rid of the case where some elements in the gradient are too small
./DeepSpeed/tests/onebit/test_nccl_backend.py: If the number in the compensated_server_m is too small (e.g 1e-8), then calling sign() might be problematic
./DeepSpeed/tests/onebit/test_nccl_backend.py: The test would skip those numbers that are too small in compensated_server_m
./DeepSpeed/tests/accelerator/test_ds_init.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/accelerator/test_ds_init.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/accelerator/test_ds_init.py: DeepSpeed Team
./DeepSpeed/tests/benchmarks/DS4Sci_EvoformerAttention_bench.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/benchmarks/DS4Sci_EvoformerAttention_bench.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/benchmarks/DS4Sci_EvoformerAttention_bench.py: DeepSpeed Team
./DeepSpeed/tests/benchmarks/DS4Sci_EvoformerAttention_bench.py:     Original shape: [*, Dim_Q, H, C_hid] -> Transpose to: [*, H, Dim_Q, C_hid]
./DeepSpeed/tests/benchmarks/DS4Sci_EvoformerAttention_bench.py:     Now, q, k, v are in shape: [*, H, Dim_Q, C_hid]
./DeepSpeed/tests/benchmarks/DS4Sci_EvoformerAttention_bench.py:     Transpose k to shape [*, H, C_hid, Dim_Q]
./DeepSpeed/tests/benchmarks/DS4Sci_EvoformerAttention_bench.py:     Now, q and k_t are in shapes: [*, H, Dim_Q, C_hid] and [*, H, C_hid, Dim_Q] respectively
./DeepSpeed/tests/benchmarks/DS4Sci_EvoformerAttention_bench.py:     [*, H, Dim_Q, Dim_Q]
./DeepSpeed/tests/benchmarks/DS4Sci_EvoformerAttention_bench.py:     Now, a is in shape [*, H, Dim_Q, Dim_Q], v is in shape [*, H, Dim_Q, C_hid]
./DeepSpeed/tests/benchmarks/DS4Sci_EvoformerAttention_bench.py:     Matmul operation results in [*, H, Dim_Q, C_hid]
./DeepSpeed/tests/benchmarks/DS4Sci_EvoformerAttention_bench.py:     [*, Dim_Q, H, C_hid]
./DeepSpeed/tests/benchmarks/DS4Sci_EvoformerAttention_bench.py:         warm up
./DeepSpeed/tests/benchmarks/DS4Sci_EvoformerAttention_bench.py:         warm up
./DeepSpeed/tests/benchmarks/unflatten_bench.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/benchmarks/unflatten_bench.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/benchmarks/unflatten_bench.py: DeepSpeed Team
./DeepSpeed/tests/benchmarks/unflatten_bench.py:!/usr/bin/env python
./DeepSpeed/tests/benchmarks/unflatten_bench.py: run the benchmark under timeit (-t), cProfile (-c), line_profiler (-l)
./DeepSpeed/tests/benchmarks/unflatten_bench.py:
./DeepSpeed/tests/benchmarks/unflatten_bench.py: usage:
./DeepSpeed/tests/benchmarks/unflatten_bench.py: ./unflatten_bench.py -t
./DeepSpeed/tests/benchmarks/unflatten_bench.py: ./unflatten_bench.py -c
./DeepSpeed/tests/benchmarks/unflatten_bench.py: kernprof -l unflatten_bench.py -l; python -m line_profiler  unflatten_bench.py.lprof
./DeepSpeed/tests/benchmarks/unflatten_bench.py: emulate a small typical model weights
./DeepSpeed/tests/benchmarks/unflatten_bench.py: warm up and check that the same output is produced
./DeepSpeed/tests/benchmarks/unflatten_bench.py:numel = flat_cpp.numel()
./DeepSpeed/tests/benchmarks/unflatten_bench.py: the programs being tested
./DeepSpeed/tests/benchmarks/unflatten_bench.py:### cProfile ####
./DeepSpeed/tests/benchmarks/unflatten_bench.py:### timeit ####
./DeepSpeed/tests/benchmarks/unflatten_bench.py:### line_profiler ####
./DeepSpeed/tests/benchmarks/unflatten_bench.py: this one requires a special way to be called
./DeepSpeed/tests/benchmarks/unflatten_bench.py: pip install line_profiler
./DeepSpeed/tests/benchmarks/unflatten_bench.py: kernprof -l unflatten_bench.py -l; python -m line_profiler unflatten_bench.py.lprof
./DeepSpeed/tests/benchmarks/flatten_bench.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/benchmarks/flatten_bench.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/benchmarks/flatten_bench.py: DeepSpeed Team
./DeepSpeed/tests/benchmarks/flatten_bench.py:!/usr/bin/env python
./DeepSpeed/tests/benchmarks/flatten_bench.py: run the benchmark under timeit (-t), cProfile (-c), line_profiler (-l)
./DeepSpeed/tests/benchmarks/flatten_bench.py:
./DeepSpeed/tests/benchmarks/flatten_bench.py: usage:
./DeepSpeed/tests/benchmarks/flatten_bench.py: ./flatten_bench.py -t
./DeepSpeed/tests/benchmarks/flatten_bench.py: ./flatten_bench.py -c
./DeepSpeed/tests/benchmarks/flatten_bench.py: kernprof -l flatten_bench.py -l; python -m line_profiler  flatten_bench.py.lprof
./DeepSpeed/tests/benchmarks/flatten_bench.py: emulate a small typical model weights
./DeepSpeed/tests/benchmarks/flatten_bench.py: warm up and check that the same output is produced
./DeepSpeed/tests/benchmarks/flatten_bench.py:numel = flat_cpp.numel()
./DeepSpeed/tests/benchmarks/flatten_bench.py: the programs being tested
./DeepSpeed/tests/benchmarks/flatten_bench.py:### cProfile ####
./DeepSpeed/tests/benchmarks/flatten_bench.py:### timeit ####
./DeepSpeed/tests/benchmarks/flatten_bench.py:### line_profiler ####
./DeepSpeed/tests/benchmarks/flatten_bench.py: this one requires a special way to be called
./DeepSpeed/tests/benchmarks/flatten_bench.py: pip install line_profiler
./DeepSpeed/tests/benchmarks/flatten_bench.py: kernprof -l flatten_bench.py -l; python -m line_profiler  flatten_bench.py.lprof
./DeepSpeed/tests/model/run_sanity_check.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/model/run_sanity_check.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/model/run_sanity_check.py: DeepSpeed Team
./DeepSpeed/tests/model/run_sanity_check.py: Import the test cases here.
./DeepSpeed/tests/model/Megatron_GPT2/test_common.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/model/Megatron_GPT2/test_common.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/model/Megatron_GPT2/test_common.py: DeepSpeed Team
./DeepSpeed/tests/model/Megatron_GPT2/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/model/Megatron_GPT2/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/model/Megatron_GPT2/__init__.py: DeepSpeed Team
./DeepSpeed/tests/model/Megatron_GPT2/run_perf_test.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/model/Megatron_GPT2/run_perf_test.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/model/Megatron_GPT2/run_perf_test.py: DeepSpeed Team
./DeepSpeed/tests/model/Megatron_GPT2/run_perf_baseline.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/model/Megatron_GPT2/run_perf_baseline.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/model/Megatron_GPT2/run_perf_baseline.py: DeepSpeed Team
./DeepSpeed/tests/model/Megatron_GPT2/run_checkpoint_test.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/model/Megatron_GPT2/run_checkpoint_test.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/model/Megatron_GPT2/run_checkpoint_test.py: DeepSpeed Team
./DeepSpeed/tests/model/Megatron_GPT2/run_checkpoint_test.py:         Cache save and load gpu counts
./DeepSpeed/tests/model/Megatron_GPT2/run_checkpoint_test.py:         save to current directory.
./DeepSpeed/tests/model/Megatron_GPT2/run_checkpoint_test.py:        ---------------remove old checkpoint---------------#
./DeepSpeed/tests/model/Megatron_GPT2/run_checkpoint_test.py:        -----------------Saving Checkpoint-----------------#
./DeepSpeed/tests/model/Megatron_GPT2/run_checkpoint_test.py:         building checkpoint arguments
./DeepSpeed/tests/model/Megatron_GPT2/run_checkpoint_test.py:         create checkpoint run...
./DeepSpeed/tests/model/Megatron_GPT2/run_checkpoint_test.py:         remove previous test log
./DeepSpeed/tests/model/Megatron_GPT2/run_checkpoint_test.py:        -----------------Loading Checkpoint-----------------#
./DeepSpeed/tests/model/Megatron_GPT2/run_checkpoint_test.py:         building checkpoint arguments
./DeepSpeed/tests/model/Megatron_GPT2/run_checkpoint_test.py:         set checkpoint load iteration
./DeepSpeed/tests/model/Megatron_GPT2/run_checkpoint_test.py:         set load gpus
./DeepSpeed/tests/model/Megatron_GPT2/run_checkpoint_test.py:         remove previous test log
./DeepSpeed/tests/model/Megatron_GPT2/run_checkpoint_test.py:     Shrink DP
./DeepSpeed/tests/model/Megatron_GPT2/run_checkpoint_test.py:     Expand DP
./DeepSpeed/tests/model/Megatron_GPT2/run_func_test.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/model/Megatron_GPT2/run_func_test.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/model/Megatron_GPT2/run_func_test.py: DeepSpeed Team
./DeepSpeed/tests/model/Megatron_GPT2/run_func_test.py:         assure no crash.
./DeepSpeed/tests/model/Megatron_GPT2/run_func_test.py:         baseline run...
./DeepSpeed/tests/model/Megatron_GPT2/run_func_test.py:         turnoff deepspeed if baseline deepspeed config
./DeepSpeed/tests/model/Megatron_GPT2/run_func_test.py:         is not provided
./DeepSpeed/tests/model/Megatron_GPT2/run_func_test.py:         skip baseline run if it exists.
./DeepSpeed/tests/model/Megatron_GPT2/run_func_test.py:         DeepSpeed run...
./DeepSpeed/tests/model/Megatron_GPT2/run_func_test.py:         baseline run...
./DeepSpeed/tests/model/Megatron_GPT2/run_func_test.py:         turn off deepspeed if a baseline deepspeed config
./DeepSpeed/tests/model/Megatron_GPT2/run_func_test.py:         is not provided
./DeepSpeed/tests/model/Megatron_GPT2/run_func_test.py:         baseline run...
./DeepSpeed/tests/model/Megatron_GPT2/run_func_test.py:         skip baseline run if it exists.
./DeepSpeed/tests/model/Megatron_GPT2/run_func_test.py:         DeepSpeed run...
./DeepSpeed/tests/model/Megatron_GPT2/run_func_test.py:     Baseline = Megatron + Torch.Optim.Adam
./DeepSpeed/tests/model/Megatron_GPT2/run_func_test.py:     Test = Megatron + Torch.Optim.Adam + ZeRO-Offload
./DeepSpeed/tests/model/Megatron_GPT2/run_func_test.py:     Baseline = Megatron + Torch.Optim.Adam
./DeepSpeed/tests/model/Megatron_GPT2/run_func_test.py:     Test = Megatron + DeepSpeedAdam + ZeRO-Offload
./DeepSpeed/tests/model/BingBertSquad/BingBertSquad_test_common.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/model/BingBertSquad/BingBertSquad_test_common.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/model/BingBertSquad/BingBertSquad_test_common.py: DeepSpeed Team
./DeepSpeed/tests/model/BingBertSquad/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/model/BingBertSquad/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/model/BingBertSquad/__init__.py: DeepSpeed Team
./DeepSpeed/tests/model/BingBertSquad/BingBertSquad_run_func_test.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/model/BingBertSquad/BingBertSquad_run_func_test.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/model/BingBertSquad/BingBertSquad_run_func_test.py: DeepSpeed Team
./DeepSpeed/tests/model/BingBertSquad/BingBertSquad_run_func_test.py:         baseline run...
./DeepSpeed/tests/model/BingBertSquad/BingBertSquad_run_func_test.py:         skip baseline run if it exists.
./DeepSpeed/tests/model/BingBertSquad/BingBertSquad_run_func_test.py:         DeepSpeed run...
./DeepSpeed/tests/model/BingBertSquad/test_e2e_squad.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/model/BingBertSquad/test_e2e_squad.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/model/BingBertSquad/test_e2e_squad.py: DeepSpeed Team
./DeepSpeed/tests/model/BingBertSquad/test_e2e_squad.py:     base run results => {"exact_match": 83.9829706717124, "f1": 90.71138132004097}
./DeepSpeed/tests/model/BingBertSquad/test_e2e_squad.py:     base run results => {"exact_match": 84.1438032166509, "f1": 90.89776136505441}
./DeepSpeed/tests/small_model_debugging/test_model.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/small_model_debugging/test_model.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/small_model_debugging/test_model.py: DeepSpeed Team
./DeepSpeed/tests/small_model_debugging/test_model.py:        "initial_scale_power": 15
./DeepSpeed/tests/small_model_debugging/test_model.py:print_params('pre-train', model)
./DeepSpeed/tests/small_model_debugging/test_model.py:    print_params('step={}'.format(n), model)
./DeepSpeed/tests/small_model_debugging/test_model.py:    if n == 5: break
./DeepSpeed/tests/small_model_debugging/stage3_test.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/small_model_debugging/stage3_test.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/small_model_debugging/stage3_test.py: DeepSpeed Team
./DeepSpeed/tests/small_model_debugging/stage3_test.py:##################################
./DeepSpeed/tests/small_model_debugging/stage3_test.py: Setup
./DeepSpeed/tests/small_model_debugging/stage3_test.py:##################################
./DeepSpeed/tests/small_model_debugging/stage3_test.py:##################################
./DeepSpeed/tests/small_model_debugging/stage3_test.py: DRIVER
./DeepSpeed/tests/small_model_debugging/stage3_test.py:##################################
./DeepSpeed/tests/small_model_debugging/stage3_test.py:     parted = [name for (name, p) in model.named_parameters() if p._partitioned]
./DeepSpeed/tests/small_model_debugging/stage3_test.py:     not_parted = [name for (name, p) in model.named_parameters() if not p._partitioned]
./DeepSpeed/tests/small_model_debugging/stage3_test.py:     print('partitioned: ', parted)
./DeepSpeed/tests/small_model_debugging/stage3_test.py:     print('full: ', not_parted)
./DeepSpeed/tests/small_model_debugging/stage3_test.py:     print()
./DeepSpeed/tests/small_model_debugging/stage3_test.py:     parted = [name for (name, p) in model.named_parameters() if p._partitioned]
./DeepSpeed/tests/small_model_debugging/stage3_test.py:     not_parted = [name for (name, p) in model.named_parameters() if not p._partitioned]
./DeepSpeed/tests/small_model_debugging/stage3_test.py:     print('partitioned: ', parted)
./DeepSpeed/tests/small_model_debugging/stage3_test.py:     print('full:' , not_parted)
./DeepSpeed/tests/small_model_debugging/stage3_test.py:     print()
./DeepSpeed/tests/small_model_debugging/stage3_test.py:    samyamspeed.disable()
./DeepSpeed/tests/small_model_debugging/partial_offload_test.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/small_model_debugging/partial_offload_test.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/small_model_debugging/partial_offload_test.py: DeepSpeed Team
./DeepSpeed/tests/small_model_debugging/partial_offload_test.py:        "initial_scale_power": 15
./DeepSpeed/tests/small_model_debugging/partial_offload_test.py:print_params('pre-train', model)
./DeepSpeed/tests/small_model_debugging/partial_offload_test.py:while True:
./DeepSpeed/tests/small_model_debugging/partial_offload_test.py:    print_params('step={}'.format(n), model)
./DeepSpeed/tests/small_model_debugging/test_mics_config.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/small_model_debugging/test_mics_config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/small_model_debugging/test_mics_config.py: DeepSpeed Team
./DeepSpeed/tests/small_model_debugging/test_mics_config.py: Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
./DeepSpeed/tests/small_model_debugging/test_mics_config.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/small_model_debugging/test_mics_config.py:     print('config_dict["zero_optimization"]', config_dict["zero_optimization"])
./DeepSpeed/tests/small_model_debugging/test_mics_config.py:        "initial_scale_power": 15
./DeepSpeed/tests/small_model_debugging/test_mics_config.py: print('------> init model with deepspeed.zero.Init()')
./DeepSpeed/tests/small_model_debugging/test_mics_config.py:print_params('pre-train', model)
./DeepSpeed/tests/small_model_debugging/test_mics_config.py:    print_params('step={}'.format(n), model)
./DeepSpeed/tests/small_model_debugging/test.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/small_model_debugging/test.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/small_model_debugging/test.py: DeepSpeed Team
./DeepSpeed/tests/small_model_debugging/test.py:     Print message except when distributed but not rank 0
./DeepSpeed/tests/small_model_debugging/test.py:linear_bk = torch.nn.functional.linear
./DeepSpeed/tests/small_model_debugging/test.py:torch.nn.functional.linear = deepspeed.pt.deepspeed_linear.LinearFunctionForZeroStage3.apply
./DeepSpeed/tests/hybrid_engine/hybrid_engine_test.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/hybrid_engine/hybrid_engine_test.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/hybrid_engine/hybrid_engine_test.py: DeepSpeed Team
./DeepSpeed/tests/perf/adam_test1.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/perf/adam_test1.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/perf/adam_test1.py: DeepSpeed Team
./DeepSpeed/tests/perf/adam_test1.py:torch.set_num_threads(128)
./DeepSpeed/tests/perf/adagrad_test.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/perf/adagrad_test.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/perf/adagrad_test.py: DeepSpeed Team
./DeepSpeed/tests/perf/adam_test.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/tests/perf/adam_test.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/tests/perf/adam_test.py: DeepSpeed Team
./DeepSpeed/release/check_release_version.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/release/check_release_version.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/release/check_release_version.py: DeepSpeed Team
./DeepSpeed/release/bump_patch_version.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/release/bump_patch_version.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/release/bump_patch_version.py: DeepSpeed Team
./DeepSpeed/docs/code-docs/source/conf.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/docs/code-docs/source/conf.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/docs/code-docs/source/conf.py: DeepSpeed Team
./DeepSpeed/docs/code-docs/source/conf.py: Configuration file for the Sphinx documentation builder.
./DeepSpeed/docs/code-docs/source/conf.py:
./DeepSpeed/docs/code-docs/source/conf.py: This file only contains a selection of the most common options. For a full
./DeepSpeed/docs/code-docs/source/conf.py: list see the documentation:
./DeepSpeed/docs/code-docs/source/conf.py: https://www.sphinx-doc.org/en/master/usage/configuration.html
./DeepSpeed/docs/code-docs/source/conf.py: -- Path setup --------------------------------------------------------------
./DeepSpeed/docs/code-docs/source/conf.py: If extensions (or modules to document with autodoc) are in another directory,
./DeepSpeed/docs/code-docs/source/conf.py: add these directories to sys.path here. If the directory is relative to the
./DeepSpeed/docs/code-docs/source/conf.py: documentation root, use os.path.abspath to make it absolute, like shown here.
./DeepSpeed/docs/code-docs/source/conf.py: -- Project information -----------------------------------------------------
./DeepSpeed/docs/code-docs/source/conf.py: The full version, including alpha/beta/rc tags
./DeepSpeed/docs/code-docs/source/conf.py: -- General configuration ---------------------------------------------------
./DeepSpeed/docs/code-docs/source/conf.py: Add any Sphinx extension module names here, as strings. They can be
./DeepSpeed/docs/code-docs/source/conf.py: extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
./DeepSpeed/docs/code-docs/source/conf.py: ones.
./DeepSpeed/docs/code-docs/source/conf.py: autodoc_pyandtic config
./DeepSpeed/docs/code-docs/source/conf.py: Add any paths that contain templates here, relative to this directory.
./DeepSpeed/docs/code-docs/source/conf.py: List of patterns, relative to source directory, that match files and
./DeepSpeed/docs/code-docs/source/conf.py: directories to ignore when looking for source files.
./DeepSpeed/docs/code-docs/source/conf.py: This pattern also affects html_static_path and html_extra_path.
./DeepSpeed/docs/code-docs/source/conf.py: -- Options for HTML output -------------------------------------------------
./DeepSpeed/docs/code-docs/source/conf.py: The theme to use for HTML and HTML Help pages.  See the documentation for
./DeepSpeed/docs/code-docs/source/conf.py: a list of builtin themes.
./DeepSpeed/docs/code-docs/source/conf.py:
./DeepSpeed/docs/code-docs/source/conf.py: Add any paths that contain custom static files (such as style sheets) here,
./DeepSpeed/docs/code-docs/source/conf.py: relative to this directory. They are copied after the builtin static files,
./DeepSpeed/docs/code-docs/source/conf.py: so a file named "default.css" will overwrite the builtin "default.css".
./DeepSpeed/docs/code-docs/source/conf.py: GitHub integration
./DeepSpeed/docs/code-docs/source/conf.py: Prepend module names to class descriptions?
./DeepSpeed/setup.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/setup.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/setup.py: DeepSpeed Team
./DeepSpeed/setup.py: Fetch rocm state.
./DeepSpeed/setup.py: Add specific cupy version to both onebit extension variants.
./DeepSpeed/setup.py:         XXX cupy support for rocm 5 is not available yet.
./DeepSpeed/setup.py: Make an [all] extra that installs all needed dependencies.
./DeepSpeed/setup.py: For any pre-installed ops force disable ninja.
./DeepSpeed/setup.py:     Fix to allow docker builds, similar to https://github.com/NVIDIA/apex/issues/486.
./DeepSpeed/setup.py: Default to pre-install kernels to false so we rely on JIT on Linux, opposite on Windows.
./DeepSpeed/setup.py:     If op is requested but not available, throw an error.
./DeepSpeed/setup.py:     If op is compatible but install is not enabled (JIT mode).
./DeepSpeed/setup.py:     If op install enabled, add builder to extensions.
./DeepSpeed/setup.py: Write out version/git info.
./DeepSpeed/setup.py:     This creates a symbolic links on Windows.
./DeepSpeed/setup.py:     It needs Administrator privilege to create symlinks on Windows.
./DeepSpeed/setup.py: Parse the DeepSpeed version string from version.txt.
./DeepSpeed/setup.py: Build specifiers like .devX can be added at install time. Otherwise, add the git hash.
./DeepSpeed/setup.py: Example: DS_BUILD_STRING=".dev20201022" python setup.py sdist bdist_wheel.
./DeepSpeed/setup.py: Building wheel for distribution, update version file.
./DeepSpeed/setup.py:     Build string env specified, probably building for distribution.
./DeepSpeed/setup.py:     build.txt exists, probably installing from distribution.
./DeepSpeed/setup.py:     None of the above, probably installing from source.
./DeepSpeed/setup.py: Set cuda_version to 0.0 if cpu-only.
./DeepSpeed/setup.py: Set hip_version to 0.0 if cpu-only.
./DeepSpeed/setup.py:             This will break if minor version > 9.
./DeepSpeed/setup.py: Parse README.md to make long_description for PyPI page.
./DeepSpeed/accelerator/cuda_accelerator.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/accelerator/cuda_accelerator.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/accelerator/cuda_accelerator.py: DeepSpeed Team
./DeepSpeed/accelerator/cuda_accelerator.py: During setup stage torch may not be installed, pass on no torch will
./DeepSpeed/accelerator/cuda_accelerator.py: allow op builder related API to be executed.
./DeepSpeed/accelerator/cuda_accelerator.py: Delay import pynvml to avoid import error when CUDA is not available
./DeepSpeed/accelerator/cuda_accelerator.py:     Device APIs
./DeepSpeed/accelerator/cuda_accelerator.py:     RNG APIs
./DeepSpeed/accelerator/cuda_accelerator.py:     Streams/Events
./DeepSpeed/accelerator/cuda_accelerator.py:     Memory management
./DeepSpeed/accelerator/cuda_accelerator.py:         if CUDA_VISIBLE_DEVICES is used automagically remap the id since pynvml ignores this env var
./DeepSpeed/accelerator/cuda_accelerator.py:     Data types
./DeepSpeed/accelerator/cuda_accelerator.py:     Misc
./DeepSpeed/accelerator/cuda_accelerator.py:     Tensor operations
./DeepSpeed/accelerator/cuda_accelerator.py:             is op_builder from deepspeed or a 3p version? this should only succeed if it's deepspeed
./DeepSpeed/accelerator/cuda_accelerator.py:             if successful this also means we're doing a local install and not JIT compile path
./DeepSpeed/accelerator/cuda_accelerator.py:     dict that holds class name <--> class type mapping i.e.
./DeepSpeed/accelerator/cuda_accelerator.py:     'AsyncIOBuilder': <class 'op_builder.async_io.AsyncIOBuilder'>
./DeepSpeed/accelerator/cuda_accelerator.py:     this dict will be filled at init stage
./DeepSpeed/accelerator/cuda_accelerator.py:             begin initialize for create_op_builder()
./DeepSpeed/accelerator/cuda_accelerator.py:             put all valid class name <--> class type mapping into class_dict
./DeepSpeed/accelerator/cuda_accelerator.py:                 avoid self references,
./DeepSpeed/accelerator/cuda_accelerator.py:                 skip sub_directories which contains ops for other backend(cpu, npu, etc.).
./DeepSpeed/accelerator/cuda_accelerator.py:             end initialize for create_op_builder()
./DeepSpeed/accelerator/cuda_accelerator.py:     create an instance of op builder and return, name specified by class_name
./DeepSpeed/accelerator/cuda_accelerator.py:     return an op builder class, name specified by class_name
./DeepSpeed/accelerator/npu_accelerator.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/accelerator/npu_accelerator.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/accelerator/npu_accelerator.py: DeepSpeed Team
./DeepSpeed/accelerator/npu_accelerator.py: During setup stage torch may not be installed, pass on no torch will
./DeepSpeed/accelerator/npu_accelerator.py: allow op builder related API to be executed.
./DeepSpeed/accelerator/npu_accelerator.py:         dict that holds class name <--> class type mapping i.e.
./DeepSpeed/accelerator/npu_accelerator.py:         'AsyncIOBuilder': <class 'op_builder.async_io.AsyncIOBuilder'>
./DeepSpeed/accelerator/npu_accelerator.py:         this dict will be filled at init stage
./DeepSpeed/accelerator/npu_accelerator.py:     Device APIs
./DeepSpeed/accelerator/npu_accelerator.py:     RNG APIs
./DeepSpeed/accelerator/npu_accelerator.py:     Streams/Events
./DeepSpeed/accelerator/npu_accelerator.py:     Memory management
./DeepSpeed/accelerator/npu_accelerator.py:     Data types
./DeepSpeed/accelerator/npu_accelerator.py:     Misc
./DeepSpeed/accelerator/npu_accelerator.py:     Tensor operations
./DeepSpeed/accelerator/npu_accelerator.py:             is op_builder from deepspeed or a 3p version? this should only succeed if it's deepspeed
./DeepSpeed/accelerator/npu_accelerator.py:             if successful this also means we're doing a local install and not JIT compile path
./DeepSpeed/accelerator/npu_accelerator.py:         get op builder class from op_builder/npu/__init__.py
./DeepSpeed/accelerator/npu_accelerator.py:     create an instance of op builder and return, name specified by class_name
./DeepSpeed/accelerator/npu_accelerator.py:     return an op builder class, name specified by class_name
./DeepSpeed/accelerator/abstract_accelerator.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/accelerator/abstract_accelerator.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/accelerator/abstract_accelerator.py: DeepSpeed Team
./DeepSpeed/accelerator/abstract_accelerator.py:     Device APIs
./DeepSpeed/accelerator/abstract_accelerator.py:     RNG APIs
./DeepSpeed/accelerator/abstract_accelerator.py:     Streams/Events
./DeepSpeed/accelerator/abstract_accelerator.py:     Memory management
./DeepSpeed/accelerator/abstract_accelerator.py:     Data types
./DeepSpeed/accelerator/abstract_accelerator.py:     Misc
./DeepSpeed/accelerator/abstract_accelerator.py:     Tensor operations
./DeepSpeed/accelerator/abstract_accelerator.py:     create an instance of op builder, specified by class_name
./DeepSpeed/accelerator/abstract_accelerator.py:     return an op builder class, specified by class_name
./DeepSpeed/accelerator/__init__.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/accelerator/__init__.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/accelerator/__init__.py: DeepSpeed Team
./DeepSpeed/accelerator/real_accelerator.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/accelerator/real_accelerator.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/accelerator/real_accelerator.py: DeepSpeed Team
./DeepSpeed/accelerator/real_accelerator.py:     Importing logger currently requires that torch is installed, hence the try...except
./DeepSpeed/accelerator/real_accelerator.py:     TODO: Remove logger dependency on torch.
./DeepSpeed/accelerator/real_accelerator.py:     because abstract_accelerator has different path during
./DeepSpeed/accelerator/real_accelerator.py:     build time (accelerator.abstract_accelerator)
./DeepSpeed/accelerator/real_accelerator.py:     and run time (deepspeed.accelerator.abstract_accelerator)
./DeepSpeed/accelerator/real_accelerator.py:     and extension would import the
./DeepSpeed/accelerator/real_accelerator.py:     run time abstract_accelerator/DeepSpeedAccelerator as its base
./DeepSpeed/accelerator/real_accelerator.py:     class, so we need to compare accel_obj with both base class.
./DeepSpeed/accelerator/real_accelerator.py:     if accel_obj is instance of DeepSpeedAccelerator in one of
./DeepSpeed/accelerator/real_accelerator.py:     accelerator.abstractor_accelerator
./DeepSpeed/accelerator/real_accelerator.py:     or deepspeed.accelerator.abstract_accelerator, consider accel_obj
./DeepSpeed/accelerator/real_accelerator.py:     is a conforming object
./DeepSpeed/accelerator/real_accelerator.py:     TODO: turn off is_available test since this breaks tests
./DeepSpeed/accelerator/real_accelerator.py:     assert accel_obj.is_available(), \
./DeepSpeed/accelerator/real_accelerator.py:        f'{accel_obj.__class__.__name__} accelerator fails is_available() test'
./DeepSpeed/accelerator/real_accelerator.py:     1. Detect whether there is override of DeepSpeed accelerators from environment variable.
./DeepSpeed/accelerator/real_accelerator.py:                 should use torch.mps.is_available() if it exists someday but this is used as proxy
./DeepSpeed/accelerator/real_accelerator.py:     2. If no override, detect which accelerator to use automatically
./DeepSpeed/accelerator/real_accelerator.py:         We need a way to choose among different accelerator types.
./DeepSpeed/accelerator/real_accelerator.py:         Currently we detect which accelerator extension is installed
./DeepSpeed/accelerator/real_accelerator.py:         in the environment and use it if the installing answer is True.
./DeepSpeed/accelerator/real_accelerator.py:         An alternative might be detect whether CUDA device is installed on
./DeepSpeed/accelerator/real_accelerator.py:         the system but this comes with two pitfalls:
./DeepSpeed/accelerator/real_accelerator.py:         1. the system may not have torch pre-installed, so
./DeepSpeed/accelerator/real_accelerator.py:            get_accelerator().is_available() may not work.
./DeepSpeed/accelerator/real_accelerator.py:         2. Some scenario like install on login node (without CUDA device)
./DeepSpeed/accelerator/real_accelerator.py:            and run on compute node (with CUDA device) may cause mismatch
./DeepSpeed/accelerator/real_accelerator.py:            between installation time and runtime.
./DeepSpeed/accelerator/real_accelerator.py:                 should use torch.mps.is_available() if it exists someday but this is used as proxy
./DeepSpeed/accelerator/real_accelerator.py:     3. Set ds_accelerator accordingly
./DeepSpeed/accelerator/real_accelerator.py:         XPU_Accelerator is already imported in detection stage
./DeepSpeed/accelerator/mps_accelerator.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/accelerator/mps_accelerator.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/accelerator/mps_accelerator.py: DeepSpeed Team
./DeepSpeed/accelerator/mps_accelerator.py: During setup stage torch may not be installed, pass on no torch will
./DeepSpeed/accelerator/mps_accelerator.py: allow op builder related API to be executed.
./DeepSpeed/accelerator/mps_accelerator.py:     Device APIs
./DeepSpeed/accelerator/mps_accelerator.py:     RNG APIs
./DeepSpeed/accelerator/mps_accelerator.py:     Streams/Events
./DeepSpeed/accelerator/mps_accelerator.py:     Memory management
./DeepSpeed/accelerator/mps_accelerator.py:     Data types
./DeepSpeed/accelerator/mps_accelerator.py:     Misc
./DeepSpeed/accelerator/mps_accelerator.py:     Tensor operations
./DeepSpeed/accelerator/mps_accelerator.py:             is op_builder from deepspeed or a 3p version? this should only succeed if it's deepspeed
./DeepSpeed/accelerator/mps_accelerator.py:             if successful this also means we're doing a local install and not JIT compile path
./DeepSpeed/accelerator/mps_accelerator.py:     create an instance of op builder, specified by class_name
./DeepSpeed/accelerator/mps_accelerator.py:     return an op builder class, specified by class_name
./DeepSpeed/accelerator/cpu_accelerator.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/accelerator/cpu_accelerator.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/accelerator/cpu_accelerator.py: DeepSpeed Team
./DeepSpeed/accelerator/cpu_accelerator.py: accelerator for Intel CPU
./DeepSpeed/accelerator/cpu_accelerator.py:     Device APIs
./DeepSpeed/accelerator/cpu_accelerator.py:             Count NUMA node for number of cpu accelerators. On machine with HBM
./DeepSpeed/accelerator/cpu_accelerator.py:             In flat mode, HBM is in separate NUMA node with no cores on this node.
./DeepSpeed/accelerator/cpu_accelerator.py:             Ignore these NUMA nodes with no cores.
./DeepSpeed/accelerator/cpu_accelerator.py:     RNG APIs
./DeepSpeed/accelerator/cpu_accelerator.py:     Streams/Events
./DeepSpeed/accelerator/cpu_accelerator.py:     Memory management
./DeepSpeed/accelerator/cpu_accelerator.py:     Misc
./DeepSpeed/accelerator/cpu_accelerator.py:         TODO itt is currently not supported yet
./DeepSpeed/accelerator/cpu_accelerator.py:         return torch.profiler.itt.range_push(msg)
./DeepSpeed/accelerator/cpu_accelerator.py:         TODO itt is currently not supported yet
./DeepSpeed/accelerator/cpu_accelerator.py:         return torch.profiler.itt.range_pop()
./DeepSpeed/accelerator/cpu_accelerator.py:     Data types
./DeepSpeed/accelerator/cpu_accelerator.py:     Tensor operations
./DeepSpeed/accelerator/cpu_accelerator.py:             is op_builder from deepspeed or a 3p version? this should only succeed if it's deepspeed
./DeepSpeed/accelerator/cpu_accelerator.py:             if successful this also means we're doing a local install and not JIT compile path
./DeepSpeed/accelerator/cpu_accelerator.py:     create an instance of op builder and return, name specified by class_name
./DeepSpeed/accelerator/cpu_accelerator.py:     return an op builder class, name specified by class_name
./DeepSpeed/accelerator/cpu_accelerator.py:             is op_builder from deepspeed or a 3p version? this should only succeed if it's deepspeed
./DeepSpeed/accelerator/cpu_accelerator.py:             if successful this also means we're doing a local install and not JIT compile path
./DeepSpeed/accelerator/cpu_accelerator.py:             return a NotImplementedBuilder to avoid get NoneType[Name] in unit tests
./DeepSpeed/scripts/check-license.py:!/usr/bin/env python3
./DeepSpeed/scripts/check-license.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/scripts/check-license.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/scripts/check-license.py: DeepSpeed Team
./DeepSpeed/scripts/check-torchcuda.py:!/usr/bin/env python3
./DeepSpeed/scripts/check-torchcuda.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/scripts/check-torchcuda.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/scripts/check-torchcuda.py: DeepSpeed Team
./DeepSpeed/scripts/check-torchcuda.py: There are many ways we could search for the string "torch.cuda", but `git
./DeepSpeed/scripts/check-torchcuda.py: grep --no-index` is nice because
./DeepSpeed/scripts/check-torchcuda.py:  - it's very fast (as compared to iterating over the file in Python)
./DeepSpeed/scripts/check-torchcuda.py:  - we can reasonably assume it's available on all machines
./DeepSpeed/scripts/check-torchcuda.py:  - unlike plain grep, which is slower and has different flags on MacOS versus
./DeepSpeed/scripts/check-torchcuda.py:    Linux, git grep is always the same.
./DeepSpeed/scripts/replace_copyright.py:!/usr/bin/env python3
./DeepSpeed/scripts/replace_copyright.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/scripts/replace_copyright.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/scripts/replace_copyright.py: DeepSpeed Team
./DeepSpeed/scripts/replace_copyright.py: These get_header_* functions are ugly, but they work :)
./DeepSpeed/scripts/replace_copyright.py:             Detected multiline comment
./DeepSpeed/scripts/replace_copyright.py:                 Ended a multiline comment
./DeepSpeed/scripts/replace_copyright.py:                 Started a multiline comment
./DeepSpeed/scripts/replace_copyright.py:                 Opened and closed multiline comment on single line
./DeepSpeed/scripts/replace_copyright.py:             Ended a multiline comment
./DeepSpeed/scripts/replace_copyright.py:             Not in a comment
./DeepSpeed/scripts/replace_copyright.py:             Detected multiline comment
./DeepSpeed/scripts/replace_copyright.py:                 multiline comment not closed on same line
./DeepSpeed/scripts/replace_copyright.py:             Ended a multiline comment
./DeepSpeed/scripts/replace_copyright.py:             Not in a comment
./DeepSpeed/scripts/replace_copyright.py:             Not in a comment
./DeepSpeed/scripts/replace_copyright.py:         Unique header, need to get user input
./DeepSpeed/scripts/replace_copyright.py:     Identify any shebang lines in the file
./DeepSpeed/scripts/replace_copyright.py:     Get the text we should preserve in this file and process to remove comment characters
./DeepSpeed/scripts/replace_copyright.py:     Format the text we want to keep into a new multiline comment
./DeepSpeed/scripts/replace_copyright.py:     Generate the copyright text we will be adding
./DeepSpeed/scripts/replace_copyright.py:     Assemble the new header
./DeepSpeed/scripts/replace_copyright.py:     Write out the new file
./DeepSpeed/scripts/replace_copyright.py:         Walk across directory looking for all files with extensions we want to modify
./DeepSpeed/scripts/check-torchdist.py:!/usr/bin/env python3
./DeepSpeed/scripts/check-torchdist.py: Copyright (c) Microsoft Corporation.
./DeepSpeed/scripts/check-torchdist.py: SPDX-License-Identifier: Apache-2.0
./DeepSpeed/scripts/check-torchdist.py: DeepSpeed Team
./DeepSpeed/scripts/check-torchdist.py: There are many ways we could search for the string "torch.distributed", but `git
./DeepSpeed/scripts/check-torchdist.py: grep --no-index` is nice because
./DeepSpeed/scripts/check-torchdist.py:  - it's very fast (as compared to iterating over the file in Python)
./DeepSpeed/scripts/check-torchdist.py:  - we can reasonably assume it's available on all machines
./DeepSpeed/scripts/check-torchdist.py:  - unlike plain grep, which is slower and has different flags on MacOS versus
./DeepSpeed/scripts/check-torchdist.py:    Linux, git grep is always the same.
./scripts/validate/__init__.py: -*- coding: utf-8 -*-
./scripts/validate/format.py: -*- coding: utf-8 -*-
./scripts/validate/format.py: Temporary replacement
./scripts/validate/format.py: The descriptions that contain () at the end must adapt to the new policy later
./scripts/validate/format.py: Type aliases
./scripts/validate/format.py:     url should be wrapped in "[TITLE](LINK)" Markdown syntax
./scripts/validate/format.py:         do not allow "... API" in the entry title
./scripts/validate/format.py:         check each category for the minimum number of entries
./scripts/validate/format.py:         skips lines that we do not care about
./scripts/validate/format.py:             every line segment should start and end with exactly 1 space
./scripts/validate/links.py: -*- coding: utf-8 -*-
./scripts/validate/links.py:     Remove routes, arguments and anchors
./scripts/tests/test_validate_format.py: -*- coding: utf-8 -*-
./scripts/tests/__init__.py: -*- coding: utf-8 -*-
./scripts/tests/test_validate_links.py: -*- coding: utf-8 -*-
./scripts/tests/test_validate_links.py:             this is valid
./scripts/tests/test_validate_links.py:             this not is valid
